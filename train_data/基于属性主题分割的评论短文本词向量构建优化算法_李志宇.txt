第 ３０ 卷 　 第 ５ 期
２０１６ 年 ９ 月
中文信息学报
ＯＦ　
ＣＨＩＮＥＳＥ　
ＩＮＦＯＲＭＡＴＩＯＮ　
ＰＲＯＣＥＳＳ
ＩＮＧ
ＪＯＵＲＮＡＬ　
ｌ．３０，Ｎｏ．５
Ｖｏ
，２０１６
Ｓｅｐ．
文章编号：１００３－００７７（
２０１６）
０５－０１０１－１０
基于属性主题分割的评论短文本词向量构建优化算法
李志宇 ，梁 循 ，周小平
（中国人民大学 信息学院，北京 １００８７２）
摘 　 要：从词向量的训练模式入手，研究了基于语料语句分割（
ＢＷＰ）算法，分隔符分割（
ＢＳＰ）算法以及 属 性 主 题 分
割（
ＢＴＰ）算法三种分割情况下的词向量训 练 结 果 的 优 劣。 研 究 发 现，由 于 评 论 短 文 本 的 自 身 特 征，传 统 的 无 分 割
（
ＮＰ）训练方法，在词向量训练结果的准确率和相似度等方面与 ＢＷＰ 算 法、
ＢＳＰ 算 法 以 及 ＢＴＰ 算 法 具 有 明 显 的 差
异。通过对 ０．
７ 亿条评论短文本进行词向量构建实验对比后发现，该文所提出的 ＢＴＰ 算法在同义词（属性词）测试
任务上获得的结果是最佳的，因此 ＢＴＰ 算法对于优化评论短文本词向 量 的 训 练，评 论 短 文 本 属 性 词 的 抽 取 以 及 情
感倾向分析等在内的，以词向量为基础的应用研究 工 作 具 有 较 为 重 要 的 实 践 意 义 。 同 时，该 文 在 超 大 规 模 评 论 语
料集上构建的词向量（开源）对于其他商品评论文本分析的应用任务具有较好可用性。
关键词：在线评论；短文本；词向量；相似度计算
中图分类号：ＴＰ　　　　 文献标识码：Ａ
Ｉｍｐ
ｒ
ｏ
ｖ
ｉ
ｎｇ
ｔ
ｈ
ｅ　
Ｗｏ
ｒ
ｄ２ｖ
ｅ
ｃ
ｏｎ　
Ｓｈｏ
ｒ
ｔ　
Ｔｅ
ｘ
ｔ
ｂｙ　
Ｔｏｐ
ｉ
ｃ：Ｐａ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ
　
　
　
Ｚｈ
ｉ
Ｘｕｎ，ＺＨＯＵ　
Ｘｉ
ａｏｐ
ｉ
ｎ
ＬＩ　
ｙｕ，ＬＩＡＮＧ　
（
１００８７２，
Ｃｈ
ｉ
ｎａ）
ｏ
ｆ　
Ｃｈ
ｉ
ｎａ，Ｂｅ
ｉ
ｉ
ｎｇ　
ｌ
ｏ
ｆ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ，
Ｒｅｎｍｉ
ｎ　
Ｕｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
Ｓｃｈｏｏ
　
　
ｊ
ｙ　
ｔ
ｏ
ｔ
ｈｅ
ｔ
ｏｐ
ｉ
ｃ．
ａｐａ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ　
ａ
ｃ
ｃ
ｏ
ｒ
ｄ
ｉ
ｎｇ
ｏｎ
ｔ
ｈｅ
ｓｈｏ
ｒ
ｔ
ｒ
ｅ
ｖ
ｉ
ｅｗ　
ｔ
ｅｘ
ｔ
ｓｂｙ　
Ａｂ
ｓ
ｔ
ｒ
ａ
ｃ
ｔ：Ｗｅ　
ｒ
ｏｐｏ
ｓ
ｅ
ａ　
ｍｅ
ｔ
ｈｏｄ
ｆ
ｏ
ｒ　
Ｗｏ
ｒ
ｄ２ｖｅ
ｃ
ｔ
ｒ
ａ
ｉ
ｎ
ｉ
ｎｇ　
　
　
　
　
　
　
　
　
ｐ
　
Ｗｅ
ｅｘａｍｉ
ｎｅ
ｔ
ｈｒ
ｅ
ｅ　
ｋ
ｉ
ｎｄｓ　
ｏ
ｆ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ　
ｍｅ
ｔ
ｈｏｄｓ，ｉ．
ｅ．Ｂａ
ｓ
ｅｄ　
ｏｎ　
Ｗｈｏ
ｌ
ｅ－ｒ
ｅ
ｖ
ｉ
ｅｗ （
ＢＷＰ），Ｂａ
ｓ
ｅｄ　
ｏｎ　
ｓ
ｅｎ
ｔ
ｅｎｃ
ｅ－Ｓｅｐａ
ｒ
ａ
ｔ
ｏ
ｒ
　
　
　
ｐａ
（
ＢＳＰ）ａｎｄ　
Ｂａ
ｓ
ｅｄ　
ｏｎ　
Ｔｏｐ
ｉ
ｃ（
ＢＴＰ），ｔ
ｏ
ｉｍｐ
ｒ
ｏｖｅ
ｔ
ｈｅ
ｒ
ｅ
ｓｕ
ｌ
ｔ
ｏ
ｆ　
Ｗｏ
ｒ
ｄ２ｖｅ
ｃ
ｔ
ｒ
ａ
ｉ
ｎ
ｉ
ｎｇ．Ｏｕ
ｒ
ｆ
ｉ
ｎｄ
ｉ
ｎｇｓ
ｓｕｇｇｅ
ｓ
ｔ
ｔ
ｈａ
ｔ
ｔ
ｈｅ
ｒ
ｅ
ｉ
ｓ
ａ
　
　
　
　
　
　
　
　
　
　
　
ｒ
ａ
ｔ
ｅ
ｓ　
ｂｅ
ｔｗｅ
ｅｎ
ｔ
ｈｅ　
Ｎｏｎｅ　
Ｐａ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ　
Ｍｏｄｅ
ｌ（
ＮＰ）ａｎｄ　
ＢＷＰ，ＢＳＰ，ＢＴＰ，ｄｕｅ
ｔ
ｏ
ａｎｄ　
ｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ｄ
ｉ
ｆ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
ｏｎ　
ａ
ｃ
ｃｕ
ｒ
ａ
ｃｙ　
ｂ
ｉ
　
　
ｙ
ｇ　
　
ｔ
ｈｅ
ｃｈａ
ｒ
ａ
ｃ
ｔ
ｅ
ｒ
ｉ
ｓ
ｔ
ｉ
ｃ　
ｏ
ｆ
ｔ
ｈｅ
ｒ
ｅ
ｖ
ｉ
ｅｗ　
ｓｈｏ
ｒ
ｔ
ｔ
ｅｘ
ｔ．Ｅｘｐｅ
ｒ
ｉｍｅｎ
ｔ
ｏｎ　
ｖａ
ｒ
ｉ
ｏｕｓ　
ｍｏｄｅ
ｌ
ｓ
ａｎｄ　
ｖｅ
ｃ
ｔ
ｏ
ｒ
ｄ
ｉｍｅｎｓ
ｉ
ｏｎｓ　
ｄｅｍｏｎｓ
ｔ
ｒ
ａ
ｔ
ｅ
ｔ
ｈａ
ｔ
ｔ
ｈｅ
　
　
　
　
　
　
　
　
　
ＢＴＰ．
ｅｎｈａｎｃ
ｅｄ　
ｂｙ　
Ｗｏ
ｒ
ｄ２ｖｅ
ｃ　
ｍｏｄｅ
ｌ
ｈａ
ｓ　
ｂｅ
ｅｎ　
ｒ
ｅ
ａ
ｔ
ｌ
ｒ
ｅ
ｓｕ
ｌ
ｔ
ｏ
ｆ　
ｗｏ
ｒ
ｄ　
ｖｅ
ｃ
ｔ
ｏ
ｒ
ｔ
ｒ
ａ
ｉ
ｎｅｄ　
ｂｙ　
　
　
　
ｇ
ｙ　
Ｋｅ
ｗｏ
ｒ
ｄ
ｓ：ｏｎ
ｌ
ｉ
ｎｅ
ｒ
ｅ
ｖ
ｉ
ｅｗ；ｓｈｏ
ｒ
ｔ
ｔ
ｅｘ
ｔ；ｗｏ
ｒ
ｄ　
ｖｅ
ｃ
ｔ
ｏ
ｒ；ｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ｃ
ａ
ｌ
ｃｕ
ｌ
ａ
ｔ
ｉ
ｏｎ
　
　
ｙ　
ｙ　
语言模型基础，即统计语言模型。
１　 引言
相对于常规语料而言，如书籍、新闻、论文、维基
百科等语料，评 论 短 文 本 的 语 言 学 规 范 非 常 弱，省
随着 社 会 化 商 务 的 发 展，在 线 评 论 已 经 成 为 了
消费者进行网络购物的重要参考决策因素之一
［
１－２］
，
略、转义、缩写等现象非常普遍。如果利用传统的训
练或者学习方法对 评 论 短 文 本 进 行 处 理，效 果 并 不
同时也成为了包括 计 算 机 科 学、管 理 科 学 以 及 情 报
理想。但从某种角 度 上 来 讲，评 论 短 文 本 的 在 文 法
分析等领域研究者在内的重要研究对象之一。通常
上的不规范，恰恰是另外一种形式的规范，即评论短
而言，在线评论包 括 微 博 评 论、商 品 评 论、点 评 评 论
文本自身特征的“规 范”，由 于 评 论 短 文 本 应 用 的 普
等评论类型，这里 我 们 统 称 为 “评 论 短 文 本”。 以 往
遍性，因此没有必要 非 要 将 评 论 短 文 本 规 约 到 常 规
关于评论短文本的应用研究主要集中在包括评论效
的语料形式上进行 处 理，反 之 应 该 在 最 大 限 度 上 保
用分析
、虚假评论识别
［
３］
以及评论观点归 纳
［
４－５］
等
［
６］
方面。然而，这些应 用 研 究 往 往 都 基 于 一 个 重 要 的
留评论短文本的语料特征。
对于评论短文本的相关建模主要是从两个角度
收稿日期：２０１５－０６－０３　 定稿日期：２０１５－１０－１５
基金项目：国家自然 科 学 基 金 （
７１５３１０１２、
７１２７１２１１）；京 东 商 城 电 子 商 务 研 究 项 目 （
４１３３１３０１２）；北 京 市 自 然 科 学 基 金
（
４１３２０６７）；中国人民大学品牌计划（
１０ＸＮＩ
０２９）；中国人民大学 ２０１５ 年度拔尖创新人才培育资助计划成果
中 文 信 息 学 报
１０２
２０１６ 年
出发：第 一，利 用 ＴＦ－ＩＤＦ，点 互 信 息、信 息 增 益 等，
型训练的精度，研究 包 括 基 于 整 句 分 割 模 式 的 预 处
对评论短文本进行 建 模，从 而 分 析 评 论 之 间 的 相 似
理模式、基于分隔符 分 割 的 预 处 理 模 式 以 及 基 于 属
度或评论的情感倾向等；第二，通过构建“词向量（词
性主题分割的预处理模式对于训练模型的影响。在
袋法）”，将评论 文 本 词 语 数 值 化。 但 这 类 建 模 方 式
后面小节中，我们将详细阐述这些方案，并重点论述
往往需要依赖于情 感 词 典、属 性 词 典 等 人 工 构 造 的
基于属性主题分割模式的预处理算法。
相关词典，具有较强的领域性，同时可扩展性较差。
随着 自 然 处 理 技 术 的 发 展，神 经 网 络 逐 步 被 引
２　 相关研究工作与研究背景
入到相关的文本处理技 术 中。２０１３ 年，谷 歌 研 究 团
［］
队的开源 的 Ｗｏ
ｒｄ２ｖｅ
ｃ 词 向 量 构 建 工 具 ７ ，引 起 了
词向量应用研究热潮，被称为 ２０１３ 年最为重要的自
然语言处理工具之一。随后，Ｗｏ
ｒｄ２ｖｅ
ｃ作为词向量
的转换工具被用于包括 短 文 本 情 感 分 析 ［８－１０］以 及 短
文本相 似 度 计 算 ［１，１１］ 等 相 关 自 然 语 言 处 理 任 务。
虽然 Ｗｏ
ｒｄ２ｖｅ
ｃ 的 应 用 范 围 广 泛，但 是 研 究 者 用 其
建模时，往往直接按照 Ｗｏ
ｒｄ２ｖｅ
ｃ的模型配置：将每
一条短文本语料（可能包含若干短句或长句）作为一
个整体行 进 行 输 入。 通 常，在 Ｗｏ
ｒｄ２ｖｅ
ｃ的 参 数 形
式里面只考虑到了 输 入 向 量 的 维 度、训 练 方 法 以 及
语料大小对模型造 成 的 影 响，却 并 没 有 考 虑 语 料 的
输入 形 式 对 Ｗｏ
ｒｄ２ｖｅ
ｃ 模 型 训 练 结 果 造 成 的 影 响。
我们研 究 发 现，不 同 的 评 论 短 文 本 输 入 形 式 会 对
ｒｄ２ｖｅ
ｃ的词向量训练结果造成明显的差异，因此
Ｗｏ
２．
１　 评论短文本的情感分析与属性提取
ｒ
ｔ　
Ｔｅｘ
ｔ）是指那些 长 度 较 短 的 文 本
Ｓｈｏ
　　 短文本（
形式。通常情况下，短文本 的 字 符 长 度 不 超 过 ４００，
／微 博 短 文 本、手 机 信 息 短 文 本、在 线
例如，
ｔ
ｔ
ｅ
ｒ
Ｔｗｉ
［，
］
评论短文 本、
ＢＢＳ 回 复 转 帖 短 文 本 等 ２ １２－１３ 。 由 于
短文本具有字数少、信 息 聚 合 度 高 以 及 文 本 语 言 不
规范等特征，使得针 对 短 文 本 的 分 析 与 研 究 产 生 了
较大的困难，其中具 有 代 表 性 的 则 是 针 对 微 博 短 文
本和评论短文本的 研 究，下 面 将 主 要 对 评 论 短 文 本
的相关研究进行综述。
随着电子商务的高速发展以及淘宝、京东、大众
点评等各类含有评 论 短 文 本 网 站 的 兴 起，评 论 短 文
本已经成为消费者在做出购买决策之前的重要参考
有必要在 Ｗｏ
ｒｄ２ｖｅ
ｃ进行词向 量 训 练 前 考 虑 输 入 语
依据 ［１４］。目前关于评论短文本 的 研 究 主 要 集 中 在：
料本身的特征，对语 料 进 行 预 处 理 后 用 以 提 升 词 向
评论短文本的效用分析、评论短文本的真实性分析、
量的训练结果。
评论短文本的决策影响分析等。但这些研究内容都
根据前文 的 阐 述，对 于 Ｗｏ
ｒｄ２ｖｅ
ｃ训 练 的 预 处
理需要优化的问题是：对于给定的大规模评论短文
会涉及两个主 题，即：评 论 短 文 本 的 情 感 分 析 与 评
论短文本的属性抽取。
评论情感分析主要是对评论的情感倾向进行分
本语料库 Ｃ＝ ｛
Ｒ１ ，
Ｒ２ ，
Ｒ３ ，…，
Ｒｎ ｝（语料库总评论数
目为 ｎ），如何在可接受时间内训练得到一 个 较 为 精
析，包含三个层次：评论对象的属性层次、评论对 象
准的词向 量 模 型。 其 目 标 为：对 于 给 定 的 词 查 询
的层次以及评论篇章层次。其主要采用的方法是将
Ｑ＝ｔ
ｅ
ｒｍ，返 回 与 其 最 相 似 的 前 Ｋ 个 结 果，其 中
ｔ
ｅ
ｒｍ 的同义属性 词 个 数 为 Ｐ，那 么，对 于 查 询 Ｑ 的
文本简化为 ＢＯＷ（
Ｂａｇ　
ｏ
ｆ　
Ｗｏ
ｒｄｓ）的形式，然后借助
词向量准确度为：ＡＣ＝Ｐ／Ｋ。为了获得对于给定测
试集 Ｔ＝ ｛
ｔ
ｔ
ｔ
ｔ
ｅ
ｒｍ１ ，
ｅ
ｒｍ２ ，
ｅ
ｒｍ３ ，…，
ｅ
ｒｍｍ ｝（测试集词条
中，Ｗｏ
ｒｄ　
Ｎｅ
ｔ等情感词典对于评论短文本的情感分
析起到了 重 要 的 作 用。 例 如，利 用 Ｗｏ
ｒｄ　
Ｎｅ
ｔ中 词
数为 ｍ）的较高准确度，有以下三种基本途径。
汇之间的相互关系（距 离、语 义 联 系 等）来 判 断 词 语
情感词典 对 评 论 短 文 本 的 情 感 倾 向 进 行 分 析。 其
１）通过对词 向 量 的 训 练 算 法 中 的 训 练 层 进 行
的情感倾向。但这也 带 来 一 个 重 要 问 题，即：Ｗｏ
ｒｄ
改进，采用不同的训 练 模 型 或 者 不 同 类 型 的 神 经 网
Ｎｅ
ｔ按照同义词 集 合 组 织 信 息，而 同 义 词 语 不 一 定
络，来获得更为精准的词向量模型。
具有相同的褒贬倾 向，这 将 导 致 对 词 语 情 感 倾 向 的
２）通过在训 练 算 法 的 输 入 层 对 语 料 进 行 预 处
理，提高算法训练的准确率和召回率。
３）通过对词向量的输出层进行后处理，提升应
用接口的准确度。
本文中，我们将集中讨论如何通过第二种方式，
即在输入层如何对语料进行预处理来提升词向量模
估计出现偏差 ［１５］。换句话说，目 前 评 论 短 文 本 情 感
分析存在的主要问题是如何针对评论短文本的特征
构建情感词之间的数值联系，即词向量的问题。
评论的属性抽取是评论短文本分析的另外一个
重要的研究内容，即 如 何 判 断 和 抽 取 评 论 中 涉 及 到
的商品属性或称对象属性的相互关系。例如，“衣服
李志宇等：基于属性主题分割的评论短文本词向量构建优化算法
５期
１０３
手感不错！”和“衣服摸起来不错！”中，词语“手感”和
法信息，通过 ＮＮＬＭ 的 研 究 发 现，ＮＷＶ 通 过 向 量
“摸起来”都是同样表达评论者对评价对象（衣服）的
间的相互计算，可以 进 一 步 拓 展 或 表 达 出 相 应 的 语
质量属性或者感官的判断。因此需要在对评论短文
义和语法特征。
本进行分析时，能够 成 功 地 发 现 和 评 价 这 类 属 性 的
相互关系。评论短文本属性的抽取对于评论属性情
感分析和评论总结都具有重要的作用。
词向 量 是 ＮＮＬＭ 实 现 后 的 关 键 产 物，在 Ｂｅｎ－
ｉ
ｏ 的工作 之 后，出 现 了 一 系 列 关 于 词 向 量 的 实 现
ｇ
［
］
与 构 建 的 相 关 工 作，包 括 Ｔｏｍａ
ｓ　Ｍｉ
ｋｏ
ｌ
ｏｖ１８－１９ 、
总而 言 之，评 论 短 文 本 的 分 析 需 要 依 赖 于 对 评
［］
Ｇｏｏｇ
ｌ
ｅ的 Ｗｏ
ｒｄ２Ｖｅ
ｃ７ 等。其中 Ｇｏｏｇ
ｌ
ｅ于 ２０１３ 年
论短文本的形式化（数学化）建模，通常而言，需要在
开源 的 Ｗｏ
ｒｄ２ｖｅ
ｃ 作 为 重 要 的 词 向 量 训 练 工 具，在
原有文本分析技术 的 基 础 上，结 合 短 文 本 的 自 身 特
情感分析、属性抽取等领域，取得了一系列的应用成
征进行改进，设计出 有 效 的 短 文 本 语 言 模 型 的 建 模
果 ［１１，２０－２１］，同时，词 向 量 训 练 的 好 坏 对 于 提 升 应 用
方法，以提高应用的效率和准确率。
成果的性能具有 重 要 的 意 义。 但 通 常 情 况 下，即 使
２　 词向量和 Ｗｏ
ｒ
ｄ２ｖ
ｅ
ｃ
２．
采用 相 同 的 Ｗｏ
ｒｄ２ｖｅ
ｃ 工 具，不 同 类 型 或 大 小 的 语
语 言 模 型 是 自 然 语 言 处 理 （Ｎａ
ｔ
ｕｒ
ｅ　
Ｌａｎｇｕａｇｅ
料库以及不同的向量维度都会对词向量的训练结果
好坏造成影响。
Ｐｒ
ｏｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ，ＮＬＰ）领 域 的 一 个 重 要 的 基 础 问 题 之
因此，本文 主 要 从 探 讨 Ｗｏ
ｒｄ２ｖｅ
ｃ训 练 词 向 量
一，它在句法分析、词 性 标 注、信 息 检 索 以 及 机 器 翻
的优化方式入手，重 点 研 究 了 不 同 的 中 文 语 料 的 预
译等子领域的相关任务中都有重要的作用。在传统
处理策略对于词向 量 训 练 结 果 的 优 化 程 度，特 别 的
语言模型中，统计语言模型具有非常广泛的应用，其
是对中文评论短文本———这一类重要的自然语言处
核心思 想 是 利 用 概 率 来 对 语 言 形 式 进 行 预 测 ［１６］。
理语 料。 本 文 主 要 贡 献 在 于：首 先，我 们 提 出 基 于
通常而言，统计语言 模 型 都 基 于 相 应 的 领 域 语 料 来
属性主题分割的短 文 本 评 论 语 料 预 处 理 算 法，对 比
进行分析工作。一 般 的，用 以 简 化 统 计 语 言 模 型 的
实验结果表明，该算 法 对 于 改 善 词 向 量 的 训 练 结 果
相关方法包 括：Ｎ－ｇ
ｒ
ａｍ 模 型、马 尔 科 夫 模 型、条 件
具有明显的提升效果；其次，我们获取了 ０．
７ 亿条评
随机场模型、决策树模型等。
随着 深 度 学 习 相 关 研 究 的 逐 步 深 入，神 经 网 络
的应用领域逐渐由 图 像、音 频 等 扩 展 到 了 自 然 语 言
处理 领 域，即 神 经 网 络 语 言 模 型 （Ｎｅｕｒ
ａ
ｌ　
Ｎｅ
ｔｗｏ
ｒｋ
Ｌａｎｇｕａｇｅ　
Ｍｏｄｅ
ｌ，ＮＮＬＭ），ＮＮＬＭ 可 以 看 作 传 统
统计语 言 模 型 的 扩 展 与 提 升，并 于 近 年 在 ＡＣＬ、
ＣＯＬＩＮＧ 等 相 关 顶 级 会 议 上 取 得 系 列 进 展。
ｉ
ｏ于
ＮＮＬＭ 具 有 代 表 意 义 的 系 统 研 究 由 Ｂｅｎｇ
Ｎｅｕｒａ
Ｐｒ
Ｌａｎｇｕａｇｅ　
Ｍｏｄｅ
２００３ 年在Ａ　
ｌ　
ｏ
ｂａｂ
ｉ
ｌ
ｉ
ｓ
ｔ
ｉ
ｃ　
ｌ
［
１７］
一文中提出 ，在该模型中作者将 每 一 个 词 汇 表 示
为一个固定维度 的 浮 点 向 量，即 词 向 量（
Ｗｏ
Ｖｅ
ｒｄ　
ｃ－
论短文本数据，通过词向量模型的训练，并优化后得
到了具备较高精度 的 词 向 量 库 （开 源），该 词 向 量 对
于其他与 在 线 商 品 评 论 相 关 的 （例 如，评 论 情 感 分
析、评论属性抽取等）自然语言处理任务具有重要的
参考意义；最后，我们给其他领域关于词向量的训练
优化研究提供了一定的参考思路：即针对特定 的 处
理语料设计相关的预处理策略或许能够显著提升词
向量的训练效果。
３　 拆分词嵌入的评论短文本分割模式
ｔ
ｏ
ｒ）。然而，
ＮＮＬＭ 中 的 词 向 量 （记 为 ＮＷＶ）和 传
统 统 计 语 言 模 型 中 的 Ｏｎｅ－Ｈｏ
ｔ　Ｒｅｐｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ
３．
１　 基 于 完 整 句 的 分 割 模 式 （
ｓ
ｅ
ｄ　
ｏｎ　
Ｗｈｏ
ｌ
ｅ
ｆ
ｏ
ｒ
Ｂａ
　
Ｐａ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ，ＢＷＰ）
（
ＯＨＲ）有着本质上的差异，主要体现在以下三点。
１）ＯＨＲ 中的向 量 元 素 采 用 ０，
１ 表 示，词 向 量
　　 完整句子是 指 以 句 号、感 叹 号、省 略 号、问 号 以
及分号分割 后 组 成 的 句 子 形 式 ［２２－２３］。 通 常 情 况 下，
中所 有 的 分 量 只 有 一 个 数 值 为 １，其 余 分 量 全 部 为
我们认为一个句子 的 结 束 是 一 种 观 点、态 度 和 说 明
０，而 ＮＷＶ 的分量由浮点数构成，其取值为连续值。
２）ＯＨＲ 的向 量 维 数 不 固 定，通 常 根 据 词 典 的
内容的结束。对于 评 论 短 文 本 而 言，一 条 评 论 通 常
大小而发生改变，并且一般较为庞大，容易造成维数
的观点既有可能相似，也有可能不同。换句话说，这
，而 ＮＷＶ 的维度通常 根 据 具 体 的 应 用 固 定
些句子之间既有可 能 存 在 逻 辑 之 间 的 联 系 性，也 有
灾难
［
１７］
在 ５０～１　
０００ 左右，具有可接受的时间复杂度。
３）ＯＨＲ 的词向量元素并不包含统计语义或语
包含几个带有完整 句 分 隔 符 的 句 子，这 些 句 子 表 达
可能是相互独立 的。 因 此，当 这 些 句 子 在 语 法 上 或
观点上是相互独立，甚至截然相反时，如果将这些句
中 文 信 息 学 报
１０４
２０１６ 年
子作为一个整体输入，用以词向量的训练，将会给训
练模型带来较大的误差。
基于 完 整 句 的 分 割 模 式 是 指 利 用 以 句 号、感 叹
号、省略号、问 号 以 及 分 号 作 为 完 整 句 的 指 示 分 割
符，对一条评论 中 的 句 子 进 行 拆 分。 同 时 考 虑 到 评
论文本的统计 信 息 （表 １），当 不 含 完 整 句 分 隔 符 的
图 １　 评论短文本案例截图
评论语句的连续字符长度达到 ２３ 时，我们将进行人
工截断，自动将该句划分为一个整句。
２　 基于分隔符的分割模式（
ｓ
ｅ
ｄ　
ｏｎ　
Ｓ
ｅｐａ
ｒ
ａ
ｔ
ｏ
ｒ
３．
Ｂａ
，
）
ｆ
ｏ
ｒ　
Ｐａ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ ＢＳＰ
　　 如图 ２ 所示，对 于 评 论 “宝 贝 做 工 不 错，物 流 速
度马 马 虎 虎！客 服 态 度 很 好！”而 言，这 是 一 种 典 型
的评论短文本的出现方式，即：观点句 １＋ 观 点句 ２
＋ … ＋ 观点句 ｎ。但观点句之间很有可 能 存 在 修 饰
　　 相比于传统的 文 本 语 料，评 论 短 文 本 在 句 点 符
词不兼容（即观点 句 １ 的 修 饰 词 不 能 用 于 观 点 句 ２
号的使用上更加的随意，内容上也更加丰富和自由，
的 情 况）以 及 观 点 句 情 感 极 性 相 反 的 情 况 。如 果 采
包括含有各种表情符号、缩写、拼写错误以及不规范
用 前 文 所 述 的 ＢＷＰ 分 割 方 式 ，由 于 消 费 者 撰 写 评
的 断 句 符 等 。 如 图 １ 所 示 ，该 图 为 淘 宝 商 城 某 商
论 时 使 用 符 号 的 不 规 范 ，极 有 可 能 造 成 不 同 的 意
品 评 论 页 面 的 截 图 。 可 以 看 到 ，对 于 评 论 短 文 本
义 、类 型 和 观 点 的 语 句 被 分 割 到 同 一 个 训 练 语 句
而 言 ，其 观 点 句 通 常 在 一 个 分 隔 符 之 内 进 行 表 达 ，
中 ，从 而 增 大 模 型 训 练 的 误 差 。 因 此 ，这 也 就 是 我
并 且 长 度 更“短 ”，同 时 在 语 法 规 范 上 也 表 现 得 尤
们 在 实 验 对 照 中 使 用 第 二 种 分 割 方 法 ，即 分 隔 符
为不足。
分割法。
图 ２　 基于分隔符分割模式与原始非分割方法的对比例图
　　 基于分隔符分 割 的 策 略，目 的 是 将 这 些 观 点 句
利用分隔符进行 拆 分。 通 常 而 言，评 论 短 文 本 中 的
分隔符包括：（。）、（，）、（；）、（、）、（空格）、（！）、（～ ）、
（＃ ）、（…）、（＊ ）、（：）、（－）、（？）、（“）、（”）、（＋ ），、（－）
３　 基于属性主题的分割模式（
ｓ
ｅ
ｄ　
ｏｎ　
Ｔｏｐ
ｉ
ｃ
ｆ
ｏ
ｒ
３．
Ｂａ
　
Ｐａ
ｒ
ｔ
ｉ
ｔ
ｉ
ｏｎ，ＢＴＰ）
以及（常见表情符号）等，同时，如果以上符号在评论
　　 在研究中我们发现，虽 然基于 ＢＳＰ 分 割 能 够 将
含有不同修饰符和不同属性观点的评论语句进行分
短文本中存在西文 格 式，将 同 样 认 为 是 评 论 文 本 分
割，以保证训练算法在这类评论上的稳定性，但 ＢＳＰ
隔符。
分割法却无法对评 论 中 存 在 相 互 联 系 的，甚 至 是 同
类的评论语 句 进 行 合 并。 因 此，在 ＢＳＰ 的 基 础 上，
我们提出了基于属性主题的分割算法。
李志宇等：基于属性主题分割的评论短文本词向量构建优化算法
５期
１０５
如图 ３ 所示，
ＢＴＰ 算 法 在 ＢＳＰ 的 基 础 上，考 虑
词向量模型，然后利用该初始词向量模 型 对 ＢＳＰ 分
了一条评论中，被分 隔 符 分 割 的 评 论 句 子 之 间 的 在
割进行重构，合并属性主题相关的句子，在保证不同
主题上的相 互 联 系。 采 用 ＢＳＰ 对 评 论 文 本 进 行 预
类型观点句得到有 效 分 割 的 同 时，保 证 了 同 类 型 观
处理后，利用词向量训练算法进行训练，得到初始的
点句的关联性，具体算法流程如算法 １ 所示。
图 ３　 基于属性主题的词嵌入分割模型
　　 算法 １ 的核心思想：首先通过分隔符对评论进
行整体拆分，然后利用 ＢＳＰ 训练得到的词 向 量 来 计
果候选短句不包含名词，则用形容词替代。最后，接
算相邻的每个最短 分 割 候 选 句 之 间 的 属 性 相 关 度。
项合并，直至满足退出要求，然后返回分割结果进行
其中，一条最短分割 候 选 句 的 属 性 特 征 由 短 句 中 的
ＢＴＰ 模型的词向量训练。
名词词向量（或者数个名词词向量 的 均 值）替 代，如
　
着使用类似层次聚 类 的 方 式，对 最 短 候 选 句 进 行 逐
算法 １：基于属性主题切割的词嵌入训练算法（
ＢＴＰ）
输入：Ｍｓ＝ ｛（
Ｗｘ ，
Ｖｘ ）｝，
Ｃ＝ ｛
Ｒ１ ，
Ｒ２ ，
Ｒ３ ，…，
Ｒｉ ｝，
Ｒｉ＝ ｛
Ｐ１ ，
Ｐ２ ，
Ｐ３ ，…，
Ｐｊ ｝，
Ｐｊ＝ ｛
Ｗ１ ，
Ｗ２ ，
Ｗ３ ，…，
Ｗｘ ｝
／＊Ｍｓ：基于分隔（
Ｓ）符切割训练的词向量结果，
Ｗｘ 为词语，
Ｖｘ 为该词语对应的词向量；
Ｃ：已经 经
过预处理的评论语料库；
Ｒｉ：对于每一条已处理 评 论，由ｊ 个 分 隔 句 组 成；
Ｐｊ：对 于 每 个 分 隔 句，由
ｘ 个词语组成；＊／
输出：ＭＴ ＝ ｛（
Ｗｘ ，
Ｖｘ ）｝　　　　　　　　　　　／＊ 基于属性主题（
Ｔ）切割训练的词向量结果 ＊／
Ｒｉｉ
Ｃ ｄｏ：
ｏ
ｒ
ｎ　
１　ｆ
　
２　　　　Ｓｅｎ
ｔ
ｅｎｃ
ｅ ＝ ［］，Ｖｅ
ｃ
ｔ
ｏ
ｒ＝ ［］
ｎ＝０
３　　　　ｍ＝０，
／＊ 初始化分割结果，词向量临查询结果列表 ＊／
／＊ 始化指针 ＊／
Ｐｊｉ
Ｒｉ ｄｏ：
ｏ
ｒ
ｎ　
４　　　　ｆ
　
Ｗｘｉ
Ｐｊ ｄｏ：
５　　　　　　ｆ
ｏ
ｒ
ｎ　
　
６　　　　　　　　ｉ
ｆ
ｓ　
Ｎｏｕｎ
ｔ
ｈｅｎ：
　Ｗｘｉ
　
Ｗｘｆｉ
Ｍｓ ）］／＊ 查询该词对应 ＭＳ 模型中对应的向量 ＊／
７　　　　　　　　　　Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｍ］［
ｎｄｖｅｃ （
ｎ］＝ ［
８　　　　　　　　　　ｎ＋＝１
９　　　　　　　　ｅ
ｌ
ｓ
ｅ：
ｔ
ｉ
ｎｕｅ
１０　　　　　　　　　 Ｃｏｎ
１１　　　　　 ｅｎｄ
１２　　　　　 Ｓｅｎ
ｔ
ｅｎｃ
ｅ［
ｍ］＝Ｐｊ
１３　　　　　 ｍ＋＝１
１４　　　 ｅｎｄ
／＊ 将查询得到的词对应的分隔句存入结果列表 ＊／
中 文 信 息 学 报
１０６
２０１６ 年
１５　　　 ｗｈ
ｉ
ｌ
ｅ　Ｍｅ
ｒ
ｉ
ｎｄｅｘ］
ｉ
ｎ　
Ｍｅ
ｒｇｅ ＞ ０．
５　
＆＆ Ｌｅｎ（Ｍｅ
ｒ
ｇｅ［
ｇｅ）＞３ｄｏ：
／＊ 只要已被处理的分隔句矩阵中存在任一两行的属性 主 题 相 似 性 的 概 率 大 于 ０．
５，同
时剩下有待被合并的行数大于 ３ 组，则合并计算继续进行 ＊／
ｆ
ｏ
ｒ
ｉ
ｎｄｅｘ１＝０；ｉ
ｎｄｅｘ１＋＝１；ｉ
ｎｄｅｘ１＜ｍ　
ｄｏ　
Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｍ］＝Ｍｅ
ａｎ（
Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｍ］）
　
１６
／＊ 计算每一行所有名词向量的均值向量，作为该分隔句的属性特征向量 ＊／
Ｍｅ
ｒｇｅ＝Ｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｍ］）
ｙ（
１７
／＊ 利用余弦相似度依次计算相邻行分隔句之间的相似度，获得可能的合并概率 ＊／
ｉ
ｆ　
Ｍｅ
ｒ
ｉ
ｎｄｅｘ２］ｉ
ｓ
ｔ
ｈｅ　
Ｍａｘ
ｉ
ｎ　
Ｍｅ
ｒ
＆＆ Ｍｅ
ｒ
ｉ
ｎｄｅｘ２］＞０．
５ｔ
ｈｅｎ：
　
　
ｇｅ［
ｇｅ　
ｇｅ［
１８
ｃ
ｔ
ｏ
ｒ［
ｉ
ｎｄｅｘ２］＝Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｉ
ｎｄｅｘ２］＋Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｉ
ｎｄｅｘ２－１］
　　Ｖｅ
１９
　　／＊ 合并相似的属性主题的特征向量 ＊／
２０
ｌ
ｅ
ｔ
ｅ（
Ｖｅ
ｃ
ｔ
ｏ
ｒ［
ｉ
ｎｄｅｘ２－１］）
　　Ｄｅ
２１
ｔ
ｅｎｃ
ｅ［
ｉ
ｎｄｅｘ２］＝ Ｓｅｎ
ｔ
ｅｎｃ
ｅ［
ｉ
ｎｄｅｘ２］＋ Ｓｅｎ
ｔ
ｅｎｃ
ｅ［
ｉ
ｎｄｅｘ２－１］
　　Ｓｅｎ
　　／＊ 合并相似属性主题的分隔句 ＊／
ｌ
ｅ
ｔ
ｅ（
Ｓｅｎ
ｔ
ｅｎｃ
ｅ［
ｉ
ｎｄｅｘ２－１］）
　　Ｄｅ
２２
２３　　　　　 ｅｎｄ
２４　　　　　 Ｗｏ
ｒｄ２ｖｅ
ｃ＿
Ｔｒ
ａ
ｉ
ｎ（
Ｓｅｎ
ｔ
ｅｎｃ
ｅ）　／＊ 将分隔完成的主题相似性句子传入词向量训练模型 ＊／
２５　 ｅｎｄ
／＊ 返回训练结果 ＊／
ＭＴ ＝ ｛（
Ｗｘ ，
Ｖｘ ）｝
２６　 ｒ
ｅ
ｔ
ｕｒｎ　
续表
４　 实验数据
４．
１　 数据描述
　　 本文的实验数据集来自天猫商城的评论短文本
数据，主要字段包括：商品 ＩＤ、评论者昵称、初 次 评
论内容、初次评论 时 间、追 加 评 论 内 容、追 加 评 论 时
数据类型
基本统
计信息
追加评 论
平均字数
０５２
２４．
字数输入范围：［
０，＋１０００）
商家回 复
平均字数
４９２
９７．
字数输入范围：［
０，＋１０００）
消费者 平
均信誉值
６４３
Ｔ１．
信誉取值范围：｛
Ｔ１，
Ｔ２，
Ｔ３，
Ｔ４｝
说明
间、评 论 相 对 位 置、评 论 者 信 誉、评 论 商 品 ＩＤ、评 论
商家 ＩＤ 以及商家 回 复。 其 中 文 本 内 容 包 括 消 费 者
的初次评论数据、追 加 评 论 数 据 以 及 商 家 的 回 复 数
据三 个 部 分，总 计 评 论 数 目 为 ７２　１５２　５４３ 条，约
４０ＧＢ。主 要 涉 及 领 域 包 含：服 装、食 品、美 妆、母
婴、数码、箱包、家电、运户，共计八大领域的 ８２ 个子
领域。数据集的相关基本统计信息如表 １ 所示。
表 １　 数据集基本信息
数据类型
初次评 论
平均字数
基本统
计信息
２４３
２３．
说明
４．
２　 数据清洗
由于 数 据 量 巨 大，因 此 数 据 清 洗 是 本 次 实 验 的
重要工作之一。本 次 实 验 过 程 中，为 了 提 高 数 据 的
读取和操作性能，我 们 将 评 论 数 据 存 储 在 当 前 流 行
的非结构化数据库之一的 Ｍｏｎｇｏｄｂ［２４］中，其性能为
普通 ＳＱＬ 数据库性能的十倍以上，大大地缩短了实
验的时间消耗。其 中，数 据 清 洗 的 核 心 步 骤 包 括 重
字数输入范围：［
０，
４００］
复评 论／无 关 评 论 的 删 除、分 词、停 用 词 的 删 除 以 及
繁简体的合并操作。
李志宇等：基于属性主题分割的评论短文本词向量构建优化算法
５期
１０７
图 ４　 数据清洗流程图
ｎ
５．
１　 性能评估
ＶＱｉ ·Ｖｓｉｍｊ
Ｑｉ ‖‖Ｖｓ
ｉｍｊ ‖
ｊ＝１
（
Ｓｉ ＝
１）
ｎ
ｉｍｊ 在 模 型 Ｘ 中 不 存 在，那
　　 特例，如果查 询 词ｓ
１．
１　 标准测试集
５．
么对于 查 询 对 （
Ｑｉ ｓ
ｉｍｊ ）而 言，其 在 模 型 Ｘ 中 的 相
似度为 －１。
５　 实验与分析
为了能够有效的测试三类预处理优化训练方案
与原始语料（非优化）训 练 方 案 之 间 的 差 异，需 要 构
∑ ‖Ｖ
则，对 于 测 试 集 Ｑ ＝ ｛
Ｑ１ ，
Ｑ２ ，
Ｑ３ ，…，
Ｑ１００ ｝，模
型 Ｘ 的平均相似度（％ ）指标如式（
２）所示。
建标准的同义（类）词测试集对四种不同类型的训练
１００
Ｓ＝
结果进行评价。具体的构建步骤为：选取待测试词
组 １００ 个，即 Ｑ ＝ ｛
Ｑ１ ，
Ｑ２ ，
Ｑ３ ，…，
Ｑ１００ ｝，查 询 每 个
词在每类已经训练好模型上的前 ５０ 个相似词，组成
∑Ｓ
ｉ
（
２）
ｉ＝１
　　 评价指标 ２：平均召回率
标准测试词对集 Ｓ＝ （
Ｑｉ｜｛
ｓ
ｓ
ｓ
ｉｍ１ ，
ｉｍ２ ，
ｉｍ３ ，…，
测试词查询得到 的 ２００ 备 选 词 进 行 筛 选，得 出 所 有
ｓ
ｉｍｎ｝），查询词 Ｑｉ 在 模 型 Ｘ 中 的 前ｎ 个 最 相 似 结
果为：Ｔ＝ （
Ｑｉ｜｛
Ｔｓ
Ｔｓ
Ｔｓ
Ｔｓ
ｉｍ１ ，
ｉｍ２ ，
ｉｍ３ ，…，
ｉｍｎ ｝），
的 测 试 词 的 同 义 （类 ） 词 组， 构 成 Ｓｔ ＝
那么对于查询词 Ｑｉ，模型 Ｘ 的召回率如式（
３）所示。
Ｑｉ ｛
ｓ
ｓ
ｓ
ｓ
ｉｍ１ ，
ｉｍ２ ，
ｉｍ３ ，…，
ｉｍｎ ｝）标准测试词对集，
（
其中，对不同的查询词 Ｑｉ 其ｎ 值可能不同。
Ｓ∩Ｔ
（
３）
ｎ
则，对 于 测 试 集 Ｑ ＝ ｛
Ｑ１ ，
Ｑ２ ，
Ｑ３ ，…，
Ｑ１００ ｝，模 型 Ｘ
的平均召回率（％ ）指标如式（
４）所示。
２００ 个备选 词 组。 首 先 删 除 重 复 词 汇，然 后 人 工 对
１．
２　 评价指标
５．
在信息检索，模 式 识 别，机 器 翻 译 等 领 域，有 两
Ｒｉ ＝
１００
类最为常用的算法评 价 指 标，即：准 确 率 （
ｅ
ｃ
ｉ
ｓ
ｉ
ｏｎ
Ｐｒ
Ｒ＝
说明，做出如下假设：
Ｓ）
 评价指标 １：平均相似度（
对于标 准 测 试 词 对 Ｓｔ 中 的 查 询 词 Ｑｉ，用 其 相
似词构建评价词对为：
Ｑｉ ｓ
Ｑｉ ｓ
Ｑｉ ｓ
Ｑｉ ｓ
ｉｍ１ ），（
ｉｍ２ ），（
ｉｍ３ ），…，（
ｉｍｎ ）｝
｛（
Ｑｉ ｓ
ｉｍｊ ），其 在
　　 那么，对于一 个 标 准 查 询 词 对 （
ｉ
（
４）
ｉ＝１
Ｒａ
ｔ
ｅ）和召回率 （
ｃ
ａ
ｌ
ｌ　
Ｒａ
ｔ
ｅ）。 本 文 将 参 考 准 确 率
Ｒｅ
和召回率的评价方式，构建模型的评价指标，为便于
∑Ｒ
５．
２　 结果分析
为了验 证 和 对 比 实 验 结 果，本 文 的 实 验 基 于
ＯＳ　
Ｘ　
１０．
１０．
４ 操作系统，
ｎ
ｔ
ｅ
ｌ　
Ｃｏ
ｒ
ｅ
ｉ
７　
４８５０Ｑ
ＭＡＣ　
Ｉ
　
内存，
处理器（四核八线程），
ＳＳＤ 存储
１６ＧＢ
５１２ＧＢ　
系 统，并 采 用 Ｐｙ
ｈｏｎ 语 言 进 行 实 现。 由 于
ｔ
ｒｄ２ｖｅ
ｃ的基础模型 包 含 Ｓｋ
ｉ
ａｍ 以 及 ＣＢＯＷ
Ｗｏ
ｐ－Ｇｒ
两类，因此本文所有 对 比 实 验 同 时 在 这 两 种 类 型 的
模型 Ｘ 中 的 向 量 组 为 （
ＶＱｉ Ｖｓｉｍｊ ）的 相 似 度 指 标 如
基础模型上进行，具 体 的 原 始 训 练 模 型 介 绍 可 以 参
式（
１）所示。
见 Ｗｏ
ｒｄ２Ｖｅ
ｃ的源码及其相关论文，此处不再详述。
中 文 信 息 学 报
１０８
２０１６ 年
最后，本实验 针 对 不 同 的 词 向 量 的 维 度 从 ５０～５００
以及 ＣＢＯＷ 模型上 的 时 间 效 率 表 现 存 在 相 互 交 叉
之间逐渐递增选取，增加纵向对比实验。
的情况，因此并 没 有 表 现 出 明 显 的 差 异。 考 虑 到 无
２．
１　 时间效率对比分析
５．
论是 ＮＰ＿
Ｓｋ
ｉ
ＣＢＯＷ 模 型、
Ｓｋ
ｉ
ＮＰ＿
ＢＳＰ＿
ｐ 模型、
ｐ模
处理时间 对 于 不 同 大 小 的 词 向 量 维 度 的 敏 感 度 较
型还 是 ＢＳＰ＿ＣＢＯＷ 模 型 的 单 机 训 练 时 间 均 在
［
２，
５］小时之间，因此，其实际意义上的时间开销（已
大，随 着 词 向 量 维 度 的 增 加，ＮＰ＿
Ｓｋ
ｉ
ｐ 以 及 ＢＳＰ＿
经是 ０．
７ 亿条评论大数据）均在可接受的范围内，所
Ｓｋ
ｉ
ｐ 模型的时间消 耗 增 长 幅 度 均 大 于 ＣＢＯＷ 模 型
以并没有必要在时间效率上对上述模型进行不同的
的增长幅度。而 ＮＰ 模型 与 ＢＳＰ 模 型 在 Ｓｋ
ｒ
ｉ
ａｍ
ｐ－ｇ
区分和优劣对比。
如图 ５ 所示，通 过 对 比 发 现，
ｒ
ｉ
ａｍ 模 型 的
Ｓｋ
ｐ－ｇ
图 ５　ＢＳＰ 算法与原始训练算法基于不同词向量维度的时间效率对比
５．
２．
２　 评价指标对比分析
Ｒ）
 平均召回率（
此可 以 看 出 ＢＴＰ 语 料 预 处 理 策 略 对 于 提 升
ｒｄ２ｖｅ
ｃ训练结果的召回率具有显著效果。同时，
Ｗｏ
如表 ２ 所 示，以 直 线 下 划 线 作 为 该 模 型 的 最 好
成绩，对 比 ＢＴＰ 模 型 与 ＮＰ 模 型，在 Ｓｋ
ｉ
ｒ
ａｍ＋
ｐ＿
ｇ
Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓｏ
ｆ
ｔｍａｘ（
ＳＧＨ）和 ＣＢＯＷ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
　
ｓｏ
ｆ
ｔｍａｘ（
ＣＢＨ）实 验 上 的 平 均 召 回 率 分 别 提 升 了
ＮＰ，
ＣＢＨ＿
ＮＰ 最大召回率
２３％ 和 １７％ ，其中，
ＳＧＨ＿
分别小于 ＳＧＨ＿
ＢＴＰ，
ＣＢＨ＿
ＢＴＰ 的最小召回率，由
我们可以发现，由于短评论语料通常字符数较小，并
且断 句 符 存 在 大 量 的 不 规 范 使 用 情 况。 因 此，从
３％ ，
０．
３％ ）远
ＮＰ 模型到 ＢＷＰ 模型的提升效果（
２．
不如 ＢＷＰ 模 型 到 ＢＳＰ 模 型 的 提 升 效 果 （
３％ ，
１２．
９．
９％ ）以 及 ＢＳＰ 到 ＢＴＰ 的 提 升 效 果 （
８．４％ ，
７．
６％ ）。
表 ２　 模型实验结果对比
ｉ
ｒ
ａｍ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓ
ｏ
ｆ
ｔｍａｘ　
Ｓｋ
　
ｐ＿
ｇ
ＮＰ　
ＢＷＰ　
ＢＳＰ　
ＣＢＯＷ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓ
ｏ
ｆ
ｔｍａｘ
　
ＢＴＰ　
ＮＰ　
ＢＷＰ　
ＢＳＰ　
ＢＴＰ
Ｄｉｍ
Ｒ　
Ｓ　
Ｒ　
Ｓ　
Ｒ　
Ｓ　
Ｒ　
Ｓ　
Ｒ　
Ｓ　
Ｒ　
Ｓ　
Ｒ　
Ｓ　
Ｒ　
Ｓ
５０
０．
４２２ ０．
４６４ ０．
５３３ ０．
４８７ ０．
６１６ ０．
７１２ ０．
７２５ ０．
５０３ ０．
４５２ ０．
５５６ ０．
４９５ ０．
６１５ ０．
７２８ ０．
７０６ ０．
７５６
７４８ ０．
１００
０．
４４５ ０．
４１７ ０．
５４９ ０．
４４６ ０．
６２２ ０．
６５８ ０．
７３６ ０．
６８９ ０．
５１２ ０．
４２１ ０．
５７７ ０．
４５８ ０．
６６３ ０．
７１４ ０．
７１５ ０．
７３１
１５０
０．
４８４ ０．
３９２ ０．
５５９ ０．
４１８ ０．
６５５ ０．
６２５ ０．
７４４ ０．
６３４ ０．
５１９ ０．
４０４ ０．
５８２ ０．
４２１ ０．
６８４ ０．
７０９ ０．
７２６ ０．
７０４
２００
０．
５０１ ０．
３８５ ０．
５５９ ０．
４０７ ０．
６６５ ０．
６０１ ０．
７５６ ０．
６１７ ０．
５３０ ０．
３９５ ０．
５９４ ０．
４０２ ０．
６８３ ０．
６６９ ０．
７４６ ０．
６８７
２５０
０．
５１１ ０．
３８１ ０．
５６１ ０．
３８０ ０．
６８４ ０．
５８２ ０．
７６１ ０．
５８８ ０．
５４６ ０．
３９２ ０．
６１３ ０．
３８８ ０．
７０１ ０．
６３０ ０．
７５７ ０．
６７４
３００
０．
５６３ ０．
３７３ ０．
５６４ ０．
３７９ ０．
６９３ ０．
５６１ ０．
７７２ ０．
５８４ ０．
５６７ ０．
３８７ ０．
６２５ ０．
３８２ ０．
７１６ ０．
６２９ ０．
７５９ ０．
６４１
３５０
０．
５６５ ０．
３６８ ０．
５７２ ０．
３７８ ０．
７０５ ０．
５３５ ０．
７８１ ０．
５４２ ０．
５８９ ０．
３７５ ０．
６２１ ０．
３７７ ０．
７１２ ０．
６１１ ０．
７６４ ０．
６２２








４００
０．
５６４ ０．
３５５ ０．
５８０ ０．
３７０ ０．
７１０ ０．
５２２ ０．
７８５ ０．
５３７ ０．
６０１ ０．
３６６ ０．
６２７ ０．
３７１ ０．
７１９ ０．
５９８ ０．
７８４ ０．
６１４
４５０
０．
５６６ ０．
３３１ ０．
５８７ ０．
３６７ ０．
７１２ ０．
５０４ ０．
７９４ ０．
５２１ ０．
６１２ ０．
３５０ ０．
６２６ ０．
３６９ ０．
７２０ ０．
５８４ ０．
７９２ ０．
６０５
５００
０．
５６５ ０．
３２５ ０．
５８９ ０．
３５０ ０．
７１０ ０．
４９４ ０．
７９６ ０．
５０５ ０．
６２４ ０．
３２４ ０．
６２４ ０．
３６５ ０．
７２６ ０．
５７１ ０．
８０２ ０．
５８５
李志宇等：基于属性主题分割的评论短文本词向量构建优化算法
５期
１０９
Ｓ）
　　 平均相似度（
由于不同的向量维度数会导致向量的分散程度
ＢＳＰ 预处理模型来说，
ＢＴＰ 模 型 的 提 升 程 度 却 并 不
不同：一般的，向量维数越大，在总词语数目固定的
任选 ＢＴＰ 或 者 ＢＳＰ 模 型 作 为 评 论 语 料 的 预 处 理
情况下，同义（属性）词间的分散程度越大，相似度越
策略。
小（纵向）。因此平均相似度只能作为词向量训练好
２．
３　 查询样例对比分析
５．
十分明显，因此如果在不考虑召回率的情况下，可以
为了能够对原始模型（
ＮＰ）和 ＢＴＰ 优化后模型
坏的一个 相 对 参 照 指 标，即：作 横 向 对 比。 以 表 ２
中波浪下划线标注的 ５０ 维度上的结果为例，对于召
产生的词向量的结 果 产 生 一 个 具 体 的 认 识 和 对 比，
回相同的词语，其相似度越高，表示同义词（属性词）
我们选 取 了 两 个 具 有 代 表 性 的 词 汇 “
ＥＭＳ”（属 性
之间的稳定性越高，因 此 在 不 同 的 环 境 下 其 应 用 的
词）以 及 “差 评 ”（形 容 词，观 点 词 ），查 询 了 它 们 在
可拓展性也就越 高。 从 表 ２ 中 可 以 看 到，无 论 是 对
ＮＰ 词向量（
２００ 维）以 及 ＢＴＰ 词 向 量 （
２００ 维）中 的
于 Ｓｋ
ｉ
ｒ
ａｍ 模型还是 ＣＢＯＷ 模型，在不同词向量
ｐ＿
ｇ
前 ２０ 个最相似的结果，如表 ３ 和表 ４ 所示。
维度上，
ＢＴＰ 模 型 的 稳 定 性 都 是 最 高 的，但 相 对 于
　
表 ３　 查询词“
ＥＭＳ”在 ＮＰ 模型和 ＢＴＰ 模型上的对比结果
Ｓｋ
ｉ
ｒ
ａｍ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓ
ｏ
ｆ
ｔｍａｘ　
　
ｐ＿
ｇ
ＮＰ　
ＣＢＯＷ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓ
ｏ
ｆ
ｔｍａｘ
　
ＢＴＰ　
ＮＰ　
ＢＴＰ
１．圆通
１１．优速
１．ｅｍｓ
　
１１．速递
１．ｅｍｓ
　
１１．中通
１．ｅｍｓ
　
１１．汽运
２．ｅｍｓ
　
１２．平邮
２．圆通
１２．中国邮政
２．邮政
１２．慢递
２．圆通
１２．快递
３．申通
１３．汇通
３．宅急送
１３．优速
３．顺风
１３．邮局
３．顺风
１３．中通
４．宅急送
１４．中铁
４．申通
１４．ＳＦ　
４．宅急送
１４．想接
４．顺丰
１４．速递
５．中通
１５．顺丰
５．顺风
１５．国通
５．顺丰
１５．同城
５．邮政
１５．物流
６．韵达
１６．国通
６．邮政
１６．快运
６．圆通
１６．韵达
６．ＥＭｓ
　
１６．邮局
７．邮政
１７．慢到
７．邮局
１７．全峰
７．申通
１７．快运
７．Ｅｍｓ
　
１７．送货员
８．全峰
１８．压货
８．Ｅｍｓ
　
１８．ＥＭｓ
　
８．平邮
１８．速递
８．宅急送
１８．中国邮政
９．速递
１９．快运
９．顺丰
１９．ＭＥＳ　
９．Ｅｍｓ
　
１９．顺丰速运
９．ＥＳＭ　
１９．快第
１０．顺风
２０．慢递
１０．韵达
２０．ＥＳＭ　
１０．陆运
２０．快弟
１０．韵达
２０．陆运























表 ４　 查询词“差评”在 ＮＰ 模型和 ＢＴＰ 模型上的对比结果
Ｓｋ
ｉ
ｒ
ａｍ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓ
ｏ
ｆ
ｔｍａｘ　
　
ｐ＿
ｇ
ＮＰ　
ＣＢＯＷ＋Ｈｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
ｓ
ｏ
ｆ
ｔｍａｘ
　
ＢＴＰ　
ＮＰ　
ＢＴＰ
１．真想
１１．添堵
１．差平
１１．心甘情愿
１．中评
１１．你们
１．中评
１１．全 ０ 分
２．中评
１２．故意
２．低分
１２．坏评
２．老子
１２．这种
２．坏评
１２．说法
３．忍无可忍
１３．别找我
３．高分
１３．差品
３．极度
１３．换算
３．好评
１３．退货
４．恶意
１４．无心
４．全一星
１４．不错
４．找麻烦
１４．因此
４．差平
１４．退钱
５．愤怒
１５．要挟
５．认栽
１５．恶评
５．你
１５．添堵
５．二分
１５．认栽
６．坚决
１６．认栽
６．好评
１６．０ 星
６．差平
１６．上图
６．一星
１６．零颗星
７．宝贝
１７．你
７．中评
１７．天理难容
７．退换货
１７．哎
７．最高分
１７．麻烦
８．过不去
１８．垃圾
８．一星
１８．千万不要
８．换认
１８．退
８．传说
１８．跪
９．没商量
１９．抗议
９．两星
１９．半星
９．退货
１９．差到
９．０ 星
１９．０ 分
１０．师
２０．好评
１０．查评
２０．零星
１０．换
２０．吃太甜
１０．一颗星
２０．零分
中 文 信 息 学 报
１１０
　　 通过表 ３ 可以发现：ＢＴＰ 模型的预处理策略能
够有效的发现属性 词 的 相 似 词 及 其 变 异，甚 至 是 错
误的拼 写 词。 例 如，
ＥＳＭ、
ＳＧＨ ＿ＢＴＰ 模 型 中 的 “
ＭＥＳ”（误 输 入）、“
ＥＭｓ”（大 小 写 变 形）等。 同
ｅｍｓ、
时可以发 现，
ＢＴＰ 模 型 的 属 性 词 召 回 率 明 显 高 于
ＮＰ 模型。 通过表 ４ 可以发现：ＢＴＰ 模型对于 同 义
２０１６ 年
／．
ｃ
ｏｄｅ．
ｌ
ｅ．
ｃ
ｏｍ／ｐ／ｗｏ
ｒ
ｄ２ｖｅ
ｃ
ｇｏｏｇ
［
８］　Ｘｕｅ　
Ｂ，Ｆｕ　
Ｃ，Ｓｈａｏｂ
ｉ
ｎ　
Ｚ．Ａ　
Ｓ
ｔ
ｕｄｙ　
ｏｎ　
Ｓｅｎ
ｔ
ｉｍｅｎ
ｔ　
Ｃｏｍ－
ｔ
ｉ
ｎｇ　
ａｎｄ　
Ｃｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ　
ｏ
ｆ
Ｓ
ｉ
ｎａ　
Ｗｅ
ｉ
ｂｏ　
ｗｉ
ｔ
ｈ　
Ｗｏ
ｒ
ｄ２ｖｅ
ｃ
　
ｐｕ
［
／／Ｐｒ
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ
ｔ
ｈｅ
２０１４ＩＥＥＥ　
Ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌ　
Ｃｏｎ－
　
　
ｅ
ｓ
ｓ　
ｏｎ，２０１４：３５８－３６３．
ｒ
ｇ
［
９］　Ｔａｎｇ　
Ｄ，Ｗｅ
ｉ
Ｆ，Ｙａｎｇ　
Ｎ，ｅ
ｔ
ａ
ｌ．Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ　
ｓ
ｅｎ
ｔ
ｉｍｅｎ
ｔ－
　
　
词的召回率同样 较 好，而 ＮＰ 模 型 中 甚 至 出 现 了 较
ｓｐｅ
ｃ
ｉ
ｆ
ｉ
ｃ　
ｗｏ
ｒ
ｄ　
ｅｍｂｅｄｄ
ｉ
ｎｇ
ｆ
ｏ
ｒ
ｔｗｉ
ｔ
ｔ
ｅ
ｒ
ｓ
ｅｎ
ｔ
ｉｍｅｎ
ｔ
ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ－
　
　
　
　
［
］
／
／
ｃ
ａ
ｔ
ｉ
ｏｎ Ｃ Ｐｒ
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ
ｔ
ｈｅ
５２ｎｄ　
Ａｎｎｕａ
ｌ　
Ｍｅ
ｅ
ｔ
ｉ
ｎｇ　
ｏ
ｆ
　
　
多将查询词的被修饰词判定为相似词的情况，例如，
ｔ
ｈｅ　
Ａｓ
ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏｎ
ｆ
ｏ
ｒ　
Ｃｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌ　
Ｌ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ，２０１４：
　
真想（差评），坚决（差评）等。但同时也需要看到，对
于 ＮＰ 模 型 和 ＢＴＰ 模 型 都 出 现 了 查 询 词 的 反 义 词
被判定为相似词的 情 况，这 种 误 判 需 要 在 后 续 的 研
究中进一步优化。
６　 结论
Ｗｏ
ｒｄ２ｖｅ
ｃ词向量训 练 的 优 化 问 题 不 仅 仅 需 要
考虑训练算法的内 部 结 构，对 于 不 同 类 型 的 训 练 语
料的预处理同样值得思考。本文针对评论短文本在
ｒｄ２ｖｅ
ｃ词向量训练中存在的问题，结合评论短文
Ｗｏ
本的自身特征提出了基于属性主题分割的语料预处
理算法 ＢＴＰ。基 于 ０．
７亿条大规模评论短文本的
实验表明，
ＢＴＰ 算法的预 处 理 策 略 对 于 提 升 词 向 量
模型的训练结果具有显著意义。本文针对评论短文
本的大规模词向量训练结果对于其他关于包括评论
短文本情感分析、评 论 短 文 本 属 性 特 征 提 取 （聚 类）
等的应用都具有较大的参考意义。
参考文献
［
１］　Ｙｕａｎ　
Ｙ，Ｈｅ　
Ｌ，Ｐｅｎｇ　
Ｌ，ｅ
ｔ
ａ
ｌ．Ａ　
Ｎｅｗ　
Ｓ
ｔ
ｕｄｙ　
Ｂａ
ｓ
ｅｄ　
ｏｎ
　
１５５５－１５６５．
［
］
１０ 　Ｇｏｄ
ｉ
ｎ　
Ｆ，Ｖａｎｄｅ
ｒ
ｓｍｉ
ｓ
ｓ
ｅｎ　
Ｂ，Ｊ
ａ
ｌ
ａ
ｌ
ｖａｎｄ　
Ａ，ｅ
ｔ
ａ
ｌ．Ａｌ
ｌ
ｅ－
　
ｖ
ｉ
ａ
ｔ
ｉ
ｎｇ　Ｍａｎｕａ
ｌ
ａ
ｔ
ｕ
ｒ
ｅ　Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ
ｏ
ｒ　Ｐａ
ｒ
ｔ－ｏ
ｆ－
　Ｆｅ
　ｆ
ｅ
ｃｈ　
Ｔａｇｇ
ｉ
ｎｇ　
ｏ
ｆ　
Ｔｗｉ
ｔ
ｔ
ｅ
ｒ　
Ｍｉ
ｃ
ｒ
ｏｐｏ
ｓ
ｔ
ｓ　
ｕｓ
ｉ
ｎｇ　
Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂ－
Ｓｐｅ
／／Ｐｒ
ｔ
ｅｄ　
Ｗｏ
ｒ
ｄ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ　
ＮＩＰＳ
ｕ
２０１４Ｗｏ
ｒ
ｋｓｈｏｐ　ｏｎ　Ｍｏｄｅ
ｒ
ｎ　Ｍａ
ｃｈ
ｉ
ｎｅ　Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ　ａｎｄ
（
）
，
Ｎａ
ｔ
ｕ
ｒ
ａ
ｌ　
Ｌａｎｇｕａｇｅ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ ＮＩＰＳ　
２０１４ ２０１４：
１－５．
［
１１］　Ｇｈ
ｉ
ｓ
ｉ
ａｎ　
Ｂ，Ｇｕｏ　Ｙ　Ｆ．Ｓｅｎ
ｔ
ｉｍｅｎ
ｔ　Ａｎａ
ｌ
ｓ
ｉ
ｓ　Ｕｓ
ｉ
ｎｇ
ｙａ
ｙ
Ｓｅｍｉ
Ｓｕｐｅ
ｒ
ｖ
ｉ
ｓ
ｅｄ　
Ｒｅ
ｃｕ
ｒ
ｓ
ｉ
ｖｅ　
Ａｕ
ｔ
ｏｅｎｃ
ｏｄｅ
ｒ
ｓ
ａｎｄ　
Ｓｕｐｐｏ
ｒ
ｔ
　
Ｓ
ｔ
ａｎ
ｆ
ｏ
ｒ
ｄ．
ｅｄｕ，
２０１４：１－５．
Ｖｅ
ｃ
ｔ
ｏ
ｒ　
Ｍａ
ｃｈ
ｉ
ｎｅ
ｓ［
ＥＢ／ＯＬ］，
［
１２］　 张林，钱冠群，樊卫国，等 ．轻 型 评 论 的 情 感 分 析 研
究［
Ｊ］．软件学报，２０１４，１２：２７９０－２８０７．
［
１３］　 周泓，刘金岭，王新功 ．基于短文本信息流 的 回 顾 式
话题识别模型［
Ｊ］．中文信息学报，２０１５，２９１：０１５．
［
１４］　 郑小平 ．在 线 评 论 对 网 络 消 费 者 购 买 决 策 影 响 的 实
证研究［
Ｄ］．中国人民大学硕士学位论文，
２００８．
［
１５］　 张紫琼，叶强，李一军 ．互联网商品评论情 感 分 析 研
究综述［
Ｊ］．管理科学学报，２０１０，１３（
６）：８４－９６．
［
１６］　 邢永康，马少 平 ．统 计 语 言 模 型 综 述 ［
Ｊ］．计 算 机 科
学，２００３，３０（
９）：２２－２６．
［
１７］　Ｂｅｎｇ
ｉ
ｏ　
Ｙ，Ｄｕｃｈａ
ｒｍｅ　
Ｒ，Ｖｉ
ｎｃ
ｅｎ
ｔ　
Ｐ，ｅ
ｔ
ａ
ｌ．Ａ　
ｎｅｕ
ｒ
ａ
ｌ
　
［
］
ｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｓ
ｔ
ｉ
ｃ
ｌ
ａｎｇｕａｇｅ　
ｍｏｄｅ
ｌＪ ．Ｔｈｅ
Ｊ
ｏｕ
ｒ
ｎａ
ｌ
ｏ
ｆ　
Ｍａ－
　
　
　
ｐ
Ｒｅ
ｓ
ｅ
ａ
ｒ
ｃｈ，２００３，３：１１３７－１１５５．
ｉ
ｎｅ　
Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ　
ｃｈ
［
］
，
１８ 　 Ｍｉ
ｋｏ
ｌ
ｏｖ　
Ｔ Ｓｕ
ｔ
ｓｋｅ
ｖｅ
ｒ
Ｉ，Ｃｈｅｎ　
Ｋ，ｅ
ｔ
ａ
ｌ．Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ
　
　
Ｗｏ
ｒ
ｄ２ｖｅ
ｃ
ｕｓ
ｔ
ｅ
ｒ
ｏ
ｒ　Ｄｏ
ｃｕｍｅｎ
ｔ　Ｃａ
ｔ
ｅｇｏ
ｒ
ｉ
ｚ
ａ
ｔ
ｉ
ｏｎ
　ａｎｄ　Ｃｌ
　ｆ
ｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ　
ｏ
ｆ　
ｗｏ
ｒ
ｄｓ
ａｎｄ　
ａ
ｓ
ｅ
ｓ
ａｎｄ
ｔ
ｈｅ
ｉ
ｒ
ｃ
ｏｍ－
　
　
　
　
ｐｈｒ
／／Ｐｒ
ｓ
ｉ
ｔ
ｉ
ｏｎａ
ｌ
ｉ
ｔ
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　ｏ
ｆ
ｈｅ　Ａｄｖａｎｃ
ｅ
ｓ
ｎ
　ｔ
　ｉ
ｐｏ
ｙ［Ｃ］
［
Ｊ］．Ｊ
ｏｕ
ｒ
ｎａ
ｌ
ｆ　Ｃｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　Ｓｙ
ｓ
ｔ
ｅｍｓ，
　ｏ
　Ｉ
Ｓｙ
ｓ
ｔ
ｅｍｓ，２０１３：３１１１－
Ｎｅｕ
ｒ
ａ
ｌ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ　
　
２０１４，１０：９３０１－９３０８．
［
２］　 张 剑 峰，夏 云 庆，姚 建 民 ．微 博 文 本 处 理 研 究 综 述
［
Ｊ］．中文信息学报，２０１２，２６（
４）：２１－２７．
［
３］　 杨铭，祁 巍，闫 相 斌，等 ．在 线 商 品 评 论 的 效 用 分 析
研究［
Ｊ］．管理科学学报，２０１２，１５（
５）：６５－７５．
［
４］　 陈燕方，李志宇 ．基于评论产品属性情感倾向评估的虚
假评论识别研究［
Ｊ］．现代图书情报技术，２
０
１
４，９：８
１－９
０．
［
５］　 任亚峰，尹 兰，姬 东 鸿 ．基 于 语 言 结 构 和 情 感 极 性 的
虚假评论识 别 ［
３）：
Ｊ］．计 算 机 科 学 与 探 索，２０１４，８（
３１３－３２０．
［
Ｂ，Ｌｅ
ｅ　
Ｌ．Ｏｐ
ｉ
ｎ
ｉ
ｏｎ　
ｍｉ
ｎ
ｉ
ｎｇ　
ａｎｄ　
ｓ
ｅｎ
ｔ
ｉｍｅｎ
ｔ
ａｎａ
ｌ
ｓ
ｉ
ｓ
６］　Ｐａｎｇ　
　
ｙ
［
Ｊ］．Ｆｏｕｎｄａ
ｔ
ｉ
ｏｎｓ
ａｎｄ
ｔ
ｒ
ｅｎｄｓ
ｉ
ｎ
ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
ｒ
ｅ
ｔ
ｒ
ｉ
ｅ
ｖａ
ｌ，
　
　
　
　
２００８，２：１－１３５．
［
／／
７］　Ｍｉ
ｋｏ
ｌ
ｏｖ　Ｔ． Ｗｏ
ｒ
ｄ２ｖｅ
ｃ　ｐ
ｒ
ｏ
ｅ
ｃ
ｔ［
ＣＰ］．
２０１３，ｈ
ｔ
ｔ
ｊ
ｐｓ：
３１１９．
［
１９］　Ｍｉ
ｋｏ
ｌ
ｏｖ　
Ｔ，Ｃｈｅｎ　
Ｋ，Ｃｏ
ｒ
ｒ
ａｄｏ　
Ｇ，ｅ
ｔ
ａ
ｌ．Ｅｆ
ｆ
ｉ
ｃ
ｉ
ｅｎ
ｔ
ｅ
ｓ
ｔ
ｉ－
　
　
ｍａ
ｔ
ｉ
ｏｎ　
ｏ
ｆ　
ｗｏ
ｒ
ｄ
ｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ
ｉ
ｎ　
ｖｅ
ｃ
ｔ
ｏ
ｒ
ｓｐａ
ｃ
ｅ．ａ
ｒＸｉ
ｖ
　
　
　
ｒ
ｅｐ
ｒ
ｉ
ｎ
ｔ
ａ
ｒＸｉ
ｖ：１３０１．
３７８１［
ＤＢ＼ＯＬ］，２０１３：１－１６．
　
ｐ
［
］
，
，
，
２０ 　Ｚｈａｎｇ　
Ｗ Ｘｕ　
Ｗ Ｃｈｅｎ　
Ｇ ｅ
ｔ
ａ
ｌ．Ａ　
Ｆｅ
ａ
ｔ
ｕ
ｒ
ｅ　
Ｅｘ
ｔ
ｒ
ａ
ｃ－
　
ｉ
ｏｎ　Ｍｅ
ｔ
ｈｏｄ　Ｂａ
ｓ
ｅｄ　ｏｎ　Ｗｏ
ｒ
ｄ　Ｅｍｂｅｄｄ
ｉ
ｎｇ
ｏ
ｒ　Ｗｏ
ｒ
ｄ
ｔ
　ｆ
／／Ｐｒ
Ｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
Ｃｏｍｐｕ
ｔ
ｉ
ｎｇ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ
ｔ
ｈｅ　
Ｎａ
ｔ
ｕ
ｒ
ａ
ｌ
　
ｙ　
Ｌａｎｇｕａｇｅ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ　
ａｎｄ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ　
Ｃｏｍｐｕ
ｔ
ｉ
ｎｇ，２０１４：
１６０－１６７．
［
２１］　Ｉ
ｒ　
Ｍ，Ｅｎｎｓ　
Ｐ，Ｂｏｙｄ－Ｇｒ
ａｂｅ
ｒ
Ｊ，ｅ
ｔ
ａ
ｌ．Ｐｏ
ｌ
ｉ
ｔ
ｉ
ｃ
ａ
ｌ
ｉ
ｄｅ－
　
　
　
ｙｙｅ
／／
ｌ
ｏｇｙ　
ｄｅ
ｔ
ｅ
ｃ
ｔ
ｉ
ｏｎ　
ｕｓ
ｉ
ｎｇ
ｒ
ｅ
ｃｕ
ｒ
ｓ
ｉ
ｖｅ　
ｎｅｕ
ｒ
ａ
ｌ
ｎｅ
ｔｗｏ
ｒ
ｋｓ［
Ｃ］
ｏ
　
　
Ｐｒ
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　ｏ
ｆ
ｈｅ　Ａｓ
ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏｎ
ｏ
ｒ　Ｃｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌ
　ｔ
　ｆ
，
：
Ｌ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ ２０１４ １－１１．
（下转第 １２０ 页）
中 文 信 息 学 报
１２０
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ（
ＡＣＬ），２０１０：３８４－３９４．
ｇｕ
［
ｒ
ｔ
１９］　Ｙ　
Ｈｏｎｇ，Ｘ　
Ｐ　
Ｚｈｏｕ，Ｔ　
Ｔ　
Ｃｈｅ，ｅ
ｔ
ａ
ｌ．Ｃｒ
ｏ
ｓ
ｓ－ａ
　
ｇｕｍｅｎ
ｉ
ｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ
ｏ
ｒ
ｌ
ｉ
ｃ
ｉ
ｔ
ｉ
ｓ
ｃ
ｏｕ
ｒ
ｓ
ｅ
ｅ
ｌ
ａ
ｔ
ｉ
ｏｎ　ｒ
ｅ
ｃ
ｏｇｎ
ｉ
ｔ
ｉ
ｏｎ
　ｆ
　ｉｍｐ
　ｄ
　ｒ
［
／／Ｐｒ
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　ｏ
ｆ
ｈｅ
ｔ　ＡＣＭ　Ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌ
　ｔ
　２１ｓ
Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
ｏｎ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　ａｎｄ　Ｋｎｏｗｌ
ｅｄｇｅ　Ｍａｎａｇｅ－
　Ｉ
ｍｅｎ
ｔ（
ＣＩＫＭ），２０１２：２９５－３０４．
２０１６ 年
［
２０］　Ｃ　
Ｃ　
Ｃｈａｎｇ，Ｃ　
Ｊ　
Ｌ
ｉ
ｎ．ＬＩＢＳＶＭ：ａ
ｌ
ｉ
ｂ
ｒ
ａ
ｒ
ｆ
ｏ
ｒ
ｓｕｐｐｏ
ｒ
ｔ
　
　
ｙ
　
ｖｅ
ｃ
ｔ
ｏ
ｒ　
ｍａ
ｃｈ
ｉ
ｎｅ
ｓ［
Ｊ］．ＡＣＭ　
Ｔｒ
ａｎｓ
ａ
ｃ
ｔ
ｉ
ｏｎｓ　
ｏｎ
Ｉ
ｎ
ｔ
ｅ
ｌ
ｌ
ｉ
ｅ
ｎ
　
ｇ ｔ
Ｓｙ
ｓ
ｔ
ｅｍｓ
ａｎｄ　
Ｔｅ
ｃｈｎｏ
ｌ
ｏｇｙ （
ＴＩ
ＳＴ），２００１，２（
３）：３８９－
　
３９６．
［
２１］　 徐凡，朱巧明，周国栋 ．基于树核的隐式篇 章 关 系 识
别［
Ｊ］．软件学报，２０１３，２４（
５）：１０２２－１０３５．
朱珊珊（
１９９２—），硕 士 研 究 生，主 要 研 究 领 域 为
洪宇（
１９７８—），通信作者，副教 授，主 要 研 究 领 域
篇章分析。
为信息抽取，信息检索，事件关系检测等。
ｉ
ｌ：ｚｈｕｓｈａｎｓｈａｎ０６３＠ｇｍａ
ｉ
ｌ．
ｃ
ｏｍ
Ｅ－ｍａ
ｉ
ｌ：ｔ
ｉ
ａｎｘ
ｉ
ａｎｅ
ｒ＠ｇｍａ
ｉ
ｌ．
ｃ
ｏｍ
Ｅ－ｍａ
丁思远（
１９９２—），硕 士 研 究 生，主 要 研 究 领 域 为
事件关系检测。
Ｅ－ｍａ
ｉ
ｌ：ｄｓ
ｅｖｅ
ｒ＠ｇｍａ
ｉ
ｌ．
ｃ
ｏｍ
ｙ．
櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚櫚
（上接第 １１０ 页）
［
２２］　 黄建传 ．汉语标点句统计分 析 ［
Ｄ］．北 京 语 言 大 学 硕
士学位论文，２００８．
［
２３］　 何玉 ．基于核心词扩展的文 本 分 类 ［
Ｄ］．华 中 科 技 大
李志宇（
１９９１—），博 士 研 究 生，主 要 研 究 领 域 为
学硕士学位论文，２００６．
［
２４］　Ｂａｎｋｅ
Ｐｕｂ
ｌ
ｉ－
ｒ　
Ｋ．ＭｏｎｇｏＤＢ
ｉ
ｎ　
ａ
ｃ
ｔ
ｉ
ｏｎ［Ｍ］．Ｍａｎｎ
ｉ
ｎｇ　
　
ａ
ｔ
ｉ
ｏｎｓ，２０１１．
ｃ
梁循（
１９６５—），通 信 作 者，博 士 生 导 师，教 授，主
自然语言处理，网络结构嵌入，社会网络分析。
要研究领域为社会计算，机器学习。
ｉ
ｌ：ｚｈ
ｉ
ｌ
ｅ
ｅ＠ｒ
ｕｃ．
ｅｄｕ．
ｃｎ
Ｅ－ｍａ
ｙｕ
ｉ
ｌ：ｘ
ｌ
ｉ
ａｎｇ＠ｒ
ｕｃ．
ｅｕｄ．
ｃｎ
Ｅ－ｍａ
周小平（
１９８５—），博 士 研 究 生，主 要 研 究 领 域 为
社会网络分析，网络隐私保护。
Ｅ－ｍａ
ｉ
ｌ：ｚｈｏｕｘ
ｉ
ａｏｐ
ｉ
ｎｇ＠ｂｕｃ
ｅ
ａ．
ｅｄｕ．
ｃｎ