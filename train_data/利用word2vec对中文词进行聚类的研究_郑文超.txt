 2013  34   12 

 word2vec 
，
（， 100876）

 ：，，。
，。，。
 Word2Vec ，，。，
， K-means ，。
 2012 ，，。
：；；；；
DOI：10.3969/j.issn.1003-6970.2013.12.040
：TP39
：A
：[1]  ,  .  word2vec  [J].  ,2013,34(12): 160-162

Research on Chinese word Clustering with Word2vec
ZHENG Wen-chao,XU Peng
(Beijing University of Posts & Telecommunications Institute of Network Technology, Beijing 100876, China)

【Abstract】Text clustering plays an important role in data mining and machine learning. After years of development, clustering
technology has produced a series of theorey. This paper explored a new method of Chinese clustering. By putting forword a new method to
Chinese word segments, this paper can split Chinese text into word segments. With Word2Vec toolset, we can transfrom word segments into
vectors. To define the cosine distance between two vectors, we can apply K-means algorithm on the vectors to cluster words. In this paper,
we downloaded network news text on the Internet, and applied the methods above, which shows good result.
【Key words】data mining; clustering; word segment; word vector; neural networks

0 

1 

，

1.1 

。

“”

。，

。，

。

，“”。

：，；
，；，
，

，、，
。，


。，，

。[1] “”，

。

，。

，

1.2 

“”，，

。

；

。

 n ，，

、

； k-mean 
。， word2vec ，
。

。
：，

：（1988-），，，：、
：（1977-），，，：、 .

160

 ： word2vec 

 1 
Fig.1 neural networks model
。，

。 N-gram ，

。，，

N ，P(wiT) = P(wi|wt-i+1,…,wt-1)。，

 3 ； 4 

 n ， P(wiT) = ∏ P(wt|wit-1)。

，； 1 ，

1.4 

 1 ； 2 。

            Bengio  2003    [3]。

，

Bengio 。，

，，

 n-gram 。(  1)

。4 ：

 wt-n+1,…,wt-2,wt-1  n − 1 ，

1) 

n − 1  wt。C(w)  w ，

2) 

， C，C  |V|×M。

3) 

 |V| ，M 。w  C(w) 

4) ，

。

，

（ ）  C(wt-n+1),…,C(wt-2),C(wt-1)

。，

 n-1 ， (n-1)×m ，

。

 x。（），

1.3 

 d+Hx ， d ，。
 tanh 。（） yi ，

，
[2]

P(w1,w2,…wn)， wn  。 

 |V| 。 i  log 。

，，，，

 softmax  [4]  y 。 y 

。，

：y = b + Wx + U*tanh (d+Hx)。

 ( 

 U ， |V|×h 

 )，，

， U 。

 n （N-gram ），

 W，
161

 ： word2vec 
 2 “” top10 

 1 “” top10 

Tab. 2 top 10 words most similar with ‘Computer’

Tab. 1 top 10 words most similar with ‘China’
 top10 











 top10 












0.445374
0.437304
0.436195
0.420332
0.414920
0.408968
0.408262
0.396693
0.390933
0.360211


0.756215
0.692575
0.692512
0.654025
0.634722
0.630025
0.629519
0.621523
0.620579
0.614451

。 W  0。

。 K-means ，。

，， y 

， 1、 2 。

。，

3 

，“”。，



 K-means 。

。；

1.5 K-means 

、OCR、

K-means  k， n  k

。，

：

。，

；。

，，，。

“”。[5]

。 word2vec

：

，，

1)  n  k  ;

。

2) ，
； ;



3) （） ;

[1]  .  [OL]. [2012.10.12]. http://terms.naer.edu.tw/detail/

4) ，，，

1679006/

； 2。

Zeng,Y Y. Word Bags Model[OJ][2012.10.12] http://terms.naer.edu.tw/
detail/1679006/ (In Chinese)

2 

[2]  .  [OL]. [2013-3-12]. http://zh.wikipedia.org/zh-cn
/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B

，。

Wikipedia. Language Model[OL].[2013-3-12]. http://zh.wikipedia.org/

“”“”，

zh-cn/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B (In

， XML 。

Chinese)

 2TB，，

[3]Yoshua B,Rejean D,Pascal V,Christian J. A Neural Probabilistic
Language Model[J]. Journal of Machine Learning Research, 2003,

，，。

3(0): 1137–1155
[4] ， . Softmax  [OL]. [2013-8-13]. http://uﬂdl.stanford.

，

edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92

“”。

Zeng,J Y, Wang F. Softmax regression[OL].[2013-8-13]. http://uﬂdl.

 word2vec ，

stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92 (In

，。Word2vec  Google 

Chinese)
[5] ，， .  k-means  [J]. 

，“”，

 , 2007, 33(3):65-69

。

Yuan F, Zhou Z Y, Song X. Initial Cluster Center Optimization

。

Algorithm[J]. Computer Engineer, 2007, 33(3):65-99

162