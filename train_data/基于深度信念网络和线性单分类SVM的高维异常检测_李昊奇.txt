

 SVM 
，，，
（，  310018）


：，

。，，
。 UCI ，，
。 PCA-SVDD ， 4.65%。
，。
：；；；
：TP183
：A
doi: 10.11959/j.issn.1000−0801.2018006

High-dimensional outlier detection based on deep
belief network and linear one-class SVM
LI Haoqi, YING Na, GUO Chunsheng, WANG Jinhua
Hangzhou Dianzi University, Hangzhou 310018, China
Abstract: Aiming at the difficulties in high-dimensional outlier detection at present, an algorithm of
high-dimensional outlier detection based on deep belief network and linear one-class SVM was proposed. The algorithm firstly used the deep belief network which had a good performance in the feature extraction to realize the dimensionality reduction of high-dimensional data, and then the outlier detection was achieved based on a one-class
SVM with the linear kernel function. High-dimensional data sets in UCI machine learning repository were selected to
experiment, result shows that the algorithm has obvious advantages in detection accuracy and computational complexity. Compared with the PCA-SVDD algorithm, the detection accuracy is improved by 4.65%. Compared with the
automatic encoder algorithm, its training time and testing time decrease significantly.
Key words: outlier detection, high-dimensional data, deep belief network, one-class SVM



1 



。

[1]。

：2017−06−21；：2017−09−26
：（No.61372157）
；
“” A （No.GK178800207001）
Foundation Items: The National Natural Science Foundation of China(No.61372157), Zhejiang Provincial First Class Disciplines: Class
A-Electronic Science and Technology (No.GK178800207001)

2018006-1

·35·



2018  1 

。

（principle component analysis，PCA）、

、、

[7]（locally linear embedding，LLE）

[2]。

[8]（canonical correlation analysis，CCA）

，（one-class
support vector machine，OCSVM）
[3]

。
，

 。OCSVM 

，

，。

、。

，OCSVM 

 PCA  CCA 

。 OCSVM ，

（kernel principle component analysis，KPCA）

，

[9]（kernel canonical correlation

（plane based support vector ma-

analysis，KCCA），

chine，PSVM）

、。

（support vector data description，SVDD）。

，

， SVDD 

。[10]

 PSVM 。， SVDD 

OCSVM ，

。

。

，

，。

，。

[11] PCA  OCSVM ，

，

 PCA ，

。、Web 、

OCSVM 。

。，

，。

OCSVM 

[12] KPCA  OCSVM 

，

。，

[4]

 。

，



，。[13]

。“”

（autoencoder，AE）
，

。

。

，

，。

。，

            （ deep belief

，

network，DBN），





。，

。

。

，

，

。，

，



[5]

 。

[6]

，

2018006-2



·36·

，。

 OCSVM ， SVDD 

，

。SVDD ，

，

 SVM ，

。

，

，，

。 2 ，



，，

，

。，

。


。

2 

，

（DBN-OCSVM） 1

，。

，， DBN 

 SVDD ，

 OCSVM。DBN  2 （restricted

（radical basis function，

Boltzmann machine，RBM）。

RBF）
。 SVM ，

 DBN ， RBM1 ，

，

 1。 1  RBM2

，

 2。 2 

。，

DBN ， OCSVM 。

，RBF 
， SVM 
。， DBN 

，，
。
，。

3 
3.1
1


DBN — RBM

DBN-OCSVM 

2

OCSVM-SVDD 

2018006-3

·37·

2018  1 



，—

：

RBM， RBM 

P ( v,h ) =

 DBN。 RBM 
 RBM ，
， RBM  DBN 

， Z = ∑ v , h e

。

。

− E ( v ,h)

e

− E ( v,h )

（4）

Z

， Z 

（Boltzmann

 RBM ， RBM

machine）。

。 RBM ，



E ( v , h ) 。：

P ( v ) = ∑ P ( v, h)

。，

h

，

=∑

。 m、 n 

h

Dm×n ，， m、
 d  X m×d 。 d  n ，
。
 RBM，

e

（5）

− E ( v , h)

Z

 E ( v , h )  − P ( v ) 
。 P ( v ) 
（negative log likelihood）：
min J NLL (W , a , b; v ) = −lbP ( v )

。 v ， h ，

 RBM 


， v ∈ {0,1} ，h ∈ {0,1} ， RBM

：

— RBM，：

⎡ ∂E ( v, h)
∇θ J NLL (W , a, b; v ) = − ⎢
∂θ
⎣⎢

data

，θ ， x

data

E ( v , h ) = −∑ vi ci − ∑ h j b j − ∑ wi , j vi h j
i

j

（1）

i, j

：
E ( v , h ) = −c T v − b T h − v TWh

（2）

−

∂E ( v, h)

 x

∂θ

model

⎤
⎥
⎥
model ⎦
（7）



 x 。

， vi  h j  v 

—，：

 h  i  j 。W 

∇ wij J NLL (W , a , b; v ) = − ⎡ vi h j
⎣

， wij  i 

data

− vi h j

model

⎤
⎦

（8）

 j 。c  b 

 .

，ci  b j  i  j 

model

，

[14]（contrastive divergence，CD）

。
， v ∈ \ ，
， RBM — RBM。
 ( v,h ) ，：

E ( v,h ) =

（6）

1
T
( v − c ) ( v − c ) − bT h − v T wh
2

。 k 

vi h j

k

 vi h j

model

（ k = 1 ）。

（8）：
∇ wij J NLL (W , a , b; v ) = − ⎡⎢ vi h j
⎣

（3）

，

I

0

− vi h j

k

⎤ （9）
⎥⎦

， .  CD  I 。

2018006-4



·38·

 RBM ， RBM 

，： φ ( yl ) − a <

， RBM 

R 2 + ζ l ， α l = 0 ，

 RBM 。 RBM 

  ，       ； φ ( yl ) − a = R 2 + ζ l ，

—，，
 RBM —。DBN 

RBM ， RBM 
。，
，
。， DBN 

2

2

1
，，；
mv
2
1
φ ( yl ) − a > R 2 + ζ l ， α l =
，
mv

0 < αl <

，。

4 

。
3.2

 DBN-SVDD  SVDD 、


 DBN  X m×d ，

OCSVM 。 OCSVM ，
SVDD 。
，SVDD 
。
，。
 a ， R 。
：

min R 2 +
a , R ,ζ

PCA-SVDD  AE ，
 3 
。 UCI ，
。 4 
，：（forest

covertype ， FC ）、
（gas senor array drift，GAS）、（daily and

sport activity，DSA）
（human activity recognition using smart-

1 m
∑ζ l
mv l

phone，HAR）。：54、128、315 
561 。，

s.t. φ ( yl ) − a ≤ R 2 + ζ l
2

∀ l = 1,", m, ζ l ≥ 0

。

（10）

， 70%

， φ ( ⋅) 

，； 30%。

。 v ，，

 5% ，

，。ζ l

20% ，

 l = 1,", m ，

U (0,1) [15]。，

。：

 [0,1] 。

α = [α1 ,", α m ] ， 0 ≤ α l ≤
T

1
mv

（11）

。 3 ，

：
max
α

m

∑α ( y
l =1

l

l

，1 ，−1 
。 PCA ， 95%
[16]。 AE 

⋅ yl ) − ∑ α lα t ( yl ⋅ yt )

，[17]。

l ,t

∀ 1≤ l, t ≤ m

s.t. 0 ≤ α l ≤

1
mv

， DBN 
（12）

DBN 。：DBN1  DBN3 
 1  3 。

2018006-5

·39·


1

2018  1 

3  RBF 





FC

GAS

DSA

HAR

SVDD

95.45%

91.73%

82.66%

86.73%

89.14%

PCA-SVDD

96.55%

95.23%

91.48%

92.34%

93.90%

DBN-SVDD

97.92%

98.29%

96.76%

97.55%

97.63%

2

3 





FC

GAS

DSA

HAR

SVDD

80.34%

83.54%

79.47%

78.62%

80.49%

PCA-SVDD

84.32%

91.45%

79.13%

79.85%

83.69%

DBN-SVDD

98.50%

96.52%

97.14%

98.45%

97.65%

， DBN  2 
。 DBN 
，[18]。
 OCSVM ，
， 3[19]。OCSVM 
“”，
     。         ： RBF  
g (2−15 , 2−9 ,", 23 ) ， C ( 2−5 , 2−4 ,", 215 )

[20]

。

4

（1）

DBN-SVDD 

 1、 2  3、 4，

 DBN-SVDD  SVDD 、 PCA-SVDD

。

（linear）

y  4 

（radical basis function，RBF）。

， 3 

 4 ，

： DBN-SVDD > PCA-SVDD > SVDD ，

，
 1、 2（）

， DBN-SVDD 

 1、 2  3、 4 。

， 97.65%， RBF-PCA-SVDD
 RBF-SVDD  93.90% 89.14%
， 4.65% 8.51%。
y  PCA ，，

 FC、GAS，
；，
 DSA 、 HAR ， PCA 
3

RBF  3  4 

2018006-6

SVDD 。



·40·

y  SVDD  PCA-SVDD ，

AE ，

，

。

，

， 4  5。

。 DBN-SVDD 

 4 ，DBN-SVDD 

，

，

，

 DBN 。

。

 AE ， 0.772 1 s，

y  DBN-SVDD ，

 DBN-SVDD  5.5  RBF  4.4 ，
 DBN-SVDD 。

，
。 DBN 

 5 ，AE 

，

 3.993 0 ms， DBN-SVDD 

。

RBF  SVDD 。 AE ，

（2）

 0.281 3 ms，

 AE  DBN-SVDD 

13.2 ；RBF  0.473 1 ms，

。 DBN-SVDD 

 7.4 。 DBN-SVDD ，

，

 RBF 

，

。 RBF 

 SVDD  1 000 。

，。

 DBN-SVDD  RBF 

 RBF ，

 AE ，

，

 3。

 0.281 3 ms， 0.473 1 ms，

 3 ，AE 

 40.54%。，

 97.24%， DBN-SVDD  RBF 

，

97.63% 97.65%。

。

 3 DBN-SVDD  AE 





RBF


DBN-SVDD



FC

GAS

DSA

HAR

AE

97.83%

97.54%

96.32%

97.25%

97.24%

DBN-SVDD

97.92%

98.29%

96.76%

97.55%

97.63%

98.50%

96.52%

97.14%

98.45%

97.65%

 4 DBN-SVDD  AE （：s）






RBF



FC

GAS

DSA

HAR

DBN-SVDD

0.076 8

0.096 9

0.149 5

0.241 6

0.141 2

DBN-SVDD

0.090 9

0.107 5

0.188 1

0.318 6

0.176 3

AE

0.436 8

0.987 4

0.632 5

1.031 3

0.772 1

2018006-7

·41·



2018  1 

 5 DBN-SVDD  AE （：ms）








DBN-SVDD

RBF

DBN-SVDD

0.182 4

0.359 7

0.473 8

0.876 7

0.473 1

AE

1.537 2

2.243 6

5.569 0

6.622 2

3.993 0

FC

GAS

DSA

HAR

0.121 3

0.183 1

0.258 1

0.562 8


0.281 3

 6  DBN 






FC

GAS

DSA

HAR

DBN1-SVDD

97.35%

97.02%

95.54%

96.76%

96.67%

DBN2-SVDD

98.50%

96.52%

97.14%

98.45%

97.65%

DBN3-SVDD

97.60%

96.87%

97.32%

98.26%

97.54%

（3）
 DBN-SVDD 
， DBN 
。
， 3 
。 1 ， 2 

5

 DBN 

 DBN 。 DBN1 

DBN3 ，

，
。 DBN 

 6。
 5  5 ，
 1  DBN1 “”，
            GAS    

97.02%，，
“”。

DBN3， DBN2  FC 
（ 0.90%），
，

0.18%~0.35%，。 DBN3


，“”。
， DBN2 。
 DBN-SVDD 

RBF  34.9%。 PCA-SVDD ，
 4.65%； AE ，
 1/13。

：
[1] , , , .  k 

，

[J]. , 2015, 31(7): 52-62.

DBN2。， 2  DBN2 

WANG Z W, CHEN Y F, XIAO S Y, et al. An AkNN algorithm
for high-dimensional big data[J]. Telecommunications Science,

。

2015, 31(7): 52-62.
[2] CHANDOLA V, BANERJEE A, KUMAR V. Anomaly detec-

5 

tion:A survey[J]. ACM Computing Surveys, 2009, 41(3): 1-58.



[3] SHIN H J, EOM D H, KIM S S. One-class support vector ma-

， DBN-SVDD 。
2018006-8

chines—an application in machine fault detection and classification[J]. Computers & Industrial Engineering, 2005, 48(2): 395-408.



·42·

[4] , , . [J].

Computing, 2014 IEEE, Intl Conf on and IEEE, Intl Conf on

, 2010, 36(21): 34-36.

and Autonomic and Trusted Computing, and IEEE, Intl Conf on

LI X, QIAN X, WANG Z Q. Efficient data mining algorithm for

Scalable Computing and Communications and ITS Associated

high-dimensional outlier data[J]. Computer Engineering, 2010,

Workshops, Dec 9-12, 2014, Bali, Indonesia. Piscataway: IEEE

36(21): 34-36.

Press, 2014: 855-858.

[5] TENENBAUM J B, DE S V, LANGFORD J C. A global geo-

[18] HINTON G E. A practical guide to training restricted Boltzmann

metric framework for nonlinear dimensionality reduction[J].
Science, 2000, 290(5500): 2319.

machines[M]. Berlin: Springer Berlin Heidelberg, 2012: 599-619.
[19] YANG J, DENG T, SUI R. An adaptive weighted one-class svm

[6] POMERANTSEV A L. Principal component analysis(PCA)[M].

for robust outlier detection[M]. Berlin: Springer Berlin Heidel-

New York: John Wiley & Sons, Inc., 2014: 4229-4233.

berg, 2016.

[7] ROWEIS S T, SAUL L K. Nonlinear dimensionality reduction

[20] LIN C J. A practical guide to support vector classification[EB/OL].

by locally linear embedding[J]. Science, 2000, 290(5500): 2323.

(2003-01-31)[2017-06-21]. http://www.researchgate.net/publication/

[8] GONZALEZ I, DÉJEAN S, MARTIN P G P, et al. CCA: an R

200085999_A_Practical_Guide_to_Support_Vector_Classication.

package to extend canonical correlation analysis[J]. Journal of
Statistical Software, 2008, 23(12).
[9] CHENOURI S, LIANG J, SMALL C G. Robust dimension

[]

reduction[J]. Wiley Interdisciplinary Reviews Computational

（1992−），，

Statistics, 2015, 7(1): 63-69.

，。

[10] , , , . 
[J]. , 2015, 31(8): 78-83.
CHENG H, FANG J L, WANG D Q, et al. Performance analysis
of simplification of hyperplane support vector machine[J]. Telecommunications Science, 2015, 31(8): 78-83.
[11] GEORGE A. Anomaly detection based on machine learning
dimensionality reduction using PCA and classification using
SVM[J]. International Journal of Computer Applications, 2012,
47(21): 5-8.
[12] BAO S, ZHANG L, YANG G. Trajectory outlier detection me-

（1978−），，，
、，
。

thod based on kernel principal component analysis[J]. Journal
of Computer Applications, 2014, 34(7): 2107-2110.
[13] SAKURADA M, YAIRI T. Anomaly detection using autoencoders with nonlinear dimensionality reduction[C]//Mlsda
Workshop on Machine Learning for Sensory Data Analysis,
December 2, 2014, Gold Coast, Australia QLD, Australia. New

（1971−），，，

York: ACM Press, 2014: 4-11.

、，

[14] HINTON G E. Training products of experts by minimizing

。

contrastive divergence[J]. Neural Computation, 2002, 14(8):
1771-1800.
[15] SUBRAMANIAM S, PALPANAS T, PAPADOPOULOS D, et
al. Online outlier detection in sensor data using non-parametric
models[C]//International Conference on Very Large Data Bases,
September 12-15, 2006, Seoul, Korea. New York: ACM Press,

（1992−），，

2006: 187-198.

，

[16] MOORE B. Principal component analysis in linear systems:
controllability, observability, and model reduction[J]. IEEE
Transactions on Automatic Control, 2003, 26(1): 17-32.
[17] HU C, HOU X, LU Y. Improving the architecture of an autoencoder for dimension reduction[C]//Ubiquitous Intelligence and

2018006-9

。