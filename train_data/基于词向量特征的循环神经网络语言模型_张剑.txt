第卷第期年月模式识别与人工智能基于词向量特征的循环神经网络语言模型张剑屈丹李中国人民解放军信息工程大学信息系统工程学院摘要真郑州循环神经网络语言模型能解决传统模型中存在的数据稀疏和维数灾难问题但仍缺乏对长距离信息的描述能力为此文中提出一种基于词向量特征的循环神经网络语言模型改进方法该方法在输入层中增加特征层改进模型结构在模型训练时通过特征层加入上下文词向量增强网络对长距离信息约束的学习能力实验表明文中方法能有效提高语言模型的性能关键词语音识别语言模型循环神经网络词向量中图法分类号国家计划项目国家自然科学基金项目资助收稿日期修回日期年生硕士研究生主要研究方向为语音识别自然语言处理屈丹作者简介张剑男年生博士副教授主要研究方向为语音识别智能信息处理李真女年生硕士讲师主要研究方向为语女音识别智能信息处理模式识别与人工智能引卷增强网络对长距离信息的学习提高语言模型的精度该方法的优势是可在神经网络中较方便地利用言语言模型是自然语言处理中最重要的组成部分在语音识别机器翻译信息检索等领域有着广泛的应用其中最具代表性的是语言模型但目前模型的发展逐渐陷入瓶颈性能提高缓慢且代价昂贵因此神经网络语言模型等高级语言模型逐渐成为研究的热点随着深度学习理论的不断发展神经网络在自然语言处理尤其是语言模型中的应用开始受到关注等率先提出利用神经网络构建语言模型的方法通过对词的分布式表达巧妙解决数据稀疏对统计建模的影响同时克服模型参数的维数灾难问题其训练的神经网络语言模型无需使用传统模型中复杂的平滑算法具备较好的模型性能在数据集上的对比实验表明相比平滑的模型的性能有明显的提高其他复杂的高级模型这在传统的类模型中经常使用到如引入语言学信息和主题特征等本文利用连续词袋模型和模型获取上下文相关的词向量改进原有循环神经网络语言模型并在数据库上进行困惑度测试在微软语料库和语料库上进行语音识别实验实验表明本文方法可提高神经网络语言模型性能有效降低语言模型在测试集上的困惑度对连续语音识别系统的性能也有较大幅度的提高神经网络模型结构的网络结构包含输入层隐含层和输出层个部分输入向量代表时刻时输入词采用编码也称为编码维数与词汇表大小相同隐含层为网络的状态表示时刻时上下文的历史信息输出向量表示待预测词在词但前馈神经网络语言模型在输入层上仍采用个词作为历史信息并未解决长距离信息建模能力差的问题为此等利用循环神经汇表上的概率分布分别为各层之间的权值矩阵网络训练语言模型通过隐含层的循环获得更多的上下文信息同时降低模型的参数个数称为循环神在时刻网络的输入由当前词向量和前一时刻的隐含层输出构成联合计算下一经网络语言模型并进行一系列改进的优势在于能利用更多的个隐含层通过隐含层循环的方式可利用更长的上下文信息更好地表示自然语言其性能优于传统的上下文信息进行词的预测对语言具有更好的建模能力但文献的理论研究表明循环神经网络中前馈神经网络语言模型近年来被大量使用于语音识别机器翻译语言理解等领域存在消失梯度问题使得基于梯度的训练算法对长距离信息的学习变得困难模在实际应用中循环神经网络的消失梯度问题使网络不能较好学习较长距离的信息其影响在传型性能分析表明的性能类似于为或的超长距离前馈神经网络模型递过程中较易丢失这是因为在循环网络中时间上因此如何提高对长距离上下文的学习能力需更进一步的研究目前学者们进行的研究主要有种优化训练算法利用高阶信息避免消失梯度问题使误差传递的层数更多达到学习更长距离信息的目的如优化方法利用改进神经网络采用门控神经元锁存多重时序中的误差信号传递隔得较远的输入的微小变化对网络训练几乎不会产生影响即使远距离输入发生较大变化时其产生的影响仍存在不能被梯度检测到的问题这就使得在一些特定情况下基于梯度的训练算法对长距离依赖的学习变得困难本文采用的是一种类似模型结合的方法通过将上下文相关词向量直接加入到原有神经网络中提高网络对长距离信息的学习能力避免网络训练中的消失梯度问题因此本文改进原有网络结构在输入层中增加一个特征层并通过权值矩阵本文从模型结合的角度研究上述问题提出基于词向量特征的循环神经网络语言模型通过计算分别与隐含层和输出层相连网络结构如图所示基于长距离历史的上下文词向量并将其作为网络的一个输入特征进行神经网络语言模型的训练中此时网络的输入变为期张剑等基于词向量特征的循环神经网络语言模型在特征层中输入向量为词的上下文相关向量其中包含更多的长距离信息是对输入向量提出通常称为种表示法最早由这种方法将词用一种低维实数向量的一个补充可使词概率的计算更准确采用随机梯度下降法训练网络此时输出向量表示待预测词在给定当前词上下文和特表示优点在于相似的词在距离上更接近能体现出不同词之间的相关性从而反映词之间的依赖关系征向量下的概率分布改进后的计算同时较低的维度也使特征向量在应用时有一个可接受的复杂度因此新近提出的许多语言模型如公式为潜在语义分析模型和潜在狄利克雷分布其中为激活函数为激活函数模型及目前流行的神经网络模型等都采用这种方法表示词向量连续词袋模型和模型本文选择模型和模型获取上下文相关词向量这两种模型由等提出旨在以较小的计算复杂度获取词的分布式表示在传统神经网络模型的基础上针对训练复杂度过大的问题进行改进采用一种模型结构去除神经网络的非线性隐含层同时将神经网络中模型的训练和词向量的计算分开提高训练效率其模型结构如图所示图结合特征层的结构词向量特征词向量定义在自然语言处理中要将自然语言理解的问题转化为机器学习的问题就需将自然语言的符号数学化其中最直观和常用的方法是表示法这种方法将每个词表示为一个很长的向量其维数是词汇表大小其中绝大多数元素为只有一个维度的值为这个维度就代表当前的词在自然语言处理中常将表示采用稀疏的方式进行存储即为每个词分配一个数字该方法因其简单易用广泛应用于各种自然语言处理任务中如模型中就采用这种词向量表示图和模型结构从结构图中可看出的模型架构类似于前馈神经网络模型但去除隐含层只包括输入层投影层和输出层输入层输出层表示每个词的词向量均采用分布式表示维数一般为或投影层维数为窗长表示上下文长度在训练时与前馈神经网络模型的区别如下输法但这种表述方法也存在一定问题其表示的任意入层的词向量在投影层上的投影不再按顺序排列而是进行合并采用均值表示单个词向量达到降低两个词之间是孤立的无法表示这两个词之间的依赖关系从词向量上看不出两个词是否相关采用稀计算量的目的词在历史信息中的顺序不影响其在投影层中的表示因此这种结构称为连续空间中的疏表示法在处理某些任务如构建模型时会引起维数灾难问题词袋模型此外由于无需进行语言模型概率的计算模型可利用未来的信息等训练而在深度学习中一般采用分布式表示的方法表示词向量这当前词真正实现根据上下文得到最优的词向量模式识别与人工智能模型的结构则与模型相反其采用当前词训练上下文的词向量目的是得到有利于预期周围词的词向量用以表征上下文信息可理解为根据上下文分类当前词这两种词向量各有优势模型在语法测试中准确性更高表明其通过对上下文的学习获得卷验最终选取窗长为的词向量训练语言模型在这一窗长下词向量不仅可完整表示一句话内的词关联信息也能有效表示句子间的上下文信息得到的语言模型性能最优实验中训练组隐含层神经元个数采用或类别层个数为并在开发集和测试更多的语法信息模型在语义测试中性能更好说明其词向量的区分性更好对单个词的信息集上分别计算模型的困惑度实验结果如表所示为对比语言模型性能还采用平滑的描述更准确模型这两种模型的共同优点在于能从规模数据中得到高质量的词向量等向量维数为时可在十亿级的数据量下完成词向量的训练这在以往是难以想象的对海量数据的有效利用使所得词向量具有更高的精度能更好描述对不同词之间的相关性同时这两个模型得到的词向量也能描述词和短语之间的相关性对句子中存在的长距离的词之间的关系进行有效表达实验及结果分析为验证本文语言模型的性能通过一系列的实验进行验证包括语料库上的困惑度实验在微软语料库和语料库上的连续语音识别实验实验采用的评价指标是语言模型的困惑度和语音识别系统的词错误率具体的实验设置及实验结果如下困惑度实验表的实验表明在词实验设置语料库是国际上广泛采用的一个英文树库由宾夕法尼亚大学建立常用来进行词性标注句法分析等自然语言处理任务在语言模型领域也被广泛采用在实验时采用相同的实验设置相同的训练开发和测试数据及相同的词汇限制这也有利于对比不同的语言模型技术实验中语料划分如下节为训练集包含万个节为确认数据作为开发集包含万个节为测试集包含万个词汇表大小限制为万集外词采用标号表示实验结果实验中采用模型和模型在训语料库上困惑度实验结果语言模型困惑度开发集测试集表中表示采用平滑的模型和分别表示隐含层神经元个数采用和的循环神经网络模型和分别代表词向量维数为的模型分别表示采用两种模型进行词向量扩展的循环神经网络模型从表中可看出本文方法可提高神经网络语言模型的性能其在开发集和测试集上的困惑度均有明显降低在隐含层神经元个数较小隐含层大小为时结合词向量对模型性能的提高更明显在开发集上模型相比原有模型困惑度由降至下降采用模型时困惑度降低到下降幅度更大达到而当隐含层神经元个数增多时随着原有模型训练更充分结合词向量特征对性能的提高进一步减弱在开发集上两种方法相比原有模型困惑度分别下降和为进一步验证本文方法的性能将本文方法与其他语言模型对比在相同的实验条件下选取文献中的结果对比的隐含层大小为词向量的维数为窗长为实验中每个模型除练集上训练上下文相关的词向量维数为在窗长范围为的条件下选取不同的窗长进行实单独测试困惑度外还与平滑的模型进行结合计算在测试集上的困惑度具体结果期张剑等基于词向量特征的循环神经网络语言模型如表所示表实验中声学特征采用维参数及其一阶和二阶差分共维特征矢量语音信号采用不同语言模型在测试集上的性能对比语言模型困惑度窗处理帧长帧移声学模型为有调音节的声韵模型共个模型基元采用三状态自左向右无跨越的隐马尔科夫模型由于语音中存在协同发音现象在声韵母结构上上下单个文相关三音子的声韵母结构语言模型的训练数据为语料库的语音标注文件采用工平滑最大熵模型随机森林模型结构化模型具包训练得到平滑的和模型前馈经神经网络模型句法神经网络模型循环神经网络语言模型表中列出多种不同语言模型的实验结果模型由工具包训练得到采用目前流行的两种平滑算法其中平滑的性能优于平滑最大熵模型随机森林模型和结构化模型是模型的一些扩展采用不同方法改进模型而神经网络类模型则由前馈神经网络模型句法神经网络模型和循环神经网络语言模型组成通过实验对比发现神经网络类语言模型的性能普遍优于类模型其中句法神经网络模型本实验采用工具箱搭建连续语音识别系统基线系统采用在基于加权有限状态机的解码器中构建静态解码网络进行一次解码利用工具提取列表在二次解码中采用平滑的和循环神经网络语言模型进行重打分得到最终的识别结果采用测试集上的词错误率作为系统的评价指标在实验中的隐含层个数为和词向量的维数为窗长为将模型送入解码器进行实验表给出不同语言模型的系统识别结果表不同语言模型在微软语料库上的词错误率语言模型基线系统词错误率模型困惑度进一步下降模型在两项测试中的困惑度和均为最低具备最好的模型性能进一步说明本文方法的有效性和循环神经网络模型由于能表示更多的长距离依赖关系具备更好的模型性能而被采用本文方法改进连续语音识别实验在该章节中选取两个国际上常用的语料库微软语料库和语料库进行连续语音识别实验验证本文语言模型在连续语音识别系统中的性能针对两个语料库的实验设置和实验结果如下从表可看出结合模型改进的的系统具有最低的识别词错误率从识别结果看在隐含层为时系统识别的词错误率为与原有相比性能提高与基线系统相比相对提高在隐含层为时模型的识别词错误率为在语言模型基础上提高与个汉语连续语音识别语料库其训练集由个年龄在岁间的男性录音组成共句约基线系统相比相对提高说明词向量的加入可使的识别性能进一步提高的语音数据测试集为另外个男性录音共句语音采样频率为采用线性脉而模型特征对语言模型的连续语音识别结果并未有明显改善还需进一步的实验验证其冲编码调制量化性能基于微软语料库的实验微软语料库是一模式识别与人工智能基于语料库的实验语料库是一个国际上广泛使用的英文语料库适用于进行大词汇量连续语音识别实验其训练集包含和两部分共语音数据测试集由和组成实验设置与微软语料库类似针对语音识别任务进行实验采用数据训练得到传统模型利用自动生成的问题集进行三音子状态聚类共个不同的绑定状态语言模型训练数据包含的在不同的隐含层个数下训练神经网络模型观察词向量对模型性能的影响采用的词向量维数为窗长为训练选择的隐含层大小分别为和不同循环神经网络模型在开发集上的困惑度如图所示卷更好的识别效果在和测试集上其识别性能在原有基础上进一步提高在测试集上隐含层个数为时结合词向量的识别的词错误率为相比和基线系统分别提高和隐含层个数为时词错误率为相比和基线系统分别提高和在测试集上可得到类似的实验结果表不同语言模型在语料库上的词错误率语言模型基线系统模型对系统的识别性能也有明显改善在测试集上隐含层为时其词错误率为相比和基线系统分别提高和隐含层为时词错误率相比和基线系统分别提高和图不同隐含层的在有效数据上的困惑度与微软语料库的实验结果相比本文方法在语料库上对系统识别性能的提升更明显这说明在语料库数据更大时词向量的训练更充分对长距离上下文信息的描述也更准确对语言模型的性从图可看出两种模型词向量的引入都能明显降低的困惑度其中模型的性能提高也更大尤其是模型在大词汇量语音识别系统中其性能相比小词汇量识别系统能略优于模型当隐含层较小时这种方法对模型困惑度的降低更明显而随着神经元个数的有明显提高证明此方法的有效性也说明训练数据量的大小对词向量的性能有一定影响增大加入特征向量使模型性能的提高幅度减小这说明在隐含层较小时特征向量的加入对网络的影为更准确对比不同识别系统的性能排除测试数据随机性对实验结果造成的干扰得到更可靠的响更大可更好提高模型的性能随着网络本身训练更充分影响开始减弱结论在上对不同模型的识别结果进行统计假设检验利用提供的工具包对词错误率为进一步验证本文方法的性能选择隐含层个数为和的语言模型进行语音识别实验采用进行显著性测试包括配对句子分词错误率重打分进行二次解码根据在和上的词错误率对比不同模型对识别性能的测试简称测试符号成对比较说话人词准确率影响实验结果如表所示表中基线系统为一表示采用模型重次解码的识别结果测试简称测试符号秩说话人词准确率打分的识别结果从表可看出本文方法训练的语言模型具有测试简称测试测试结果如表所示表中的每项给出对应的两种语言模型的测试期张剑等基于词向量特征的循环神经网络语言模型结果其中表示在测试中词错误率更低表示在测试中模型的词错误率更低识别结果的不同是由于测试数据的随机性造成的从表中可看种显著性测试结果均表明在的显著性水出平下本文提出的两种词向量改进的模型均优于传统的模型和原有模型肯定本文方法的有效性表不同语言模型词错误率的显著性测试结果传统结束语本文提出一种结合词向量特征的循环神经网络语言模型改进方法通过引入词关联信息的方法解决模型中存在长距离信息的学习问题改善语言模型的性能并进行困惑度测试和连续语音识别实验实验表明本文方法可有效改善语言模型性能降低识别系统的平均词错误率在今后的研究工作中将重点关注如下方面提高词关联信息的准确率考虑更长的关联信息句子关联信息的有效利用参考文献