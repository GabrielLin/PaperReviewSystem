第 ４３ 卷 　 第 ６ 期
２０１６ 年 ６ 月

计 算 机 科 学
ｔ
ｅ
ｒ　Ｓｃ
ｉ
ｅｎｃ
ｅ
Ｃｏｍｐｕ

Ｖｏ
ｌ．
４３Ｎｏ．
６
Ｊｕｎｅ
２０１６
　

基于 Ｗｏ
ｒ
ｄ２Ｖｅ
ｃ的一种文档向量表示
唐 　 明 　 朱 　 磊 　 邹显春
（西南大学计算机与信息科学学院 　 重庆 ４００７１５）
　
摘 　 要 　 在文本分类中，如何运用 ｗｏ
ｒ
ｄ２ｖｅ
ｃ词向量高效地表达一篇文档一直是一个 难 点。 目 前，将 ｗｏ
ｒ
ｄ２ｖｅ
ｃ模 型 与
聚类算法结合形成的 ｄｏ
ｃ
２ｖｅ
ｃ模型能 有 效 地 表 达 文 档 信 息。 但 是，这 种 方 法 很 少 考 虑 单 个 词 对 整 篇 文 档 的 影 响 力。
为了解决这个问题，利用 ＴＦ－ＩＤＦ 算法计算每篇文档中词的权重，并 结 合 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 词 向 量 生 成 文 档 向 量，最 后 将 其 应
用于中文文档分类。在搜狗中文语料库上的实验验证了新方法的有效性。
关键词 　ＴＦ－ＩＤＦ，
ｗｏ
ｒ
ｄ２ｖｅ
ｃ，
ｄｏ
ｃ
２ｖｅ
ｃ，文本分类
中图法分类号 　ＴＰ１８１ 　　　 文献标识码 　Ａ　　　ＤＯＩ　１０．
１１８９６／
ｉ
ｓ
ｓｎ．
１００２－１３７Ｘ．
２０１６．
６．
０４３
ｊ．
　

Ｄｏ
ｃｕｍｅｎ
ｔ　
Ｖｅ
ｃ
ｔ
ｏ
ｒ　
Ｒｅ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ　
Ｂａ
ｓ
ｅ
ｄ　
ｏｎ　
Ｗｏ
ｒ
ｄ２Ｖｅ
ｃ
ｐｒ
ＴＡＮＧ　
Ｍｉ
ｎｇ　ＺＨＵ　
Ｌｅ
ｉ　ＺＯＵ　
Ｘｉ
ａｎ－ｃｈｕｎ
（
Ｓｃｈｏｏ
ｌ
ｏ
ｆ　
Ｃｏｍｐｕ
ｔ
ｅ
ｒ
ａｎｄ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｓｃ
ｉ
ｅｎｃ
ｅ，
Ｓｏｕ
ｔ
ｈｗｅ
ｓ
ｔ　
Ｕｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
Ｃｈｏｎｇｑ
ｉ
ｎｇ　
４００７１５，
Ｃｈ
ｉ
ｎａ）
　
　
　
ｙ，

　
ｂｙ
ｔ
ｈｅ　
ｗｏ
ｒ
ｄ　
ｖｅ
ｃ
ｔ
ｏ
ｒ
ｏ
ｆ　
ｗｏ
ｒ
ｄ２ｖｅ
ｃ．
Ａｔ
ｓ
ｔ
ｒ
ａ
ｃ
ｔ　Ｉ
ｎ
ｔ
ｅｘ
ｔ
ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ
ｉ
ｓ
ｓｕｅ
ｓ，
ｉ
ｔ
ｉ
ｓ　
ｄ
ｉ
ｆ
ｆ
ｉ
ｃｕ
ｌ
ｔ
ｔ
ｏ　
ｅｘｐ
ｒ
ｅ
ｓ
ｓ
ａ　
ｄｏ
ｃｕｍｅｎ
ｔ
ｅ
ｆ
ｆ
ｉ
ｃ
ｉ
ｅｎ
ｔ
ｌ
Ａｂ
　
　
　
　
　
　
　
　
ｙ　
　
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍ　
ｃ
ａｎ　
ｅｘｐ
ｒ
ｅ
ｓ
ｓ
ｔ
ｈｅ
ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
ｏ
ｆ
ｄｏ
ｃｕｍｅｎ
ｔ
ｒ
ｅ
ｓ
ｅｎ
ｔ，
ｄｏ
ｃ
２ｖｅ
ｃ　
ｂｕ
ｉ
ｌ
ｔ
ｏｎ
ｔ
ｈｅ
ｃ
ｏｍｂ
ｉ
ｎａ
ｔ
ｉ
ｏｎ　
ｏ
ｆ　
ｗｏ
ｒ
ｄ２ｖｅ
ｃ
ａｎｄ　
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　
　
　
　
　
　
　
　
ｇｏ
ｐ
’
，
ｃ
ｏｎｓ
ｉ
ｄｅ
ｒ
ｓ
ａ
ｓ
ｉ
ｎｇ
ｌ
ｅ　
ｗｏ
ｒ
ｄｓ
ｉ
ｎ
ｆ
ｌ
ｕｅｎｃ
ｅ
ｆ
ｏ
ｒ
ｔ
ｈｅ
ｅｎ
ｔ
ｉ
ｒ
ｅ　
ｄｏ
ｃｕｍｅｎ
ｔ．
Ｔｏ　
ｓ
ｏ
ｌ
ｖｅ
ｔ
ｈ
ｉ
ｓ　
ｒ
ｏ－
ｗｅ
ｌ
ｌ．
Ｈｏｗｅ
ｖｅ
ｒｔ
ｈ
ｉ
ｓ　
ｍｅ
ｔ
ｈｏｄ
ｒ
ａ
ｒ
ｅ
ｌ
ｖｅ
ｒ
　
　
　
　
　
　
　
　
ｐ
ｙ　
ｙ　
ｌ
ｅｍ，
ｉ
ｎ
ｔ
ｈ
ｉ
ｓ　
ｒ，
ＴＦ－ＩＤＦ　
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍ　
ｗａ
ｓ　
ｕｓ
ｅｄ
ｔ
ｏ　
ｃ
ａ
ｌ
ｃｕ
ｌ
ａ
ｔ
ｅ
ｔ
ｈｅ
ｒ
ｉ
ｔ　
ｗｅ
ｉ
ｔ
ｏ
ｆ　
ｗｏ
ｒ
ｄｓ
ｉ
ｎ　
ｄｏ
ｃｕｍｅｎ
ｔ
ｓ，
ａｎｄ　
ｗｏ
ｒ
ｄ２ｖｅ
ｃ　
ｗａ
ｓ
ｂ
　
　
　
　
　
　
ｐａｐｅ
ｇｏ
ｇｈ
ｇｈ
，
ｃ
ｏｍｂ
ｉ
ｎｅｄ
ｔ
ｏ　
ｒ
ａ
ｔ
ｅ　
ｄｏ
ｃｕｍｅｎ
ｔ
ｖｅ
ｃ
ｔ
ｏ
ｒ
ｓ ｗｈ
ｉ
ｃｈ　
ｗｅ
ｒ
ｅ　
ｕｓ
ｅｄ
ｆ
ｏ
ｒ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ
ｔ
ｅｘ
ｔ
ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ．
Ｅｘｐｅ
ｒ
ｉｍｅｎ
ｔ
ｓ　
ｏｎ
ｔ
ｈｅ　
Ｓｏｇｏｕ　
Ｃｈ
ｉ－
　
　
　
　
　
　
ｇｅｎｅ
ｏ
ｆ
ｔ
ｈ
ｉ
ｓ　
ｎｅｗｌ
ｒ
ｏｐｏ
ｓ
ｅｄ　
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍ．
ｄｅｍｏｎｓ
ｔ
ｒ
ａ
ｔ
ｅ
ｔ
ｈｅ
ｅ
ｆ
ｆ
ｉ
ｃ
ｉ
ｅｎｃｙ　
ｓ
ｅ
ｃ
ｏ
ｒ
ｌ
ａｂｏ
ｒ
ａ
ｔ
ｏ
ｒ
ｎｅ
　
　
　
　
　
ｙ　
ｐ
ｇｏ
ｐｕｓ
ｙ　
，
，
，
Ｋｅ
ｒ
ｄ
ｓ　ＴＦ－ＩＤＦ Ｗｏ
ｒ
ｄ２ｖｅ
ｃ Ｄｏ
ｃ
２ｖｅ
ｃ Ｔｅｘ
ｔ
ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ
　
ｙｗｏ
　

１　 引言
目前，使用 最 广 泛 的 文 档 表 示 方 法 几 乎 都 基 于 词 袋 法
［ ，］
（
Ｂａｇ－ｏ
ｆ－Ｗｏ
ｒ
ｄ，
ＢＯＷ）１ ２ 。词袋法将文 档 看 成 是 一 些 词 的 集

的值为 ０，但是我们知道“土 豆”与“马 铃 薯”是 同 一 种 食 物；
４）
词袋法很难区分同一个词在不同语境中的意义，如“先生”，根
据上下文，它可能是对男性的称呼，也可能是古代对老师的 称
呼，但在词袋法中，其文档向量计算相似度为 １。

合，在该集合中，每个 词 的 出 现 是 相 互 独 立 的，且 不 考 虑 词 的

随着深度学习的发展 ［１０，１１］，基 于 神 经 网 络 的 自 特 征 抽 取

顺序、语法和语义等 信 息。 它 将 一 篇 文 档 表 示 成 与 训 练 词 汇

的词向量表示方法越来越受工业界和学术界的关注。基于 前

集 合 相 同 维 度 的 向 量 ，向 量 中 每 个 位 置 的 值 即 是 该 位 置 所 代

［］
［］
人 的研究，Ｍｉ
ｋｏ
ｌ
ｏｖ 等人 ６ 在 ２０１３ 年提出了 ｗｏ
ｒ
ｄ２ｖｅ
ｃ模型 ７

表的词在文档中出现的次数，并且随着新词汇的增加，文档 向

用于计算词 向 量 （即 下 文 的 Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ，后 面

量维度也会增加。虽然词袋法在传统分类器上的分类效果 不

均简称为词向量）。ｗｏ
ｒ
ｄ２ｖｅ
ｃ模型 利 用 词 的 上 下 文 信 息 将 一

错，比如目 前 比 较 成 熟 的 分 类 技 术：回 归 模 型、最 近 邻 分 类

个 词 转 化 成 一 个 低 维 实 数 向 量 ，越 相 似 的 词 在 向 量 空 间 中 越

（
ＫＮＮ）、贝 叶 斯 分 类、决 策 树、ＲＢＦ 神 经 网 络、支 持 向 量 机

相近。将词向量应用 于 自 然 语 言 处 理 非 常 成 功 ，已 经 被 广 泛

［ ］
（
ＳＶＭ）等 ３－５ ，但它依 旧 存 在 几 个 主 要 问 题 ：
１）维 度 太 高，文

［ ］
［ ， ， ］
应用 于 中 文 分 词 ［１２，１３］、
Ｔａｇｇ
ｉ
ｎｇ１４ 、情 感 分 类 １０ １１ １５ 、句
ＰＯＳ　

本向量的维数 与 训 练 数 据 集 中 出 现 的 所 有 单 词 的 数 目 一 样

法依存分析 ［１０，１６］等。

多，这样容易出现所谓的“维度灾难”现象，而且如果某一 个 词

然而一篇文档由无 数 词 构 成，如 何 利 用 词 向 量 有 效 地 表

汇 在 训 练 集 中 没 有 出 现 过 ，则 该 词 汇 在 测 试 集 中 出 现 时 就 无

示一篇文档是当前的一个难点。目前在这方面的研究进展 缓

法成为该文本的特 征；
２）一 篇 普 通 文 档 只 有 １０００ 个 词 左 右，

慢，常见的 方 法 有 对 一 篇 文 档 所 包 含 的 所 有 词 向 量 求 平 均

而词向量的维 度 却 能 达 到 １０ 万，利 用 率 仅 为 １％ ，所 以 基 于

［ ］
值 ［１７］、对词 向 量 聚 类 ［１８］以 及 ｄｏ
ｃ
２ｖｅ
ｃ 模 型 １９ 。 但 这 些 方 法

ＢＯＷ 表示的文档向量非常稀疏，不利于一些自然语言处理 任

并未重视单个词对 整 个 文 档 的 影 响 力。 针 对 这 个 问 题，本 文

务；
３）词袋法无法很好地表示一篇文档的语义，它假设词 与 词

［］
在 ｗｏ
ｒ
ｄ２ｖｅ
ｃ的基础上，利用 ＴＦ－ＩＤＦ 算 法 ８ 对 每 篇 文 档 中 的

之间相互独立，并不 考 虑 词 与 词 之 间 的 关 系，如 “土 豆”与 “马

分词进行加权，并在搜狗中文实验语料库上进行测试，测试 结

铃薯”这两个词在用词 袋 法 所 表 示 的 文 档 向 量 计 算 相 似 度 时

果验证了该方法的有效性。

到稿日期：
２０１６－０１－１９ 　 返修日期：
２０１６－０４－２０　　
唐 　 明（
ｉ
ｌ：
ｔ
ａｎｇｍｉ
ｎｇ＠ｓｗｕ．
ｅｄｕ．
ｃｎ；朱 　 磊 （
１９９２－ ），男，硕 士 生，主 要 研 究 方 向 为
１９７４－ ），男，硕士，工程师，主要研究方向 为 数 据 挖 掘，
Ｅ－ｍａ
机器学习；邹显春（
１９６５－ ），男，硕士，副教授，主要研究方向为数据挖掘、机器学习。

· ２１４ ·

Ｈｕ
ｆ
ｆｍａｎ 树。在这棵 Ｈｕ
ｆ
ｆｍａｎ 树 中，叶 子 节 点 共 Ｎ （＝｜Ｄ｜）

２　 相关工作

个，分别对应词典 Ｄ 中的词，非叶子节点 Ｎ －１ 个。通过随机

２．
１　 词的向量化

梯 度上升算法对 Ｘｗ 的结果进行预测，使得 ｐ（
ｏｎ
ｔ
ｅｘｔ（
ｗ｜
ｃ
ｗ））

词的向量化就是将 语 言 中 的 词 进 行 数 学 化，也 即 把 一 个
词表示成一个向量。词的向量化主要有以下 ３ 种表达方式。
（
１）
ｏｎｅ－ｈｏ
ｔ
ｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ 方式
　

值最大化，
ｏｎ
ｔ
ｅｘｔ（
ｃ
ｗ）指词的上下文中的 ２
ｃ 个词。
当神经网络 训 练 完 成 时，即 可 求 出 所 有 词 的 词 向 量 ｗ。
有趣的是，当利用词向量表示一个词时，可以发现类似这样 的

这是一种最简单的 方 式，用 一 个 很 长 的 向 量 来 表 示 一 个

［ ］
规律：“
ｋ
ｉ
ｎｇ”－ “ｍａｎ”＋ “
ｗｏｍａｎ”＝ “
ｅｎ”２２ ，可 以 看 出 词
ｑｕｅ

词。向量的长度为 词 典 的 大 小 （通 常 达 到 １０ ），向 量 的 分 量

向量非常有利于表达词的语义特征。

只有一个 １，其余全为 ０，
１ 的位 置 对 应 该 词 在 词 典 中 的 位 置 。

２．
２　 文档的向量化

５

比如，“土 豆”表 示 为 ［
０　０　０　０　０　１　０　０　０　０　０　０　０　０

（
１）
ＢＯＷ 模型

０ … ］，而 “马 铃 薯 ”表 示 为 ［
０　１　０　０　０　０　０　０　０

文档的向量化就是 将 一 篇 文 档 表 示 为 一 个 向 量 ，主 要 是

０ ０ ０ ０ ０ ０ …］。这种方式 虽 然 可 以 简 单 明 了 地 表 达 一
个词语，但是却无法有效表达它们的语义信息。“土豆”和“马
铃薯”虽然是同一种 食 物，但 利 用 常 规 的 向 量 距 离 公 式，比 如
欧 几 里 德 距 离 或 者 余 弦 距 离 公 式 ，都 无 法 有 效 计 算 它 们 的 相
似度，显然这种方式不能很好地表达词之间的相似性。
（
２）
Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ
ｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ（词向量）
　
这种方式 能 很 好 地 克 服 ｏｎｅ－ｈｏ
ｔ
ｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ 方 式 的 缺
　
［］
点，最早由 Ｈｉ
ｎ
ｔ
ｏｎ９ 提出，它是将词 映 射 到 一 个 低 维、稠 密 的

实数向量空间中（空间大小一般为 １００ 或者 ２００），使得词义越
相近的词在空间的距 离 越 近。上 面 的 例 子 可 以 类 似 地 表 达 如
“土 豆 ”可 以 表 示 为：［
下，
０．
８４３ －０．
１２５　０．
７３４ －０．
３４５
０．
６５４ …］，而“马 铃 薯”为 ［
９２３ －０．
２３１　０．
６９８ －０．
２３３
０．

基于词的向量化。将 文 档 向 量 化 之 后，就 可 以 利 用 常 规 的 距
离向量公式比较两篇文档 之 间 的 相 似 度。传 统 的 ＢＯＷ 可 以
看作是词的 ｏｎｅ－ｈｏ
ｔ表示向量的叠加，比如 ２．
１ 节中“土豆”的
词向量为［
０　０　０　０　０　１　０　０　０　０　０　０　０　０　０ … ］，
“马 铃 薯 ”的 词 向 量 为 ［
０　１　０　０　０　０　０　０　０　０　
０　０　０
０ ０ …］，而一篇仅包含“土豆”和“马铃薯”这两个词的文本就
可以 表 示 为 ［
０　１　０　０　１　０　０　０　０　０　０　０　０　０　０ …］。
显 然 这 种 表 达 方 式 存 在 的 问 题 与 ｏｎｅ－ｈｏ
ｔ一 样，由 于 其 特 征
向 量 的 高 维 性 和 稀 疏 性 ，很 难 利 用 常 规 的 向 量 距 离 公 式 有 效
地计算两篇文档之间的相 似 度。当 然 传 统 的 ＢＯＷ 也 有 许 多
优化的方法，利用 ＴＦ－ＩＤＦ 加权是 其 中 一 种，即 将 文 本 向 量 中

０．
７４３ …］，显然，这种表示方式 有 利 于 使 用 距 离 向 量 公 式 比

出现非“
０”的值替换为 ＴＦ－ＩＤＦ 权 值，这 样 的 特 征 向 量 比 传 统

较词向量之间的相似度。

ＢＯＷ 在文本分类方面更有效。

（
３）
ｗｏ
ｒ
ｄ２ｖｅ
ｃ模型训练词向量
通过 借 鉴 Ｂｅｎｇ
ｉ
ｏ 提 出 的 ＮＮＬＭ （Ｎｅｕ
ｒ
ａ
ｌ　
Ｎｅ
ｔｗｏ
ｒ
ｋ　
Ｌａｎ－
［ ］
［ ］
Ｌ
ｉ
ｎｅ
ａ
ｒ模型 ２８ ，Ｍｉ
Ｍｏｄｅ
ｌ）２７ 以及 Ｈｉ
ｎ
ｔ
ｏｎ 的 Ｌｏｇ＿
ｋｏ
ｌ
ｏｖ
ｇｕａｇｅ　

等提出了 ｗｏ
ｒ
ｄ２ｖｅ
ｃ语言模型

。ｗｏ
ｒ
ｄ２ｖｅ
ｃ可 以 快 速 有 效 地

［
１９］

（
２）
ｄｏ
ｃ
２ｖｅ
ｃ模型
［ ， ］
ｄｏ
ｃ
２ｖｅ
ｃ模型 １８ １９ 的训 练 与 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 模 型 类 似，在 利 用

词的上下文对当前词进行预测的训练过程中添加了一个文 档
特征向量，其预测模型和训练过程分别如图 ３ 和图 ４ 所示。

训练词向量。
ｒ
ｄ２ｖｅ
ｃ模型有两种，分别是 ＣＢＯＷ 模 型（见 图 １）以 及
ｗｏ
Ｓｋ
ｒ
ｔ）前 后
ｉ
ａｍ 模型（见图 ２）。其中 ＣＢＯＷ 模型利用词 ｗ（
ｐ－ｇ
各ｃ（这里ｃ＝２）个词去预测当前词；而 Ｓｋ
ｒ
ｉ
ａｍ 模型恰好相
ｐ－ｇ
反，它利用词 ｗ（
ｔ）去预测它前后各ｃ（
ｃ＝２）个词。

图 ３　ｄｏ
ｃ
２ｖｅ
ｃ预测模型

图 １　ＣＢＯＷ 模型

图 ２　Ｓｋ
ｒ
ｉ
ａｍ 模型
ｐ－ｇ

由于 ＣＢＯＷ 模 型 的 训 练 和 Ｓｋ
ｒ
ｉ
ａｍ 模 型 的 训 练 类 似，
ｐ－ｇ
这里仅介绍 ＣＢＯＷ 模型的训练 过 程。其 中 输 入 层 是 词 ｗ（
ｔ）

图 ４　ｄｏ
ｃ
２ｖｅ
ｃ的训练过程

的 上下文中的 ２
ｃ 个词向量，而投影层向量 Ｘｗ 是这 ２
ｃ 个词向

从图 ３ 可以看出，该模型是利用前几个词 Ｗ 去 预 测 当 前

量的累加和。输出层是以训练语料库中出现过的词作叶子 节

的词，比如用“我”、“喜欢”、“吃”、“苹果”这个句子中 的 前 ３ 个

点，以各词 在 语 料 库 中 出 现 的 次 数 作 为 权 值 构 造 出 的 一 棵

词去 预 测 第 ４ 个 词，而 且 在 训 练 过 程 中 添 加 了 一 个 文 档 特 征

· ２１５ ·

向量 Ｄ，其余部分与 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 预 测 模 型 类 似，可 以 将 该 文 档
特征向量看作是一个表示当前文档中的其余部分信息或者 主
题信息的向量。通常 根 据 文 档 特 征 向 量 的 维 度 不 同，将 该 文
档的特征向量与词向量相结合的方法包括取平均值和串联 两
种方式。图 ４ 中的训练过程是利用串联的方式对文档向 量 进
行训练。其中 ｚ 代表文档向量，
ｘ 代 表 词 向 量，
ｗ 代表隐含层
权值，
ｒ
ｄ２ｖｅ
ｃ类似，
ｙ 代表输出层的值。与 ｗｏ
ｙ 中的每一维 即
代表词典中的一个词，
Ｅ１ 、
Ｅ２ 和 Ｐ 分 别 代 表 它 们 之 间 的 连 接
矩阵。其训练过程与 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 模 型 类 似，并 利 用 ＢＰ 算 法 调
节参数。
（
ｒ
ｄ２ｖｅ
ｃ的方法
３）基于 ｗｏ

图 ５　 文本分类的工作流程

目前基于 ｗｏ
ｒ
ｄ２ｖｅ
ｃ的 文 档 表 示 方 法 主 要 是 一 些 聚 类 的
方法或者取平均值的方法，而 表 现 较 优 的 是 ｄｏ
ｃ
２ｖｅ
ｃ 模 型，因
为 它 在 训 练 的 时 候 加 入 了 表 达 文 档 的 特 征 向 量 ，所 以 用 于 文

４　 实验
４．
１　 文本分类模型评估

档分类等任务时，一般表现比 ｗｏ
ｒ
ｄ２ｖｅ
ｃ好。
综合考虑 词 的 向 量 化 方 式 和 文 档 的 向 量 化 方 法，结 合

采用 的 评 估 指 标 包 括 准 确 率、召 回 率、
Ｆ１ 指 标 以 及 宏 平

ｗｏ
ｒ
ｄ２ｖｅ
ｃ和 ＴＦ－ＩＤＦ 算法，提出了一种 基 于 ｗｏ
ｒ
ｄ２ｖｅ
ｃ的 文 档

均。其中准确率 ｐ 是 指 文 本 分 类 正 确 的 样 本 数 与 所 有 分 类

向量表示方法。

文本数的比例：

ａ （如果
ａ＋ｂ＝０，则 ｐ＝１）
ｐ＝
ａ＋ｂ

３　 基于 ＴＦ－ＩＤＦ 算法的 ｗｏ
ｒ
ｄ２ｖ
ｅ
ｃ改进方法
在 ｗｏ
ｒ
ｄ２ｖｅ
ｃ词向量的基 础 上，结 合 ＴＦ－ＩＤＦ 算 法 提 出 了
文档向量的表示方法。
ｒ
ｄ２ｖ
ｅ
ｃ与 ＴＦ－ＩＤＦ 结合
３．
１　ｗｏ
对于包含 Ｍ 个文档的集合 Ｄ ，其中 Ｄｉ（
２，…，
ｉ＝１，
Ｍ ）已
经采用 分 词 工 具 ＡＮＳＪ 对 中 文 文 档 进 行 分 词，将 其 通 过
ｒ
ｄ２ｖｅ
ｃ模型训练，得到 每 个 分 词 对 应 的 Ｎ 维 词 向 量 ｗ ，其
ｗｏ
中 ｗ＝ （
ｖ１ ，
ｖ２ ，…，
ｖＮ ）。
对于每类文档集 中 的 每 个 文 档 里 的 每 个 分 词，利 用 ＴＦ－
ＩＤＦ 算法计算其在该文档中的权重值 Ｋ（
ｔ，
Ｄｉ），其 表 示 为 词ｔ
在文档 Ｄｉ（
２，…，
ｉ＝１，
Ｍ ）中 的 权 重。ＴＦ－ＩＤＦ 综 合 考 虑 了 词

召回率ｒ 是文本分类正确的样本数与该类的实 际 文 本 数
的比例：

ａ （如果
ｒ＝
ａ＋ｃ＝０，则ｒ＝１）
ａ＋ｃ
样本数，
ｃ 是属于该类但未被区分出来的样本数。

Ｆ１ 指标是将准确率与召回率同时考虑的一种指标：
２ｐｒ
Ｆ１ ＝
ｐ＋ｒ

评估；宏平均是对所有类别 的 ｐ，
ｒ 以 及 Ｆ１ 的 平 均 值，用 来 评

重ｉｄｆ。

４．
２　 实验结果与实验分析
实验选用的测试数据是搜狗实验室的中文文本分类语 料
（
１）

其中，
Ｍ 为训练文档的总 数；
ｎｔ为 训 练 文 档 集 中 出 现 词ｔ 的 文
档数。

ｔｆ（
ｔ，
Ｄｉ）×ｉｄｆ（
ｔ）
２
［
（
，
）
ｄ
ｔ
ｔ
Ｄ
×
ｉ
ｔ）］
∑ ｆ
ｉ
ｆ（

库 ［２５］，共包 含 文 本 １７９１０ 篇，分 为 ９ 类，分 别 是 财 经、
ＩＴ、健
康、体育、旅游、教育、招 聘、文 化 和 军 事，其 中 每 类 有 １９９０ 篇
文本。在预处理过程 采 用 ＡＮＳＪ 分 词 工 具 对 文 本 进 行 分 词，

ＴＦ－ＩＤＦ 的计算公式如下：

Ｋ（
ｔ，
Ｄｉ）＝

（
６）

准 确 率、召 回 率 以 及 Ｆ１ 是 对 单 独 类 别 的 分 类 性 能 进 行
估系统的总体分类性能。

ｉｄｆ（
ｔ）＝ｌ
Ｍ／ｎｔ＋０．
ｏｇ（
０１）

（
５）

其中，
ａ 是分类正确的 样 本 数 据，
ｂ是错误地划分到该类别的

在单个文档中出现的 概 率ｔ
ｆ 以及该词在整个文档集中的权
词ｔ的ｉｄｆ 计算公式如下：

（
４）

去掉标点符号、停用词、助词等后得到词条 ２４７２６３ 个。
（
２）

槡

将提出的 ＴＦ－ＩＤＦ 加权的 ｗｏ
ｒ
ｄ２ｖｅ
ｃ模型 与 ＴＦ－ＩＤＦ 加 权
的 ＢＯＷ 模型（即 将 ＢＯＷ 向 量 中 分 词 相 应 位 置 的 值 替 换 成

ｔ∈Ｄｉ

其中，
ｔｆ（
ｔ，
Ｄｉ）为词ｔ 在 第ｉ 篇 文 档 中 的 词 频，分 母 为 归 一 化

ＴＦ－ＩＤＦ 权值）、均 值 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 模 型 以 及 ｄｏ
ｃ
２ｖｅ
ｃ模 型 的 分 类

因子。

效果进行对 比，其 中 分 类 器 分 别 采 用 了 ＫＮＮ（
ｌ
ｉ
ｂ－
Ｋ ＝３０）、

对于每篇文档 Ｄｉ（
ｉ＝１，
Ｍ ），其 文 档 向 量 可 以 表 示
２，…，
为如下形式：

ｄｉ＝ ∑ ｗｔＫ （
ｔ，
Ｄｉ）
ｔ∈Ｄｉ

ＳＶＭ 以及 ＲＢＦ 神经网络。ＴＦ－ＩＤＦ 加权的 ＢＯＷ 模型在构建
词向量 时，特 征 提 取 采 用 了 文 档 频 率 选 择 （
ｃｕｍｅｎ
ｔ　
Ｆｒ
ｅ－
Ｄｏ

（
３）

ＤＦ）与信息增益选择（
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｇａ
ｉ
ｎ，
ＩＧ）结 合 ２３ ２４
ｑｕｅｎｃｙ，

［ ， ］

其中，
ｗｔ 表示分词ｔ 的 词 向 量，所 以 文 档 向 量 ｄ 也 是 一 个 Ｎ

的方 法，且 将 词 条 在 整 个 所 属 类 别 文 本 中 出 现 次 数 为 ５ 以 下

维的实数向量。

的过滤掉，最终只 选 取 了 ６７７５６ 个 词 条 来 构 建 词 向 量。 均 值

３．
２　 文本分类的工作流程

ｗｏ
ｒ
ｄ２ｖｅ
ｃ模 型 是 对 一 篇 文 本 计 算 其 所 有 ｗｏ
ｒ
ｄ２ｖｅ
ｃ所 得 词 向

对未知文档的分类过程如图 ５ 所示。由于对于单篇 未 知

量的均值。其中，
ｒ
ｄ２ｖｅ
ｃ 以 及 ｄｏ
ｃ
２ｖｅ
ｃ 的 计 算 采 用 ｇｅｎｓ
ｉｍ
ｗｏ

的文档，单独计算 其ｉｄｆ 值 并 无 意 义，因 此 在 计 算 其 ＴＦ－ＩＤＦ

开源软件实现 ［２６］。所有实验采用 五 分 交 叉 验 证，即 把 数 据 集

值时，
ｉｄｆ 值（如果是新词，则ｉｄｆ 取 ０．
０１）依然选取该 分 词 在

随机划分成 ５ 份，每 次 取 其 中 ４ 份 进 行 训 练，
１ 份 进 行 测 试，

训练阶段的值。

然后把 ５ 次分类结果的平均值作为最终结果。测试结果 用 正

· ２１６ ·

确率（
Ｆ１ 指 标 进 行 评 测，测 试 结 果 如 表 １－ 表
ｒ）、
ｐ）、召 回 率 （
Ｃ２、
Ｃ３、
Ｃ４、
Ｃ５、
Ｃ５、
Ｃ７、
Ｃ８、
Ｃ９ 分 别 代
４ 所列。其中 类 别 Ｃ１、

８４％ 。可 以 看 出，均 值 ｗｏ
ｒ
ｄ２ｖｅ
ｃ模 型 所 生 成
１３．
６４％ 以及 １．

表财经类、
ＩＴ 类、健康类、体育 类、旅 游 类、教 育 类、招 聘 类、文

的词向量比传统的 ＢＯＷ 所生成 的 词 向 量 能 更 有 效 地 表 示 一

化类以及军事类；
ａｖｇ 代表 Ｃ１－Ｃ９ 的宏平均值。

篇文档的特征。而本文提出的基于 ＴＦ－ＩＤＦ 加 权 的 ｗｏ
ｒ
ｄ２ｖｅ
ｃ
模型相比 均 值 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 模 型 又 有 一 些 提 升，在 ＳＶＭ、ＫＮＮ

表 １　ＴＦ－ＩＤＦ 加权 ＢＯＷ 模型（％ ）
ＳＶＭ　

类别
ｐ　

ＫＮＮ　
Ｆ１　

ｒ　

ｐ　

ｒ　

以及 ＲＢＦ 分 类 器 上，宏 平 均 正 确 率 分 别 提 升 了２．
５７％ ，
０．

ＲＢＦ
Ｆ１　

ｐ　

２％ 。而宏平均 Ｆ１ 分别提升了７．
８３％ ，
１０．
９％ ，
１４．
８３％ 以及 ２．

ｒ　

Ｆ１

１２　７２．
５８　７８．
３５　８２．
３８　７０．
７５　７６．
１２　７３．
２１　８０．
１１　７６．
５０
Ｃ１　 ８５．
Ｃ２　 ９０．
７８　７８．
３９　８４．
１３　９２．
６０　７５．
３８　８３．
１１　７０．
９３　６９．
２７　７０．
０９
Ｃ３　 ８１．
６４　６８．
６１　７４．
５６　８０．
６２　７６．
０８　７８．
２８　８４．
５０　６０．
７６　７０．
６９

５８％ 以及 ３．
７２％ ；宏 平 均 召 回 率 分 别 提 升 了２．
７３％ ，
１．
０２％
以及 ３．
３８％ ；宏平 均 Ｆ１ 分 别 提 升 了 ２．
６５％ ，
０．
８５％ 以 及 ３．
８５％ 。而 ＴＦ－ＩＤＦ 加权的 ｗｏ
ｒ
ｄ２ｖｅ
ｃ模型 与 ｄｏ
ｃ
２ｖｅ
ｃ模 型 的 宏

Ｃ４　 ９５．
４７　７１．
２９　８１．
６３　９７．
７１　６１．
４６　７５．
４６　９６．
６７　７９．
２８　８７．
１１

平均值效果相差不大，在几组分类器上的效果相当，可见提 出

Ｃ５　 ６６．
９８　６６．
２７　６６．
６２　８８．
８９　５６．
０８　６８．
７７　９１．
３５　６３．
１３　７４．
６６

的 方 法 的 有 效 性，其 可 以 作 为 另 外 一 种 用 于 文 档 分 类 的 有 效

Ｃ６　 ６７．
０１　７３．
８２　７０．
２５　４５．
０１　８３．
４２　５８．
４７　８７．
５９　４３．
２１　５７．
８７
Ｃ７　 ７２．
３９　７０．
８１　７１．
５９　６１．
８７　６５．
６３　６３．
６９　５１．
３３　８８．
４３　６４．
９６

方法。由图 ６ 也可以清楚地看出，无论采用何种分类器，基 于

Ｃ８　 ７７．
１４　７５．
３２　７６．
２２　５７．
０３　７１．
４１　６３．
４２　５３．
２２　７５．
３９　６２．
３９

ｒ
ｄ２ｖｅ
ｃ 模 型 在 宏 平 均 上 均 有 不 错 的 表 现，
ＴＦ－ＩＤＦ 加 权 的 ｗｏ

Ｃ９　 ８４．
９５　８０．
９３　８２．
８９　８９．
６５　７９．
４５　８４．
２４　８４．
３９　８２．
７６　８３．
５７

验证了所提出的生成文档向量的方法在文档分类方面的有 效

９８
ａ
ｖｇ　 ８０．
１６　７３．
１１　７６．
２５　７７．
３１　７１．
０７　７２．
４０　７７．
０２　７１．
３７　７１．

性。

表 ２　 均值 ｗｏ
ｒ
ｄ２ｖｅ
ｃ模型（％ ）
ＳＶＭ　

类别
ｐ　

ＫＮＮ　
Ｆ１　

ｒ　

ｐ　

ｒ　

ＲＢＦ
Ｆ１　

ｐ　

ｒ　

Ｆ１

６７　８３．
８５　８５．
７１　９０．
９８　８５．
３８　８８．
１　７６．
５９　８２．
４５
Ｃ１　 ８７．
６１　７９．
Ｃ２　

８２．
６　８１．
５４　８２．
０６　８６．
９８　８０．
５１　８３．
６２　７３．
０８　７４．
３７　７３．
７１

Ｃ３　 ８４．
０５　７９．
７４　８１．
８４　８２．
５４　８８．
４６　８５．
４　８７．
１１　５４．
６７　６７．
０８
Ｃ４　 ９８．
７１　９８．
４６　９８．
５９　９９．
２　 ９５．
９　９７．
５２　９９．
５０　８１．
１１　８９．
３４
Ｃ５　 ８６．
６３　８３．
０８　８４．
８２　８５．
２２　８２．
８２　８４．
０１　９０．
４９　６１．
１１　７２．
８８
Ｃ６　 ８７．
８６　７７．
９５　８２．
６１　９０．
２０　８２．
５６　８６．
２１　９２．
４５　４６．
１３　６１．
４９
Ｃ７　 ８０．
０５　８６．
４１　８３．
１１　８１．
８６　８３．
３３　８２．
５９　４９．
０７　９１．
４６　６３．
８７
Ｃ８　 ６５．
０５　７２．
０５　６８．
３７　６８．
０３　８０．
７７　７３．
８６　５８．
００　８２．
８１　６８．
２０

图 ６　４ 种文档向量分类效果比较

Ｃ９　 ８６．
４３　９３．
０８　８９．
６３　９２．
８６　９３．
３３　９３．
０９　８８．
９２　８７．
８４　８８．
３４

结束语 　 针 对 当 前 文 本 向 量 表 示 方 法 的 不 足，借 助

８２
ａ
ｖｇ　 ８４．
３４　８４．
０１　８４．
０８　８６．
４３　８５．
９　８６．
０４　７９．
４７　７３．
５７　７３．

ｒ
ｄ２ｖｅ
ｃ的 优 点，将 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 和 ＴＦ－ＩＤＦ 结 合，提 出 了 一 种
ｗｏ

表 ３　ｄｏ
ｃ
２ｖｅ
ｃ模型（％ ）
ＳＶＭ　

类别
ｐ　

ｒ　

ＫＮＮ　
Ｆ１　

ｐ　

ｒ　

基于 ｗｏ
ｒ
ｄ２ｖｅ
ｃ的 ＴＦ－ＩＤＦ 加 权 计 算 文 档 向 量 算 法 。 在 搜 狗

ＲＢＦ
Ｆ１　

ｐ　

ｒ　

Ｆ１

２１　８６．
３７　８５．
７９　８８．
９５　８７．
４３　８８．
１８　８１．
５６　８４．
３７　８２．
９４
Ｃ１　 ８５．
Ｃ２　 ８６．
５９　８２．
３５　８４．
４２　８７．
５６　８２．
４８　８４．
９４　９１．
０２　６７．
３８　７７．
４４
Ｃ３　 ８７．
２５　８５．
３３　８６．
２８　８３．
８９　８５．
９６　８４．
９１　９０．
１２　６５．
３９　７５．
７９

中文实验语料库上的实验 表 明，相 较 于 ＴＦ－ＩＤＦ 加 权 的 ＢＯＷ
模 型 以 及 均 值 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 模 型，本 算 法 有 更 好 的 文 本 分 类 效
果。

Ｃ４　 ９６．
５３　９７．
８０　９７．
１６　９７．
８５　９８．
２１　９８．
０３　９６．
５２　９６．
３４　９６．
４３

参 考 文 献

Ｃ５　 ８８．
４７　８９．
６８　８９．
０７　８５．
３２　９１．
２５　８８．
１９　８１．
２５　８５．
３６　８３．
２５
Ｃ６　 ８９．
５１　８１．
２８　８５．
２　８５．
９８　８０．
２３　８３．
０１　９０．
２４　５８．
５６　７１．
０３
Ｃ７　 ８１．
９８　８５．
６３　８３．
７７　８１．
９５　８４．
８９　８３．
３９　４２．
３６　９２．
６５　５８．
１４
Ｃ８　 ８２．
１５　８１．
３３　８１．
７３　７９．
５６　７７．
８５　７８．
７０　８５．
８９　５２．
３３　６５．
０４
Ｃ９　 ８７．
６９　９２．
８７　９０．
２１　９１．
５９　９２．
８７　９２．
２３　９１．
２４　８７．
４１　８９．
２８
７０
ａ
ｖｇ　 ８７．
２６　８６．
９６　８７．
０６　８６．
９６　８６．
８０　８６．
８４　８３．
３６　７６．
６４　７７．

ＳＶＭ　
ｐ　

ｒ　

ＫＮＮ　
Ｆ１　

ｐ　

ｒ　

ｐ　

Ｌａｎｇｕａｇｅ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ ［Ｍ］．
Ｃａｍｂｒ
ｉ
ｄｇｅ：ＭＩＴ　
ｒ
ｅ
ｓ
ｓ，
１９９９
ｐ
Ｂａ
ｓ
ｅｄ　
ｏｎ　
Ｒｅ
ｌ
ａ
ｔ
ｉ
ｏｎ　
Ｓ
ｔ
ｒ
ｕｃ
ｔ
ｕｒ
ｅ［
Ｃ］∥Ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌ　
Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
ｏｎ

ＲＢＦ
Ｆ１　

［Ｍ］．
Ｎｅｗ　
Ｙｏ
ｒｋ：
ＡＣＭ　
ｒ
ｅ
ｓ
ｓ，
１９９９
ｐ
［
２］ Ｍａｎｎ
ｉ
ｎｇ　
Ｃ　
Ｄ，Ｓｃｈü
ｔ
ｚ
ｅ　
Ｈ．Ｆｏｕｎｄａ
ｔ
ｉ
ｏｎｓ　
ｏ
ｆ
Ｓ
ｔ
ａ
ｔ
ｉ
ｓ
ｔ
ｉ
ｃ
ａ
ｌ　Ｎａ
ｔ
ｕｒ
ａ
ｌ
　
［
Ｍ，
Ｃｈｏ
ｉ
Ｃ，Ｙｏｕｎ　
Ｂ，
ｅ
ｔ
ａ
ｌ．Ｗｏ
ｒ
ｄ　
Ｓｅｎｓ
ｅ　
Ｄｉ
ｓ
ａｍｂ
ｉ
ｔ
ｉ
ｏｎ
３］ Ｈｗａｎｇ　
　
　
ｇｕａ

表 ４　ＴＦ－ＩＤＦ 加权 ｗｏ
ｒ
ｄ２ｖｅ
ｃ（％ ）
类别

［
１］ Ｂａ
ｅ
ｚ
ａ－Ｙａ
ｔ
ｅ
ｓ　Ｒ，
Ｒｉ
ｂｅ
ｉ
ｒ
ｏ－Ｎｅ
ｔ
ｏ　Ｂ．Ｍｏｄｅ
ｒ
ｎ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　Ｒｅ
ｔ
ｒ
ｉ
ｅｖａ
ｌ
　Ｉ

ｒ　

Ｆ１

６４　８７．
６４　８７．
０７　８９．
５６　８７．
４１　８８．
４５　８０．
１７　８５．
８５　８２．
８９
Ｃ１　 ８６．
Ｃ２　 ８６．
４５　８２．
１６　８４．
２１　８６．
２２　８１．
７１　８３．
８７　９０．
２２　６５．
７３　７５．
９９
Ｃ３　 ８６．
８０　８４．
４０　８５．
５５　８３．
８６　８６．
７１　８５．
２２　８９．
３３　６７．
０１　７６．
５３
Ｃ４　 ９８．
２５　９６．
８３　９７．
５３　９８．
１６　９７．
６４　９７．
９０　９６．
３７　９６．
３６　９６．
３６

Ａｄｖａｎｃ
ｅｄ　
Ｌａｎｇｕａｇｅ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ　
ａｎｄ　
Ｗｅｂ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｔｅ
ｃｈｎｏ
ｌ
ｏ－
　
２００８：
１５－２０
ｇｙ．
［
ａｍｓ：Ｐｈｒ
ａ
ｓ
ｅ
ａｎｄ
４］ Ｗａｎｇ　Ｘ，Ｍｃ
ｃ
ａ
ｌ
ｌ
ｕｍ　
Ａ，Ｗｅ
ｉ　
Ｘ．Ｔｏｐ
ｉ
ｃ
ａ
ｌ　
Ｎ－Ｇｒ
　
Ｔｏｐ
ｉ
ｃ　
Ｄｉ
ｓ
ｃ
ｏｖｅ
ｒ
ｔ
ｈ　
ａｎ　Ａｐｐ
ｌ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ
ｔ
ｏ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｒｅ
ｔ
ｒ
ｉ
ｅｖａ
ｌ
　
　
ｙ，ｗｉ

Ｃ５　 ８７．
９３　８９．
２０　８８．
５４　８４．
３３　９２．
０９　８８．
０２　８０．
６０　８４．
１０　８２．
２８

［
Ｃ］∥ＩＥＥＥ　Ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌ　Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　ｏｎ　Ｄａ
ｔ
ａ　Ｍｉ
ｎ
ｉ
ｎｇ．
ＩＥＥＥ

Ｃ６　 ８８．
６６　７９．
９５　８４．
０７　８７．
３１　７９．
５５　８３．
２１　９１．
１５　５８．
５２　７１．
２４

Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｓｏ
ｃ
ｉ
ｅ
ｔ
２００７：
６９７－７０２
ｙ，

Ｃ７　 ８０．
５４　８６．
９６　８３．
５７　８２．
２１　８５．
７８　８３．
９３　４３．
３０　９４．
８７　５９．
４５
Ｃ８　 ７９．
１０　７９．
８７　７９．
４４　７９．
３７　７８．
２４　７８．
７７　８６．
７１　５１．
７１　６４．
７１
Ｃ９　 ８７．
８６　９３．
６４　９０．
６１　９２．
０８　９３．
１２　９２．
５８　９０．
８５　８８．
３７　８９．
５７
６７
ａ
ｖｇ　 ８６．
９１　８６．
７４　８６．
７３　８７．
０１　８６．
９２　８６．
８９　８３．
１９　７６．
９５　７７．

由表 １－ 表 ４ 可 以 发 现，均 值 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 模 型 在 ＳＶＭ、

［
５］ Ｈａ
ｒ
ｕｅ
ｃｈａ
ｉ
ｓ
ａｋ　
Ｃ，Ｊ
ｉ
ｔ
ｋｒ
ｉ
ｔ
ｔ
ｕｍ　
Ｗ，Ｓａｎｇｋｅ
ｅ
ｔ
ｔ
ｒ
ａｋａ
ｒ
ｎ　
Ｃ，ｅ
ｔ
ａ
ｌ．Ｉｍ－
　
ｙａ
ｅｍｅｎ
ｔ
ｉ
ｎｇ　
Ｎｅｗｓ　
Ａｒ
ｔ
ｉ
ｃ
ｌ
ｅ　
Ｃａ
ｔ
ｅｇｏ
ｒ
Ｂｒ
ｏｗｓ
ｉ
ｎｇ　
Ｂａ
ｓ
ｅｄ　
ｏｎ　
Ｔｅｘ
ｔ　
Ｃａ
ｔ－
ｌ
ｙ　
ｐ
ｅｇｏ
ｒ
ｉ
ｚ
ａ
ｔ
ｉ
ｏｎ　
Ｔｅ
ｃｈｎ
ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ－
ｑｕｅ ［Ｃ］∥２００８ＩＥＥＥ／ＷＩＣ／ＡＣＭ　Ｉ
ｉ
ｏｎａ
ｌ　Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　ｏｎ　Ｗｅｂ
ｎ
ｔ
ｅ
ｌ
ｌ
ｉ
ｅ
ｎ
ｔ
ｅ
ｌ
ｌ
ｉ
ｔ　Ａｇｅｎ
ｔ
ｔ
　Ｉ
　ａｎｄ
　Ｉ
ｇｅｎｃ
ｇｅｎ
Ｔｅ
ｃｈｎｏ
ｌ
ｏｇｙ．
ＩＥＥＥ　
Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｓｏ
ｃ
ｉ
ｅ
ｔ
２００８：
１４３－１４６
ｙ，

ＫＮＮ 以及 ＲＢＦ 分类器上的宏平均准确率、召回率以及 Ｆ１ 值

［
６］ Ｍｉ
ｋｏ
ｌ
ｏｖ　
Ｔ，
Ｓｕ
ｔ
ｓｋｅｖｅ
ｒ
Ｉ，
Ｃｈｅｎ　
Ｋ，
ｅ
ｔ
ａ
ｌ．
Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ－
　
　

比 ＴＦ－ＩＤＦ 加权 ＢＯＷ 模型的有不少的提升，比如宏平均正 确

ｔ
ｉ
ｏｎｓ　
ｏ
ｆ　
Ｗｏ
ｒ
ｄｓ
ａｎｄ　
Ｐｈｒ
ａ
ｓ
ｅ
ｓ
ａｎｄ
ｔ
ｈｅ
ｉ
ｒ　
Ｃｏｍｐｏ
ｓ
ｉ
ｔ
ｉ
ｏｎａ
ｌ
ｉ
ｔ
Ｊ］．
Ａｄ－
　
　
　
ｙ［

率在 ＫＮＮ 分类器上由 ７７．
３１％ 提 升 到 了 ８６．
４３％ ，在 另 外 ２

ｖａｎｃ
ｅ
ｓ
ｎ　Ｎｅｕｒ
ａ
ｌ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ　Ｓｙｓ
ｔ
ｅｍｓ，２０１３，２６：
　ｉ
　Ｉ

个分类器上 也 分 别 提 升 了 ４．
１８％ （
ＳＶＭ）和 ２．
４５％ （
ＲＢＦ）。

３１１１－３１１９

宏平均召回 率 在 ＳＶＭ、
ＫＮＮ 以 及 ＲＢＦ 分 类 器 上 分 别 提 升 了

　　　

（下转第 ２６９ 页）

· ２１７ ·

［
１０］ Ｇｕ
ｌ
ｌ
ｏ　
Ｆ，
Ｐｏｎ
ｔ
ｉ　
Ｇ，Ｔａｇａ
ｒ
ｅ
ｌ
ｌ
ｉ　
Ａ．
Ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　
ｕｎｃ
ｅ
ｒ
ｔ
ａ
ｉ
ｎ　
ｄａ
ｔ
ａ　
ｖ
ｉ
ａ　
ｋ－

ｄｅ
ｔ
ｅ
ｒｍｉ
ｎ
ｉ
ｎｇ　
ｏｐ
ｔ
ｉｍａ
ｌ
ｎｕｍｂｅ
ｒ
ｏ
ｆ
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｓ
ｉ
ｎ　
Ｋ－ｍｅ
ａｎｓ
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ
　
　
　
　
　

ｍｅｄｏ
ｉ
ｄｓ［Ｍ］∥Ｓｃ
ａ
ｌ
ａｂ
ｌ
ｅ　
Ｕｎｃ
ｅ
ｒ
ｔ
ａ
ｉ
ｎ
ｔ
Ｍａｎａｇｅｍｅｎ
ｔ．
Ｓｐ
ｒ
ｉ
ｎｇｅ
ｒ　
Ｂｅ
ｒ－
ｙ　

ａ
ｌ
ｒ
ｉ
ｔ
ｈｍ［
Ｊ］．
Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ　
ａｎｄ　
Ａｐｐ
ｌ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎｓ，
２０１０，
４６
ｇｏ

ｌ
ｉ
ｎ　
Ｈｅ
ｉ
ｄｅ
ｌ
ｂｅ
ｒ
２００８：
２２９－２４２
ｇ，

（
１６）：
ｉ
ｎ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ）
２７－３１（

［
ｕ，
Ｌ
ｉ
Ｌｅ
ｉ．
Ｒｅ
ｓ
ｅ
ａ
ｒ
ｃｈ　
ｏｎ　
Ｍｕ
ｌ
ｔ
ｉ－ａ
ｔ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅ　
Ｇｒ
ｏｕｐ　
Ｄｅ
ｃ
ｉ
ｓ
ｉ
ｏｎ
１１］Ｘｉ
ｅ　
Ｘｉ
ａｏ－ｌ
　
Ｕｎｄｅ
ｒ
Ｉ
ｎ
ｔ
ｅ
ｒ
ｖａ
ｌ　
Ｎｕｍｂｅ
ｒ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ［
Ｊ］．
Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ，
　
　
ｉ
ｎ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ）
２１０－２１３（
２０１４，
４０（
１０）：

周世兵，徐振源，唐旭清 ．新的 Ｋ－均值算法最 佳 聚 类 数 确 定 方 法
［
４６（
１６）：
２７－３１
Ｊ］．计算机工程与应用，
２０１０，
［
Ｓｅ
ａ
ｒ
ｃｈ　
ｒ
ａｎｇｅ　
ｏ
ｆ
ｔ
ｈｅ　
Ｏｐ
ｔ
ｉｍａ
ｌ
ｃ
ｌ
ｕｓ
ｔ
ｅ－
１６］Ｙｕ　
Ｊ
ｉ
ａｎ，
Ｃｈｅｎｇ　
Ｑｉ
ａｎ－ｓｈｅｎｇ．
　
　

谢小璐，李磊 ．区间数信息下的多属性群决策研究［
Ｊ］．计 算 机 工

ｒ
ｉ
ｎｇ　
ｎｕｍｂｅ
ｒ
ｉ
ｎ
ｆ
ｕｚ
ｚ
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍｓ［
Ｊ］．
Ｓｃ
ｉ
ｅｎｃ
ｅ
ｉ
ｎ　
Ｃｈ
ｉ
ｎａ：
　
　
　
ｙ　
ｇｏ

程，
４０（
１０）：
２１０－２１３
２０１４，

Ｓｅ
ｒ
ｉ
ｅ
ｓ　
Ｅ，
２００２，
３２（
２）：
ｉ
ｎ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ）
２７４－２８０（

［
ｔ
ｈ　
Ｖ．
Ｔｈｅ　
Ａｐｐ
ｌ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ
１２］Ｒｅｙｎｏ
ｌ
ｄｓ　
Ｐ　
Ａ，
Ｒｉ
ｃｈａ
ｒ
ｄｓ　
Ｇ　
Ｊ，
Ｒａｙｗａ
ｒ
ｄ－ｓｍｉ
ｏ
ｆ　
Ｋ－Ｍｅｄｏ
ｉ
ｄｓ
ａｎｄ　
ＰＡＭ　
ｔ
ｏ
ｔ
ｈｅ　
Ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　
ｏ
ｆ　
Ｒｕ
ｌ
ｅ
ｓ［
Ｊ］．
Ｌｅ
ｃ
ｔ
ｕｒ
ｅ
　
　
Ｎｏ
ｔ
ｅ
ｓ
ｉ
ｎ　
Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｓｃ
ｉ
ｅｎｃ
ｅ，
２００４，
３１７７：
１７３－１７８
　
［
１３］ Ａｇｇａ
ｒｗａ
ｌ　
Ｃ　
Ｃ．Ｙｕ　
Ｐ　
Ｓ．Ａ　
ｓｕｒ
ｖｅｙ　
ｏ
ｆ
ｕｎｃ
ｅ
ｒ
ｔ
ａ
ｉ
ｎ　
ｄａ
ｔ
ａ
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍｓ
　
ｇｏ
ａｎｄ　
ａｐｐ
ｌ
ｉ
ｅ
ａ
ｔ
ｉ
ｏｎｓ［
Ｊ］．
ＩＥＥＥ　
Ｔｒ
ａｎｓ
ａ
ｃ
ｔ
ｉ
ｏｎｓ　
Ｏｎｋｎｏｗｌ
ｅｄｇｅ
ａｎｄ　
Ｄａ
ｔ
ａ
　
６０９－６２３
Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ，
２００９，
２１（
５）：
［
Ｆｅｎｇ　
Ｊ
ｉ
ｎ－ｇｏｎｇ，
Ｆａｎ　
Ｄｏｎｇ－ｍｅ
ｉ，
ｅ
ｔ
ａ
ｌ．
Ｎｅｗ　
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ
１４］Ｌｕ　
Ｚｈ
ｉ－ｍａｏ，
　
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍｓ
ｆ
ｏ
ｒ
ｌ
ａ
ｒ
ｄａ
ｔ
ａ　
ｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ［
Ｊ］．
Ｓｙｓ
ｔ
ｅｍ　
Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ　
ａｎｄ
　
　
ｇｏ
ｇｅ　
ｐ
Ｅｌ
ｅ
ｃ
ｔ
ｒ
ｏｎ
ｉ
ｃ
ｓ，
２０１４（
５）：
ｉ
ｎ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ）
１０１０－１０１５（

于剑，程乾生 ．模糊聚类方法中的最佳聚类数的搜索范围［
Ｊ］．中
国科学：
３２（
２）：
２７４－２８０
Ｅ 辑，
２００２，
［
１７］Ｄｕｄｏ
ｉ
ｔ
Ｓ，
Ｆｒ
ｉ
ｄ
ｌ
Ｊ．
Ａ　
ｒ
ｅｄ
ｉ
ｃ
ｔ
ｉ
ｏｎ－ｂａ
ｓ
ｅｄ
ｒ
ｅ
ｓ
ａｍｐ
ｌ
ｉ
ｎｇ　
ｍｅ
ｔ
ｈｏｄ
ｆ
ｏ
ｒ
　
　
　
ｙａｎｄ　
ｐ
ｅ
ｓ
ｔ
ｉｍａ
ｔ
ｉ
ｎｇ
ｔ
ｈｅ　
ｎｕｍｂｅ
ｒ
ｏ
ｆ
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｓ
ｉ
ｎ　
ａ　
ｄａ
ｔ
ａ
ｓ
ｅ
ｔ［
Ｊ］．
Ｇｅｎｏｍｅ　
Ｂ
ｉ
ｏ－
　
　
　
　
ｌ
ｏｇｙ，
２００２，
３（
７）：
１－２１
［
１８］ Ｋａｏ　Ｂ，Ｌｅ
ｅ　Ｓ，Ｌｅ
ｅ　Ｆ，
ｅ
ｔ
ｌ．Ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　Ｕｎｃ
ｅ
ｒ
ｔ
ａ
ｉ
ｎ　Ｄａ
ｔ
ａ　Ｕｓ
ｉ
ｎｇ
　ａ
Ｖｏ
ｒ
ｏｎｏ
ｉ　
Ｄｉ
ａｇ
ｒ
ａｍｓ
ａｎｄ　
Ｒ－Ｔｒ
ｅ
ｅ
Ｉ
ｎｄｅｘ．［
Ｊ］．Ｋｎｏｗｌ
ｅｄｇｅ　
＆ Ｄａ
ｔ
ａ
　
　
Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ
ＩＥＥＥ　
Ｔｒ
ａｎｓ
ａ
ｃ
ｔ
ｉ
ｏｎｓ　
ｏｎ，
２０１０，
２２（
９）：
１２１９－１２３３
　

［
１９］Ｅｒ
ｅｄｍ　
Ａ，
Ｉｍｒ
ｅ　
ＧüＮＤＥＭ　
Ｔ．Ｍ－ＦＤＢＳＣＡＮ：Ａ　
ｍｕ
ｌ
ｔ
ｉ
ｃ
ｏ
ｒ
ｅ　
ｄｅｎｓ
ｉ－

卢志茂，冯进玫，范冬梅，等 ．面向大数据处理的划分聚类新方 法

ｓ
ｅｄ　
ｕｎｃ
ｅ
ｒ
ｔ
ａ
ｉ
ｎ　
ｄａ
ｔ
ａ
ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍ［
Ｊ］．
Ｔｕｒｋ
ｉ
ｓｈ　
Ｊ
ｏｕｒ
ｎａ
ｌ
ｔ
　
ｇｏ
ｙ－ｂａ

［
５）：
１０１０－１０１５
Ｊ］．系统工程与电子技术，
２０１４（

ｏ
ｆ　
Ｅｌ
ｅ
ｃ
ｔ
ｒ
ｉ
ｃ
ａ
ｌ　
Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ　
＆ Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｓｃ
ｉ
ｅｎｃ
ｅ
ｓ，
２０１４，
２２（
１）：

［
ｉ
１５］Ｚｈｏｕ　
Ｓｈ
ｉ－ｂ
ｉ
ｎｇ，Ｘｕ　
Ｚｈｅｎ－ｙｕａｎ，Ｔａｎｇ　
Ｘｕ－ｑ
ｎｇ．Ｎｅｗ　
ｍｅ
ｔ
ｈｏｄ
ｆ
ｏ
ｒ
　

（上接第 ２１７ 页）
［
７］ Ｍｉ
ｋｏ
ｌ
ｏｖ　
Ｔ，Ｃｈｅｎ　Ｋ，Ｃｏ
ｒ
ｒ
ａｄｏ　Ｇ，
ｅ
ｔ
ｌ．Ｅｆ
ｆ
ｉ
ｃ
ｉ
ｅｎ
ｔ　Ｅｓ
ｔ
ｉｍａ
ｔ
ｉ
ｏｎ　ｏ
ｆ
　ａ
Ｗｏ
ｒ
ｄ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ
ｉ
ｎ　
Ｖｅ
ｃ
ｔ
ｏ
ｒ　
Ｓｐａ
ｃ
ｅ［
Ｃ］∥ＩＣＬＲ　
２０１３．
２０１３
　
［
８］ Ｊ
ｏａ
ｃｈ
ｉｍｓ　
Ｔ．
Ａ　
Ｐｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｓ
ｔ
ｉ
ｃ　
Ａｎａ
ｌ
ｉ
ｓ　
ｏ
ｆ
ｔ
ｈｅ　
Ｒｏ
ｃ
ｃｈ
ｉ
ｏ　
Ａｌ
ｒ
ｉ
ｔ
ｈｍ
　
ｙｓ
ｇｏ
ｗｉ
ｔ
ｈ　
ＴＦＩＤＦ　
ｆ
ｏ
ｒ　
Ｔｅｘ
ｔ　
Ｃａ
ｔ
ｅｇｏ
ｒ
ｉ
ｚ
ａ
ｔ
ｉ
ｏｎ ［Ｍ ］．
Ｓｐ
ｒ
ｉ
ｎｇｅ
ｒ　
ＵＳ，
１９９７：
１４３－１５１
［
９］ Ｈｉ
ｎ
ｔ
ｏｎ　Ｇ　
Ｅ．Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ　
ｄ
ｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ
ｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ　
ｏ
ｆ
ｃ
ｏｎｃ
ｅｐ
ｔ
ｓ
　
　
［
Ｃ］∥Ｐｒ
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ　
ＣｏｇＳｃ
ｉ．
１９８６：
１－１２

１４３－１５４

［
１８］Ｋｉｍ　
Ｈ　
Ｋ，Ｋｉｍ　
Ｈ，Ｃｈｏ　
Ｓ．Ｂａｇ－ｏ
ｆ－Ｃｏｎｃ
ｅ
ｔ
ｓ：Ｃｏｍｐ
ｒ
ｅｈｅｎｄ
ｉ
ｎｇ　
Ｄｏ－
ｐ
ｔ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ
ｔ
ｈｒ
ｏｕｇｈ　
Ｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ　
Ｗｏ
ｒ
ｄｓ
ｉ
ｎ　
Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ
ｃｕｍｅｎ
　
　
／ｓ
／ｄｏ
／ＴＲ／
／／ｄｍ．
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ［ＯＬ］．ｈ
ｔ
ｔ
ｓｎｕ．
ａ
ｃ．ｋｒ
ｔ
ａ
ｔ
ｉ
ｃ
ｃ
ｓ
ｐ：
ＳＮＵＤＭ－ＴＲ－２０１５－０５．
ｆ
ｐｄ
［
１９］Ｌｅ　
Ｑ　
Ｖ，Ｍｉ
ｋｏ
ｌ
ｏｖ　
Ｔ．Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ　
ｏ
ｆ
Ｓｅｎ
ｔ
ｅｎｃ
ｅ
ｓ
　
ａｎｄ　
Ｄｏ
ｃｕｍｅｎ
ｔ
ｓ［
Ｊ］．
Ｅｐ
ｒ
ｉ
ｎ
ｔ　
Ａｒ
ｘ
ｉ
ｖ，
２０１４，
４：
１１８８－１１９６
［
２０］ Ｍｏ
ｒ
ｉ
ｎ　
Ｆ，Ｂｅｎｇ
ｉ
ｏ　
Ｙ．Ｈｉ
ｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ
Ｐｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｓ
ｔ
ｉ
ｃ　
Ｎｅｕｒ
ａ
ｌ　
Ｎｅ
ｔｗｏ
ｒｋ
　
Ｌａｎｇｕａｇｅ　
Ｍｏｄｅ
ｌ［
Ｊ］．
Ａｉ
ｓ
ｔ
ａ
ｔ
ｓ．
２００５，
５：
２４６－２５２

［
ｗｉ
ｔ
ｈ　
Ｃｏｍｐｏ
ｓ
ｉ－
Ｃ　
Ｄ，
ｅ
ｔ
ａ
ｌ．
Ｐａ
ｒ
ｓ
ｉ
ｎｇ　
１０］Ｓｏ
ｃｈｅ
ｒ　
Ｒ，
Ｂａｕｅ
ｒ
Ｊ，Ｍａｎｎ
ｉ
ｎｇ　
　
　

［
２１］ Ｍｎ
ｉ
ｈ　
Ａ，
Ｈｉ
ｎ
ｔ
ｏｎ　
Ｇ　
Ｅ．Ａ　
Ｓｃ
ａ
ｌ
ａｂ
ｌ
ｅ　
Ｈｉ
ｅ
ｒ
ａ
ｒ
ｃｈ
ｉ
ｃ
ａ
ｌ　
Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ　
Ｌａｎ－

ｉ
ｏｎａ
ｌ　
Ｖｅ
ｃ
ｔ
ｏ
ｒ　
Ｇｒ
ａｍｍａ
ｒ
ｓ［
Ｃ］∥ Ｍｅ
ｏ
ｆ
ｔ
ｈｅ　
Ａｓ
ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏｎ
ｆ
ｏ
ｒ
ｅ
ｔ
ｉ
ｎｇ　
ｔ
　
　

Ｍｏｄｅ
ｌ［
Ｃ］∥Ａｄｖａｎｃ
ｅ
ｓ
ｉ
ｎ　
Ｎｅｕｒ
ａ
ｌ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ
　
　
ｇｕａｇｅ　

Ｃｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌ　
Ｌ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ．
２０１３：
４５５－４６５

Ｓｙｓ
ｔ
ｅｍｓ．
２００９：
１０８１－１０８８

［
Ｍｏｄｅ
ｌ
ｓ
ｆ
ｏ
ｒ
１１］Ｓｏ
ｃｈｅ
ｒ　
Ｒ，
Ｐｅ
ｒ
ｅ
ｌ
ｉ
ｎ　
Ａ，Ｗｕ　
Ｊ　
Ｙ，
ｅ
ｔ
ａ
ｌ．
Ｒｅ
ｃｕｒ
ｓ
ｉ
ｖｅ　
Ｄｅ
ｅｐ　
　
　
ｙｇ
ｒ
ｔ
ｉｍｅｎ
ｔ　Ｔｒ
ｅ
ｅｂａｎｋ ［
Ｃ］∥
Ｓｅｍａｎ
ｔ
ｉ
ｃ　
Ｃｏｍｐｏ
ｓ
ｉ
ｔ
ｉ
ｏｎａ
ｌ
ｉ
ｔ
　ａ　Ｓｅｎ
ｙ　ｏｖｅ
Ｐｒ
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ
ｔ
ｈｅ　
Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
ｏｎ　
Ｅｍｐ
ｉ
ｒ
ｉ
ｃ
ａ
ｌ　
Ｍｅ
ｔ
ｈｏｄｓ
ｉ
ｎ　
Ｎａ
ｔ
ｕｒ
ａ
ｌ
　
　
Ｌａｎｇｕａｇｅ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ （
ＥＭＮＬＰ）．
２０１３：
１６３１－１６４２
［
ｅｄ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ　
Ｃｈａ
ｒ
ａ
ｃ－
１２］Ｓｕｎ　
Ｙ，
Ｌ
ｉ
ｎ　
Ｌ，
Ｙａｎｇ　
Ｎ，
ｅ
ｔ
ａ
ｌ．
Ｒａｄ
ｉ
ｃ
ａ
ｌ－Ｅｎｈａｎｃ
　

［
Ｇ．Ｌ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ　
Ｒｅｇｕ
ｌ
ａ
ｒ
ｉ
ｔ
ｉ
ｅ
ｓ
ｉ
ｎ　
Ｃｏｎ
ｔ
ｉ－
２２］ Ｍｉ
ｋｏ
ｌ
ｏｖ　
Ｔ，Ｙｉ
ｈ　
Ｗ，Ｚｗｅ
ｉ
　
ｇ　
ｎｕｏｕｓ　
Ｓｐａ
ｃ
ｅ　
Ｗｏ
ｒ
ｄ　
Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ ［
Ｃ］∥ＨＬＴ－ＮＡＡＣＬ．
２０１３：
７４６－７５１
［
２３］Ｓａｎ
ｔ
ａｎａ　
Ｌ　
Ｅ　
Ａ，
Ｄｅ　
Ｏｌ
ｉ
ｖｅ
ｉ
ｒ
ａ　
Ｄ　
Ｆ，
Ｃａｎｕ
ｔ
ｏ　
Ａ　
Ｍ　
Ｐ，
ｅ
ｔ
ａ
ｌ．Ａ　
Ｃｏｍ－
　
ｒ
ａ
ｔ
ｉ
ｖｅ　
Ａｎａ
ｌ
ｉ
ｓ　
ｏ
ｆ　
Ｆｅ
ａ
ｔ
ｕｒ
ｅ　
Ｓｅ
ｌ
ｅ
ｃ
ｔ
ｉ
ｏｎ　
Ｍｍｅ
ｔ
ｈｏｄｓ
ｆ
ｏ
ｒ　
Ｅｎｓ
ｅｍｂ
ｌ
ｅ
ｓ
　
ｐａ
ｙｓ

ｔ
ｅ
ｒ　
Ｅｍｂｅｄｄ
ｉ
ｎｇ ［
Ｊ］．
Ｌｅ
ｃ
ｔ
ｕｒ
ｅ　
Ｎｏ
ｔ
ｅ
ｓ
ｉ
ｎ　
Ｃｏｍｐｕ
ｔ
ｅ
ｒ　
Ｓｃ
ｉ
ｅｎｃ
ｅ，
２０１４，
　

ｗｉ
ｔ
ｈ　
Ｄｉ
ｆ
ｆ
ｅ
ｒ
ｅｎ
ｔ　
Ｃｏｍｂ
ｉ
ｎａ
ｔ
ｉ
ｏｎ　
Ｍｅ
ｔ
ｈｏｄｓ ［
Ｃ］∥Ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌ
Ｊ
ｏ
ｉ
ｎ
ｔ
　

８８３５：
２７９－２８６

Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
ｏｎ　Ｎｅｕｒ
ａ
ｌ　Ｎｅ
ｔｗｏ
ｒｋｓ，
２００７（
Ｉ
ＪＣＮＮ　２００７）．
ＩＥＥＥ，

［
ｓ
ｅｄ　
Ｎｅｕｒ
ａ
ｌ　
Ｌａｎｇｕａｇｅ
Ｂ．Ｆｅ
ａ
ｔ
ｕｒ
ｅ－ｂａ
１３］ Ｍａｎｓｕｒ　
Ｍ，Ｐｅ
ｉ　
Ｗ，Ｃｈａｎｇ　
Ｍｏｄｅ
ｌ
ａｎｄ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ　
Ｗｏ
ｒ
ｄ　
Ｓｅｇｍｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ ［
Ｃ］∥Ｉ
ＪＣＮＬＰ．
２０１３：
　
１２７１－１２７７
［
１４］Ｚｈｅｎｇ　
Ｘ，
Ｃｈｅｎ　
Ｈ，
Ｘｕ　
Ｔ．
Ｄｅ
ｅｐ　
Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ
ｆ
ｏ
ｒ　
Ｃｈ
ｉ
ｎｅ
ｓ
ｅ　
Ｗｏ
ｒ
ｄ　
Ｓｅｇ－
　
ｍｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎ　
ａｎｄ　
ＰＯＳ　
Ｔａｇｇ
ｉ
ｎｇ ［
Ｃ］∥ＥＭＮＬＰ．
２０１３：
６４７－６５７
［
１５］Ｔａｎｇ　
Ｄ，Ｗｅ
ｉ
Ｆ，
Ｙａｎｇ　
Ｎ，
ｅ
ｔ
ａ
ｌ．
Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ　
Ｓｅｎ
ｔ
ｉｍｅｎ
ｔ－Ｓｐｅ
ｃ
ｉ
ｆ
ｉ
ｃ　
Ｗｏ
ｒ
ｄ
　
　
Ｅｍｂｅｄｄ
ｉ
ｎｇ
ｆ
ｏ
ｒ　Ｔｗｉ
ｔ
ｔ
ｅ
ｒ　Ｓｅｎ
ｔ
ｉｍｅｎ
ｔ　Ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ ［Ｃ］∥ ＡＣＬ．
　
２０１４：
１５５５－１５６５
［
１６］Ｚｈａｎｇ　
Ｍ，Ｚｈａｎｇ　
Ｙ，Ｃｈｅ　
Ｗ，ｅ
ｔ
ａ
ｌ．Ｃｈ
ｉ
ｎｅ
ｓ
ｅ　
Ｐａ
ｒ
ｓ
ｉ
ｎｇ　
Ｅｘｐ
ｌ
ｏ
ｉ
ｔ
ｉ
ｎｇ
　
Ｃｈａ
ｒ
ａ
ｃ
ｔ
ｅ
ｒ
ｓ［
Ｃ］∥ＡＣＬ．
２０１３：
１２５－１３４
［
１７］Ｘｉ
ｎｇ　
Ｃ，Ｗａｎｇ　
Ｄ，
Ｚｈａｎｇ　
Ｘ，
ｅ
ｔ
ａ
ｌ．Ｄｏ
ｃｕｍｅｎ
ｔ　
Ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ　
ｗｉ
ｔ
ｈ
　

２００７：
６４３－６４８
［
２４］Ｆｏ
ｒｍａｎ　
Ｇ．Ａｎ　
Ｅｘ
ｔ
ｅｎｓ
ｉ
ｖｅ　
Ｅｍｐ
ｉ
ｒ
ｉ
ｃ
ａ
ｌ
Ｓ
ｔ
ｕｄｙ　
ｏ
ｆ　
Ｆｅ
ａ
ｔ
ｕｒ
ｅ　
Ｓｅ
ｌ
ｅ
ｃ
ｔ
ｉ
ｏｎ
　
Ｍｅ
ｔ
ｒ
ｉ
ｃ
ｓ
ｏ
ｒ　Ｔｅｘ
ｔ　Ｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ ［
Ｊ］．Ｔｈｅ
ｏｕｒ
ｎａ
ｌ
ｆ　Ｍａ
ｃｈ
ｉ
ｎｅ
　ｆ
　Ｊ
　ｏ
Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ　
Ｒｅ
ｓ
ｅ
ａ
ｒ
ｃｈ，
２００３，
３：
１２８９－１３０５
／ｄ
／
／／ｗｗｗ．
［
ｈ
ｔ
ｔ
ｓ
ｏｇｏｕ．
ｃ
ｏｍ／
ｌ
ａｂｓ
ｌ
ＯＬ］．
２５］ 搜狗 ．文本分类语料库［
ｐ：
ｃ．
ｈ
ｔｍｌ
［
／／ｒ
２６］Ｇｅｎｓ
ｉｍ．Ｔｏｐ
ｉ
ｃ　
Ｍｏｄｅ
ｌ
ｌ
ｉ
ｎｇ
ｆ
ｏ
ｒ　
Ｈｕｍａｎｓ ［
ＯＬ］．ｈ
ｔ
ｔ
ａｄ
ｉｍｒ
ｅ－
ｐ：
　
ｅｋ．
ｃ
ｏｍ／ｇｅｎｓ
ｉｍ
ｈｕｒ
［
２７］Ｂｅｎｇ
ｉ
ｏ　
Ｙ，Ｓｃｈｗｅｎｋ　
Ｈ，Ｓｅｎé
ｃ
ａ
ｌ
Ｊ　
Ｓ，ｅ
ｔ
ａ
ｌ．Ｎｅｕｒ
ａ
ｌ
Ｐｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｓ
ｔ
ｉ
ｃ
　
　
　
Ｌａｎｇｕａｇｅ　Ｍｏｄｅ
ｌ
ｓ ［Ｍ ］∥Ｉ
ｎｎｏｖａ
ｔ
ｉ
ｏｎｓ
ｎ　Ｍａ
ｃｈ
ｉ
ｎｅ　Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ．
　ｉ
Ｓｐ
ｒ
ｉ
ｎｇｅ
ｒ　
Ｂｅ
ｒ
ｌ
ｉ
ｎ　
Ｈｅ
ｉ
ｄｅ
ｌ
ｂｅ
ｒ
２００６
ｇ，

Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｉ
ｏｎｓ　
ｏ
ｆ　
Ｗｏ
ｒ
ｄ　
Ｖｅ
ｃ
ｔ
ｏ
ｒ
ｓ［
Ｃ］∥２０１４ Ａｎｎｕａ
ｌ
Ｓｕｍｍｉ
ｔ
ａｎｄ
　
　

［
２８］ Ｍｎ
ｉ
ｈ　
Ａ，
Ｈｉ
ｎ
ｔ
ｏｎ　
Ｇ．
Ｔｈｒ
ｅ
ｅ　
Ｎｅｗ　
Ｇｒ
ａｐｈ
ｉ
ｃ
ａ
ｌ　
Ｍｏｄｅ
ｌ
ｓ
ｆ
ｏ
ｒ　
Ｓ
ｔ
ａ
ｔ
ｉ
ｓ
ｔ
ｉ
ｃ
ａ
ｌ
　

ｃ
ｉ
ｆ
ｉ
ｃ　
Ｓ
ｉ
ｌ
ａｎｄ
Ｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ　
Ｐｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ　
Ａｓ－
Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
Ａｓ
ｉ
ａ－Ｐａ
　
　
ｇｎａ

Ｌａｎｇｕａｇｅ　
Ｍｏｄｅ
ｌ
ｌ
ｉ
ｎｇ ［
Ｃ］∥Ｐｒ
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓ　
ｏ
ｆ
ｔ
ｈｅ
２４
ｔ
ｈ
Ｉ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌ
　
　
　

ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏｎ（
ＡＰＳ
ＩＰＡ）．
ＩＥＥＥ，
２０１４：
１－５

Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ　
ｏｎ　
Ｍａ
ｃｈ
ｉ
ｎｅ　
Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇ．
ＡＣＭ，
２００７：
６４１－６４８

· ２６９ ·