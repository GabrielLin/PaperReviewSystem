 28   4 
2015  4 


PＲ ＆ AI

Vol． 28
Apr．












（  





No． 4
2015

*


 450001）

 N-gram ，

． ． 
，． ，，． 
，．


，，，



TP 391

DOI 10． 16451 / j． cnki． issn1003-6059． 201504002

Ｒecurrent Neural Network Language Model Based on
Word Vector Features
ZHANG Jian，QU Dan，LI Zhen
（ Institute of Information System Engineering，The PLA Information Engineering University，
Zhengzhou 450001）

ABSTＲACT
The recurrent neural network language model（ ＲNNLM） solves the problems of data sparseness and
dimensionality disaster in traditional N-gram models． However，the original ＲNNLM is still lack of long
dependence due to the vanishing gradient problem． In this paper，an improved method based on
contextual word vectors is proposed for ＲNNLM． To improve the structure of models，a feature layer is
added into the input layer． Contextual word vectors are added into the model with feature layer to
reinforce the ability of learning long－distance information during the training． Experimental results show
that the proposed method effectively improves the performance of ＲNNLM．
Ke y Words

Speech Ｒecognition，Language Model，Ｒecurrent Neural Network，Word Vector

*  863 （ No． 2012AA011603） 、（ No． 61175017） 
： 2014－02－27； ： 2014－03－27
1988 ，，、． E-mail： Crsmx_23@ 163． com． ，
 ，，
1974 ，，，、． ，，
1982 ，，，
，
、．



300

1



28 

，
． 




，、、
， N-gram ．
 N-gram ，
，， （ Neural
Network Language Model，NNLM） 
［1］



．

 （ Deep Learning）    
，，
［2］

． Bengio  
，
（ Distributed Ｒepresentation） 
， ．
 N-gram
，，
Associated Press （ AP） News 
， Kneser-Ney  N-gram ，NNLM 
．

， N-gram 
［9］
， ．
        （ Continuous Bags-of-Words，
CBOW）  Skip-gram 
，， Penn
Treebank ，
 Wall Street Journal ．
， ，
，
 ．

2


ＲNNLM （ Input Layer） 、

 （ Hidden Layer）  （ Output Layer） 3
， w（ t）  t ，
One-hot （  1-of-N ） ，
．  s（ t） ， t 
， y（ t） 


N－1 ，
［3－4］
． ，Mikolov 


． U、V、W 
．

，
，，

 t ， w（ t） 
 s（ t － 1） ，

 （ Ｒecurrent NNLM，ＲNNLM） ，
． ＲNNLM 

，，
，，

，
． ［5］，

，
［10 －11］
．
、、

 （ Vanishing Gradient） ，
 ． 

，
，

 ＲNNLM  N  8  9
［6］
 ．

． ，

， ＲNNLM 
． 
 2 ： 1） ，，
，，
［7］

， Hessian-free  ； 2）
 Long Short-Term Memory ，
 （ Gating Neurons） 
［8］
 ．


， ，
，
，
． 
，
， ，
 ．
，，
 （ Feature Layer） ， F、G

，
． 

， 1 ．

，
 ，


x（ t） = w（ t） + s（ t － 1） + f（ t） ．

4





： 

301

， f（ t） 
，，

［12］
 ，    Word
      Hinton
Ｒepresentation． 

w（ t） ，． 
， y（ t） 
 w（ t） 、 s（ t －1） 

， ，
，．

 f（ t） ．  ＲNNLM 

，
． ，，


s（ t） = f（ Uw（ t） + Ws（ t － 1） + Ff（ t） ） ，

 （ Latent Semantic Analysis，LSA） 
        （ Latent Dirichlet Allocation，

y（ t） = g（ Vs（ t） + Gf（ t） ） ，
，f（ z）  sigmoid ：
f（ z） = （ 1 + e －z ） －1 ，
g（ z）  softmax ，
g（ z m ） =

ezm
．
∑ ezm
k

LDA） ，，
．
 Skip-gram 
 CBOW  Skip-gram 
［13］
．  Mikolov  ，

3． 2

 ． 
，
， Log-linear ，
        ，       
N-gram，
， 2 ．

1
Fig． 1

 ＲNNLM 

Structure of ＲNNLM with feature layer

3



3． 1



，
，
， One-hot ．
 ，
， 0，
 1，．
， One-hot 
， ID．
，
， N-gram 

（ a） CBOW
2
Fig． 2

（ b） Skip-gram

CBOW  Skip-gram 

Architectures of CBOW and Skip-gram model

，CBOW 
，，、
（ Projection Layer） ． 、
，，
 50  100， D， C 
．  ： 

．  ： 

 ，
，，

，
， ； 

． 
，

，， N-gram ，
．

． ，
， w（ t+1） 、w（ t+2） 

      ，        
（ Distributed Ｒepresentation）   ，

 w（ t） ，
．



302

Skip-gram  CBOW ，
 w（ t） ，
，，
 ．
，CBOW 
，，

28 

， 30 ． 
，
， ，
．
 3  ＲNNLM，
 10  100， 100，

； Skip-gram 
，，

， 1 ．
， Kneser-Ney  5-

．

gram ．


［14］

，Mikolov 

 1 000 ，
，． 
 ，
． ，
 ，
 ．

4



，
， Penn Treebank 
， Wall Street Journal 
． 
 （ Perplexity，PPL） 
 （ Word Error Ｒate，WEＲ） ． 
．
4． 1 Penn Treebank 
4． 1． 1

1

，



Penn Treebank ［15］ 
，，
、，
． 
（ 、，
） ， ．
 Penn Treebank ： 0 ～ 20 
， 93  Tokens； 21 ～ 22 
，
， 7． 4  Tokens； 23 ～ 24 
， 8． 2  Tokens；  1
，〈unk〉．

 CBOW  Skip-gram 

Table 1

Penn Treebank 

Experimental results of perplexity test on Penn Treebank
corpus

5-gram （ KN5）


 
148． 0
141． 2

ＲNN 10
ＲNN 10 + CBOW 50
ＲNN 10 + Skip-gram 50

252． 8
202． 9
187． 1

239． 3
192． 2
179． 1

ＲNN 100
ＲNN 100 + CBOW 50
ＲNN 100 + Skip-gram 50

158． 9
140． 9
133． 2

151． 1
134． 6
127． 0

 1 ，KN5     Kneser-Ney    5gram ，ＲNN 10  ＲNN 100 
     10  100         ，
CBOW 50  Skip-gram 50 
50 ． ＲNN +CBOW、ＲNN +Skip-gram 
                 
．
 1 ，
，
．  （ 
 10） ， ．
，ＲNN-CBOW  ＲNN ，
   252． 8   202． 9，  19． 7% ；  
Skip-gram ， 187． 1，
， 26% ． ，
，
    ．     ，       
ＲNN ， 11%  16． 3% ．
，
． ，

4． 1． 2

［16］，ＲNNLM  300，
 50， 30． 

 （  50） 
 10 ～ 50 ，

， Kneser-Ney  5-gram
，． 

4





： 

 2 ．
2
Table 2

303

 13  MFCC 
   ， 39         ．  


Performance comparison of different language models
on test set



+ KN5

Hamming ， 25 ms， 10 ms 
， 187 ，
 （ HMM） ． 
，，

5-gram （ GT5）
5-gram （ KN5）


165． 2
141． 2

－
－

（ Triphone） 
， SＲILM 





142． 1
131． 9
146． 1

138． 7
131． 3
125． 5

 Kneser-Ney  3-gram  4-gram
．




ＲNN + CBOW LM
ＲNN + Skip-gram LM

140． 2
131． 3
124． 7

116． 7
110． 0
105． 7

118． 7
112． 6

99． 4
97． 3

 2 ． Ngram  SＲILM ，
，，Kneser-Ney  （ KN5） 
 Good-Turing  （ GT5） ． 、
 N-gram ，
 N-gram ． 
 （ Feedforward Neural Network
LM） 、 （ Syntactical Neural Network
LM） ．
，
 N-gram ，

［17］

 Kaldi 



， 3-gram，
（ Weighted Finite State Transducer，WFST） 
，， Latticeto-nbest  N-best ，，
Kneser-Ney  4-gram 
 N-best ，． 
（ WEＲ） ．
  ，ＲNNLM        100 
200， 50， 30，
．  3 
．
3
Table 3



WEＲ of different language models on Microsoft corpus


4-gram

 / %
16． 45
16． 31

ＲNN ，． ＲNN + Skip-gram 
 （ 112． 6  97． 3） 

ＲNN 100
ＲNN 100 + CBOW 50
ＲNN 100 + Skip-gram 50

15． 91
15． 90
15． 76

，，
．

ＲNN 200
ＲNN 200 + CBOW 50
ＲNN 200 + Skip-gram 50

15． 64
15． 63
15． 45


，． 

4． 2


   ，           

——— Wall Street Journal（ WSJ） 
，
． 
．

 3   ，  Skip-gram     
ＲNNLM ． 
， 100 ，
15． 76% ， ＲNNLM ， 0． 97% ； 

4． 2． 1

， 4． 19% ．  200
，ＲNN-Skip-gram          
15． 45% ， ＲNN  1． 21% ； 

， 100 
 18 ～ 40 ， 19 688 ，

， 6． 08% ．  Skip-gram 
 ＲNNLM ，

33 h ；  25 ，
500 ；  16 kHz， 16 bit 

 CBOW  ＲNN 
，

（ Pulse Code Modulation，PCM） ．

．


 Speech Corpora （ Version 1． 0） 



304

 WSJ 
Wall Street Journal（ WSJ） 

4． 2． 2

，
．  WSJ0  WSJ1 
， 81． 3 h ， Eval92  Dev93
．
， WSJ-20K 
， SI-284 
 HMM-GMM ， Kaldi 
， 4 368 ．
 37 MB  Tokens．
，
，
50， 30．  30、
100、
200  300，
 3 ．

28 

， Eval92  Dev93 
     ＲNNLM        ． 
Eval92 ，  30 ，  Skipgram  ＲNNLM  7． 67% ，
 ＲNN  1． 41%  12． 2% ；
     100 ，     7． 21% ， 
ＲNNLM  2． 2%  17． 5% ． 
Dev93  ．
4
Table 4

 WSJ 
WEＲ of different language models on WST corpus
%


Eval92

Dev93


4-gram

8． 74
8． 35

12． 29
11． 57

ＲNN 30
ＲNN 30 + CBOW 50
ＲNN 30 + Skip-gram 50

7． 78
7． 64
7． 67

11． 39
11． 28
11． 23

ＲNN 100
ＲNN 100 + CBOW 50
ＲNN 100 + Skip-gram 50

7． 37
7． 30
7． 21

11． 26
10． 77
10． 93

ＲNN-CBOW 
．  Eval92 ， 30 ，
 7． 64% ， ＲNN  1．
8%  12． 6% ；  100 ， ＲNN
 0． 95%  16． 5% ．
3
Fig． 3

 ＲNNLM 
Perplexity of ＲNNLM with different hidden layer sizes
on valid data

          ，    
WSJ ． 
，，
，

 3 ，
 ＲNNLM ， Skip-gram 

．  ＲNN-CBOW ，
，

 CBOW ． ，
，

，，
 ．

， ． 
，

，
，

，． 
，．

， WSJ 
．  NIST  SCTK 

，
 30  100 ，

（ Significance Test） ，： 
    （ Matched Pair Sentence Segment Word

100-Best         ，   Eval92 
Dev93 ，

Error） ， MP ； 
   （ Signed Paired Comparison Speaker Word

．  4 ．  4 ，
4-gram  4-gram 
，

Accuracy） ， SI ； Wilcoxin 
（ Wilcoxin Signed Ｒank Speaker Word Ac-

．
 4 ，

curacy） ， WI ．  5 ．
 5 

4





： 

，，“MP ∶ ＲNN-CBOW” MP ，
ＲNN-CBOW ； “MP ∶ ＲNN” MP
，ＲNN ，
．  5 
3 ， 5% 
，
， ＲNN 
 N-gram  ＲNN ，
．
5
Table 5



Significance test results for WEＲ of different language
models

 N-gram

ＲNNLM

305

Czech Ｒepublic，2011： 5528－5531
［5］ Bengio Y，Simard P，Frasconi P． Learning Long-Term Dependencies with Gradient Descent Is Difficult． IEEE Trans on Neural Networks，1994，5（ 2） ： 157－166
［6］ Son L H，Allauzen A，Yvon F． Measuring the Influence of Long
Ｒange Dependencies with Neural Network Language Models / / Proc
of the NAACL-HLT Workshop： Will We Ever Ｒeally Ｒeplace the Ngram Model？ On the Future of Language Modeling for HLT． Montreal，Canada，2012： 1－10
［7］ Martens J，Sutskever I． Learning Ｒecurrent Neural Networks with
Hessian-Free Optimization ［EB / OL］． ［2014 － 02 － 10］． http： / /
www． icml-2011． org / papers /532_icmlpaper． pdf
［8］ Sundermeyer M，Schlüter Ｒ，Ney H． LSTM Neural Networks for Language Modeling［EB / OL］．［2014－02－10］． http： / / www－i6． informatik．

ＲNN-CBOW

ＲNN-Skip-gram

MP ∶ ＲNN-CBOW
SI ∶ ＲNN-CBOW
WI ∶ ＲNN-CBOW

MP ∶ ＲNN-Skip-gram
SI ∶ ＲNN-Skip-gram
WI ∶ ＲNN-Skip-gram

MP ∶ ＲNN-CBOW MP ∶ ＲNN-Skip-gram
SI ∶ ＲNN-Skip-gram
SI ∶ ＲNN-CBOW
WI ∶ ＲNN-CBOW WI ∶ ＲNN-Skip-gram

rwth － aachen． de / publications / download /820 / Sundermeyer － 2012．
pdf
［9］ Shi Y，Wiggers P，Jonker C M． Towards Ｒecurrent Neural Networks
Language Models with Linguistic and Contextual Features / / Proc of
the 13th Annual Conference of the International Speech Communication Association． Portland，USA，2012： 1664－1667
［10］Auli M，Galley M，Quirk C，et al． Joint Language and Translation

5

  

Modeling with Ｒecurrent Neural Networks / / Proc of the Conference on Empirical Methods in Natural Language Processing． Sea-


，
，
， ．
 ，
． ，
： ，
，．

ttle，USA，2013： 1044－1054
［11］ Yao K，Zweig G，Hwang M Y，et al． Ｒecurrent Neural Networks
for Language Understanding ［EB / OL］． ［2014－02－10］． http： / /
research． microsoft． com / pubs /200236 / ＲNN4LU． pdf
［12］ Hinton G E． Learning Distributed Ｒepresentations of Concepts / /
Proc of the 8th Annual Conference of the Cognitive Science Society． Amherst，USA，1986： 1－12
［13］ Mikolov T，Chen K，Corrado G，et al． Efficient Estimation of
Word Ｒepresentations in Vector Space［EB / OL］． ［2014 － 02 －
10］． http： / / arxiv． org / pdf /1301． 3781． pdf
［14］ Mikolov T，Sutskever I，Chen K，et al． Distributed Ｒepresenta-









tions of Words and Phrases and Their Compositionality ［EB / OL］．
［2014－02－10］． http： / / papers． nips． cc / paper /5021-distributed-

［1］ Schwenk H． Continuous Space Language Models． Computer Speech
and Language，2007，21（ 3） ： 492－518

representations-of -words -and -phrases -and -their -compositionality ．
pdf

［2］ Bengio Y，Ducharme Ｒ，Vincent P，et al． A Neural Probabilistic

［15］ Marcus M P，Marcinkiewicz M A，Santorini B． Building a Large

Language Model． Journal of Machine Learning Ｒesearch，2003，3：

Annotated Corpus of English： the Penn Treebank． Computational

1137－1155

Linguistics，1993，19（ 2） ： 313－330

［3］ Mikolov T，Karafiát M，Burget L，et al． Ｒecurrent Neural Network

［16］ Mikolov T，Deoras A，Kombrink S，et al． Empirical Evaluation

Based Language Model / / Proc of the 11th Annual Conference of the

and Combination of Advanced Language Modeling Techniques

International Speech Communication Association． Makuhari，Japan，

［EB / OL］． ［2014 － 02 － 14］． http： / / www． fit． vutbr． cz / ～

2010： 1045－1048
［4］ Mikolov T，Kombrink S，Burget L，et al． Extensions of Ｒecurrent

imikolov / ～ rnnlm / is 2011_emp． pdf
［17］ Povey D，Ghoshal A，Boulianne G，et al． The Kaldi Speech Ｒe-

Neural Network Language Model / / Proc of the IEEE International

cognition Toolkit ［EB / OL］．［2014－02－10］． http： / / homepages．

Conference on Acoustics ，Speech and Signal Processing ． Prague ，

inf． ed． ac． uk / aghoshal / pubs / asru11-kaldi． pdf