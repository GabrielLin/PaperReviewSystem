
Journal of Northwestern Polytechnical University

2016  8 
 34  4 

Aug． 2016
Vol． 34 No． 4

 LDA 
，，，，
（  ，  710072）

： 。
，



LDA( latent dirichlet allocation)  CBOW-LDA ，
 CBOW( continuous bag-of-word) ，
 LDA 
，
。，
 LDA
，
 3% 。
： ; LDA ; ; 
： TP391
： A
： 1000-2758（2016）04-0698-05





，、
。
。LDA（ latent dirichlet allocation，
） 
， （  ）
 ， LDA 
， LDA 

［1-4］
。

CBOW  Mikolov ［2］  2013  
 。
，
。
。
，
［4］
 。
，
 LDA ，
 CBOW 
， LDA 
 。

1



［4］ LDA  VSM（ vector space
model， ） 
。 TF-IDF ， 2 
，
。TF-IDF 
，。

，［5］
，
 LDA 
。，
。
 LDA 
 。
，LDA                 
，；

。
 LDA ：

： 2016-03-19
61303224、
61403311） （20155553036、
2013ZC53034） 
： （61402373、
： （1987—） ，，。

，： LDA 

4 

· 699·

1） ，、
、，
 。
2） ，
 （  ） ，
 LDA ，。

2
2． 1

 CBOW-LDA 



 。
，，
。
，
， 1 。

2

CBOW-LDA 

， 。
CBOW 
，
，
。 ，
 。

1


3

2． 2

，CBOW-LDA 
 。
 ，
 LDA ，
。 LDA 
，，
LDA                 
。
2． 3

CBOW 

CBOW-LDA 


 one-hot representation 

，CBOW  distributed representation 
［7］



，

 C  t  C（ t） 
， W（ t）， W（ t） 
 W（ t） 。
1） ， k 
， C（ t － k） ，
C（ t － k + 1） ，…，C（ t + k － 1） ，C（ t + k） ， hash
 W（ t － k） ，W（ t － k + 1） ，
…，W（ t + k － 1） ，W（ t + k） ，
W（ t）  Context（ W（ t） ） ， CBOW 
           W（ t）    
Context（ W（ t） ） 。
2） ， W（ t） 



· 700·









 Context（ W（ t） ） 。





 34 



［1，N］） 。 θ  Multi（ θ） 

（1）

 z， － 。 z 
 Multi（ φ）  w i ，

3 ）              ， 
W（ t）  Context（ W（ t） ） 

 － 。
 Gibbs  θ  φ 。

 P（ W（ t） | Context（ W（ t） ） ） ， W（ t）
。


p（ z i ，z －i ，w | α，β）
p（ z i | z －i ，w，α，β） =
p（ z －i ，w | α，β）

t +k

V（ t） =

Context（ W（ t） ）
∑
t －k

CBOW 
L =

lnP（ W（ t） |
∑
w∈C

Context（ W（ t） ） ）

（2）

∝

 。
 （ negative sampling） 
［8］




lnσ（ v'

n k－i，m + α k

∝

k
－i，m

n w－i，k + β w

．

K

（n
∑
k =1

·v w（ t） ） +

K

（5）

 θ  φ ，
p（ z i = k | z －i ，w，α，β）

。，lnp（ w t + j | w t ）

T
w（ t +j）

p（ z，w | α，β）
p（ z －i ，w | α，β）

+ αk ）

（6）

V

（n
∑
w =1

w
－i，k

+ βw ）

（3）

n k－i，m  m  k ；

E w k ～ P v（ w）  Huffman 

n w－i，k  w i  k ； 
 k ，

Ew ～ P
∑
k =1
k

ln（ σ（ － v'wTk ·v w（ t） ） ）
v（ w）

，P v（ w） ，W k 
  Huffman             
。
 Huffman ，
 。
 cos ，
     ，  LDA        
。
2． 4 

n km + α

θ m，k =

K

n
∑
k =1

k
m

，φ k，w =

n wk + β
n
∑
w =1

+ kα

（7）

V
w
k

+ Wβ

w

，V ；n k  w  k 
k

；n m  k  m 。
， CBOW-LDA 
， LDA 
 － ， θ  φ
，。

LDA  －  －  3
［9］

，

。 

3



，CBOW-LDA  LDA 
 － ，

3． 1

 LDA  。
LDA ，

 （ perplexity） ，，

 p（ z） 。LDA 

，          。      
（8） 

N

p（ θ，z | w，α，β） = p（ θ | α） ∏ p（ z n | θ） p（ w n | z n ，β）



{

n =1

（4）
：
1）  z， Dirichlet  Dir（ β） 
 φ；
2）  Dirichlet  Dir（ α） 
 θ；
3）  N  w i （ i ∈

Perplexity（ W） = exp －

M

ln（ p（ w m ） ）
∑
m =1
M

Nm
∑
m =1

}

（8）

，W ，w m  m 
，p（ w m ）  w m ，N m 
 m 。

，： LDA 

4 

3． 2

2



 IT 
，


CBOW-LDA

。  6 000    
2015  3  2015  4  30  43
。， 14 536
， 233 296。
3． 3



，“Jieba”
          ，       
46 516 ；  CBOW-LDA 
Word2vec 0． 8 （2015  7  ） 
。
Word2vec  1 ， Cbow =
1  CBOW ，Hs = 0 

20

TF-LDA

· 701·

2 
25

30

35

40

45

50

55

60

151 138 125 116 112 106 100 96

92

155 143 131 120 111 109 103 100 95

，，
。，
 20 ～ 60 ，CBOW-LDA 
 3% 。
， K  30，
 4 。
 500 ，，
 300 。

。 ，
 0． 75。
1

Word2vec 







Size



50

Window



5

Sample



0

Cbow

 Cbow 

1

Hs

 Softmax

0

CBOW-LDA 
lda 1． 0． 3  Python  LDA 。
，。 

4



 CBOW-LDA 
 TF-LDA ，
，。

 α  β ： α = 50 /
K，β = 0． 01。K ，
       。 Gibbs       
500 。
， Gibbs
。
θ  φ ，，
。
3． 4


5

，
。［4］
 TF-IDF  LDA  （ 
TF-LDA） ，
2 
 2 。

4







，



· 702·









 LDA 
。  LDA 
，







 34 

，
。 
，。

：
［1］ Cheng Xueqi，Yan Xiaohui，Lan Yanyan，et al． BTM： Topic Modeling Over Short Texts［J］． IEEE Trans on Knowledge and
Data Engineering，2014，26（12） ： 2928-2941
［2］ Mikolow Tomas，Yih Wentau Scott，Zweiq Geoffery． Linguistic Reqularities in Contrmcous Space Word Representations［C］∥
Proceedings of the 12nd Conference of the North Anerican Chapter of the Association for Computational Linguistics，Atlanta，
USA： NAACL，2013
［3］ Dermouche M，Velcin J，Khouas L，et al． A Joint Model for Topic-Sentiment Evolution Over Time［C］∥Proceedings of 14 th
IEEE International Conference on Data Mining． Shenzhen，China，2014
［4］ Huang Bo，Yang Yan，Mahmood Amjad，et al． Microblog Topic Detection Based on LDA Model and Single-Pass Clustering［C］
th
∥Proceedings of 7 International Conference on Rough Sets and Current Trends in Computing． Chengdu，China，2012

［5］ Darling M William，Song Fei． Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA［J］． ArXiv：1303． 2826，2013
［6］ Bai Xue，Chen Fu，Zhan Shaobin． A New Clustering Model Based on Word2vec Mining on Sina Weibo Users' Tags［J］． International Journal of Grid Distribution Computing，2014，7（3） ： 41-48
［7］ Zhou Xinjie，Wan Xiaojun，Xiao Jianguo． Repre-Sntation Learning for Aspect Category Detection in Online Reviews［C］． Proceedings of the 29 th AAAI Conference on Artificial Intelligence． Austin，Texas，USA，2015
［8］ Mikolov Tomas，Sutskever Hya． Distributed Representutions of Words and Phrases and Their Compositionality［C］∥Proceedings
of the Ilth Newral Information Processing Systems Conference Lake Tahoe，USA： NIPS，2013
［9］ Cao Ziqiang，Li Sujian，Liu Yang，et al． A Novel Neural Topic Model and Its Supervised Extension［C］∥Proceedings of the
29 th AAAI Conference on Artificial Intelligence． Austin，Texas，USA，2015

A LDA Model Based Topic Detection Method
Guo Lantian，Li Yang，Mu Dejun，Yang Tao，Li Zhe
（ School of Automation，Northwestern Polytechnical University，Xi＇an 710072，China）

Abstract： Topic Detection is one of the most important techniques in hot topic extraction and evolution tracking．
Due to the high dimensionality problem which hinders processing efficiency and topics mal-distribution problem
which makes topics unclear，it is difficult to detect topics from a large number of short texts in social network． To
address these challenges，we proposed a new LDA （ Latent Dirichlet Allocation） model based topic detection method called CBOW-LDA topic modeling method． It utilizes a CBOW（ Continuous Bag-of-Word） method to cluster the
words，which generate word vectors and clustering by vectors similarity． This method decreases the dimensions of
LDA output，and makes topic more clearly． Through the analysis of topic perplexity in the real-world dataset，it is
obvious that topics detected by our method has a lower perplexity，comparing with word frequency weighing based
vectors． In a condition of same number of topic words，perplexity is reduced by about 3% ．
Keywords： word vectors； LDA model； topic detection； perplexity