

SCI-TECH INFORMATION DEVELOPMENT ＆ ECONOMY

2015   25   2 

：
1005－6033（2015）02－0145-04

：2014－11－18

Word2vec 
 
（，，
710071）


： Word2vec ，，

，、Log＿Linear  Log＿Bilinear 
， Word2vec ，
， Word2vec ， Word2vec
。
：
Word2vec；；

：
TP317

1

：A
，



。

，
。、
。

2
2.1




，

。

。

w2，…，wn）， p
 n  S=（w1，

［1］。

（S）。， p（S）

。， n－gram、

， p（S）=p（w1）
·p（w2｜w1）
·p（w3｜w12）…p（wn｜w1n-1）。，

 log＿linear 。

，，

，、、

， p（wi｜context（w）

i ）。

。

 p
 p（wi ｜context（w）
i ），

。

（S）。， p

，Google  2013
 Word2vec



［2］

。

［3］

（wi｜context（w）
i ）。
2.2

Word2vec ，


，。

44444444444444444444444444444444444444444

（），，，

────────────────
：

，，1980  11 ，2007 

 2 ，300402．

Cleaning off the English Obstacles in Literature Retrieval
Course in Higher Vocational College
JIA Ru
ABSTRACT: Higher vocational college students’English level will directly influence the teaching effects of literature
retrieval course. According to the characteristics of literature retrieval course and the status of students’English level, this
paper puts forward some suggestions on improving the teaching methods of literature retrieval course, which include
applying the English education through in the whole course, combining the literature retrieval course with the professional
English course, and carrying out the bilingual teaching if possible.
KEY WORDS: literature retrieval course; English obstacles; higher vocational college
145

Ä
Ã
Å
Æ
Ç


Â

È
É

Á
Ä
Å
Æ
Ç
ÁÂÃÂÃÈÉÁÂÃÁÂÃÄÅÆÇÈÉÂÄÅÃÁÂÄÅÃÆÇÈÉÂÄÅÃÁÂ

 

Word2vec 

 E-mail:bjb＠sxinfo．net



［3］，

 Word2vec 。［6］，Log＿Linear 

。，

：

。，
distributed representation 

（1） X；

。 distributed representation 、
。

（2） Y；

distributed representation ，

（3） d；

               。   ，distributed

representation  distributed ：

（4）（x，
y） （
f x，y） f：
X×

Y→Rd；

（5） v∈Rd。

。


2.3

2003 ，Bengio 

Log＿Linear ， x∈X，

 3 

［4］

 NNLM （Neural Network Language Model）， 1 
。NNLM  wt ， p
（wt=i｜context），。

y∈Y，p（y｜x；
v）=


tanh

。，exp（x）=ex，v·（
f x，y）=
f x，
y′）
）
Σexp（v·（

y′∈Y

y）。
∑dk=1vkf（x，
k

（
f x，y）（x，y），
y）
（
f x，
y） f（x，
k

 i =P（wi =i｜context）
softmax

exp（v·（
f x，y））

， v  vk 。v 

 vk 。

 Log＿Linear ，Hinton［7］ Log＿Bilinear 

。Log＿Bilinear  Log＿Linear 

（
f x，y）。 Log＿Bilinear ，
（
f x，
y）（x，y）

C（wt-2） C（wt-1）

C（wt-n+1）

 d ， Log＿Bilinear  y

。，

 C


wt-n+1 

wt-2 

1

wt-1 



NNLM  C  V。V 
 i。，

。

： C  wi， context（w）
i （
 n-1 ），（context（w）
w）

i ，
i 。

Hinton  Log＿Bilinear ， Word2vec 

。

3

3.1

Word2vec 
Word2vec 

Word2vec  Mikolov ［8－9］，

。Word2vec ，

 CBOW  Skip＿gram， 2 。

。 1 ，
，NNLM

INPUT PROJECTION OUTPUT INPUT PROJECTION OUTPUT
w（t-2）
w（t-2）

。NNLM 
 C（w）
i ，

w（t-1）

    softmax       p （wt ｜wt -1， … ，wt -n +2，
wt -n +1）=
e

ywt

∑e
i

yi

， yi  i  log 。

w（t-1）

SUM

w（t） w（t）

w（t+1）

w（t+1）

w（t+2）

w（t+2）

CBOW

2

，
，softmax 。
，Bengio 

 NNLM ，

［5］

 NNLM 。

。，

Skip-gram

CBOW  Skip＿gram 

 2 ，CBOW  Skip＿gram 、

。，CBOW ，
Skip＿gram 。Word2vec 

10 000 ， p（wt｜context），

                 ，    Hierachy

NNLM  wt ，

Softmax  Negative Sampling。

 10 000 。，

 4 ， 1 。

 100 ， 100 。 wt 

3.2

，
 200 ， 50。

3.2.1 CBOW＋HS  Skip＿gram＋HS

。
2.4
146

Word2vec 

 Word2vec ［10］  CBOW＋HS  Skip＿

Log＿Linear  Log＿Bilinear 

gram＋HS ， 3 。［11］，

Log＿Linear 。，Log＿Linear 

， 2 。

Á
Â
Ã
Ä
Å
Æ
Ç
È
É




Á
Â
Ã
Â
Á
ÄÅÆ
Word2vec 

 

 E-mail:bjb＠sxinfo．net

 1 word2vec 

，vTw  context（w），



CBOW

Skip_gram

Hierachy Softmax

CBOW+HS

Skip_gram+HS

Negative Sampling

CBOW+NS

Skip_gram+NS

CBOW

 v ( context ( w))

v (context ( w))





Skip＿gram＋HS  w 。Skip＿gram＋HS 

 p（context（w）｜w）=

CBOW＋HS  Skip＿gram＋HS 

v ( w)

v

p（u｜w）。

3.2.2 CBOW＋NS  Skip＿gram＋NS

Skip_gram

v (context ( w))

∏

u∈context（w）

。，CBOW＋NS  Skip＿gram＋NS 

，

v

。

 CBOW＋NS ，（context（w），w）， w 

，。



。 context（w）
 NEG（w）， w′， w′＝w，

...

3

1 ， 0 。， 1，

CBOW＋HS  Skip＿gram＋HS 

 0。 CBOW＋HS 

 2 CBOW＋HS  Skip＿gram＋HS 




CBOW+HS

，


 （context（w），
w）

。

Skip_gram+HS

 p（w）=

  2c 

 w 

p（u｜context（w））。

 Skip＿gram＋NS ，（w，context（w）
）。

（context（w），w）

  w  c   w 

∏

u∈｛w｝∪NEG（w）

，


 w′， w′＝w， 1 ， 0 
。 CBOW＋HS 
。 p （w）=

（）

。
。 N ，


  N-1 。。
，

∏

∏

p（w″｜w）。

w'∈context（w）w″∈｛w′｝∪NEG（w′）

4

Word2vec 
Word2vec 

。

4.1



，

4.1.1 Word2vec 



。

［10］ Word2vec  C 

，
。

 linux 。，

，。 CBOW＋HS 

Word2vec.c、
demo－word.sh  distance.c 3 。Word2vec.c 

，（context（w），w），

 Word2vec ，demo－word.sh 

 w ，

， distance.c 

 θ  l 。

。 linux ， demo－word.sh 

，j  w 。

 Word2vec 。distance.c 

 l ， 1， 0。，

，

， w。

。

，。

4.1.2 Word2vec 

w
j

w
j

w
j

， σ（vTwθ）＝

1
-v θ ，
1+e
T
ω

Word2vec 。
。Word2vec

vTw  context（w）
 1-σ（vTwθ）。，

 3 。

， θ 。

4.2

 w 
 p（w｜context（w）
）。

 Word2vec 
，Word2vec 

。，，
Word2vec 。

 Skip＿gram＋HS，（w，
context（w）），

， Word2vec ，

 context（w） 2c 。 w  context（w）

。 2012  6  7 

， p（context（w）｜w） 2c  w  u 

。 Word2vec 

 p（u｜w）， u∈context（w）。p（u｜w） CBOW＋HS 

，，

， u 。，
 CBOW＋HS

4 。
147

Word2vec 

 

3

train

Word2vec 







output



window 

threads

binary

hs




 E-mail:bjb＠sxinfo．net

min-count

   binary


cbow

https：
//code.google. com/p/ word2vec/.







size


   
softmax 

negative



alpha


   
CBOW 



classes



［3］

Joseph

Turian，Lev

Ratinov，Yoshua

Bengio.Word



representations：a simple and general method for semi －supervised



learning ［G］//Proceedings of the 48th Annual Meeting of the



Association for Computational Linguistics.Uppsala，Sweden：
［s.n］，







2010：384－394.
［4］ Yoshua Bengio，
Rejean Ducharme，Pascal Vincent，Christian
Jauvin. A neural probabilistic language model［J］.Journal of Machine
Learning Research，2003（3）：
1137－1155.
［5］ Morin，F，Bengio Y. Hierachical probabilistic neural network
language model ［C］.AISTATS，Hastings，
Barbados，
January，06 －
08，2005.
［6］ Michael Collins. Log－linear models［EB/OL］.［2014－09－15］.

4

http：//www.cs.columbia.edu/ ～mcollins/ logline ar.pdf.



，
。 5 “” 5
。

［7］ Mnih A，
Hinton G. Three new graphical models for statistical
language modelling ［G］// Proceedings of the 24th international
conference on Machine learning.［S.l］：
［s.n］，2007：641-648.
［8］

Tomas Mikolov，
Kai Chen，
Greg Corrado，et al. Efficient

estimation of word representations in vector space［EB/OL］.［2014－
09－19］. http：
//arxiv.org/abs/1301.3781v3.
［9］ Mikolov T，
Sutskever I，Chen K，
et al.Distributed representations
of words and phrases and their compositionality［C］//NIPS，Lake
5

5


“” 5 

Tahoe，USA，December，
05－08，2013.
［10］ Tomas Mikolov.Word2vec code ［CP/OL］.［2014 －09 －18］.



http：//word2vec.googlecode.com/ svn/trunk/.

Word2vec                ，   
CBOW  Skip＿gram 。 hierarchy softmax 

［11］ .Word2vec  ［EB/OL］［2014－11－15］
.
.
http：//blog.csdn.net/itplus/article/details/37969519.
（：
）

negative sampling ，Word2vec 
。，

────────────────
： ，，
1988  9 ，

，，
。Word2vec ，

 2012 ，

。

 2 ，
710071.


［1］ .［M］.：
，2012.
［2］ Tomas Mikolov.Word2vec project ［EB/OL］.［2014 －09 －18］.

Exploration of the Working Principle and Application of Word2vec
ZHOU Lian
ABSTRACT：This paper studies the working principle and application of Word2vec，defines the key problems of
statistical language model, analyzes the characteristics of word vector, probes into the basic principles of neural network
language model，Log＿Linear model and Log＿Bilinear model，
makes a detailed analysi on the working principle of word
vector’s training framework of word2vec, and derives the objective functions of the training models，
and introduces the
main files in Word2vec project and training parameters, and applies Word2vec into the training of Chinese word vector.
KEY WORDS：Word2vec; word vector; statistical language model
148