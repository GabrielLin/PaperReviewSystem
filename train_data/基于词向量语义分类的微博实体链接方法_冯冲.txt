 42   6 

    

2016  6 

Vol. 42, No. 6

ACTA AUTOMATICA SINICA

June, 2016


 1

 1

 1

 1

 1, 2

  , 、
(Natural language processing, NLP). , .
, . 
, , 
. , , 
.  NLPCC2014 , 
, .


, , , , 


DOI

, , , , . . , 2016, 42(6): 915−922

10.16383/j.aas.2016.c150715

An Entity Linking Method for Microblog Based on Semantic
Categorization by Word Embeddings
FENG Chong1

SHI Ge1

GUO Yu-Hang1

GONG Jing1

HUANG He-Yan1, 2

Abstract As a widely applied task in natural language processing (NLP), named entity linking (NEL) is to link a given
mention to an unambiguous entity in knowledge base. NEL plays an important role in information extraction and question
answering. Since contents of microblog are short, traditional algorithms for long texts linking do not fit the microblog
linking task well. Precious studies mostly constructed models based on mentions and its context to disambiguate entities,
which are difficult to identify candidates with similar lexical and syntactic features. In this paper, we propose a novel NEL
method based on semantic categorization through abstracting in terms of word embeddings, which can make full use of
semantic involved in mentions and candidates. Initially, we get the word embeddings through neural network and cluster
the entities as features. Then, the candidates are disambiguated through predicting the categories of entities by multiple
classifiers. Lastly, we test the method on dataset of NLPCC2014, and draw the conclusion that the proposed method gets
a better result than the best known work, especially on accurancy.
Key words

Word embedding, entity linking, social media processing, neural network, multiple classifiers

Citation Feng Chong, Shi Ge, Guo Yu-Hang, Gong Jing, Huang He-Yan. An entity linking method for microblog based
on semantic categorization by word embeddings. Acta Automatica Sinica, 2016, 42(6): 915−922


, 
 2015-10-29
 2016-05-03
Manuscript received October 29, 2015; accepted May 3, 2016
 (973 ) (2013CB329303), 
 (863 ) (2015AA015404),  (61
502035),  (20121101120026) 
Supported by National Basic Research Program of China (973
Program) (2013CB329303), National High Technology Research
and Development Program of China (863 Program) (2015AA015
404), National Natural Science Foundation of China (61502035),
and Specialized Research Fund for the Doctoral Program of
Higher Education (20121101120026)
 

Recommended by Associate Editor KE Deng-Feng
1.   100081 2. 
  100081
1. College of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081 2. Beijing Engineering
Research Center of High Volume Language Information Processing and Cloud Computing Applications, Beijing 100081

.  2014  9  30 , 
 1.67 , 
 2 [1] . 、
, 
.


[2−3] . ,  “, 
” , “” ,  6 
. ,  “
”, .
, 
. 
[4] : 1) ,  140
; 2) , 



916





. 
, 
, .
, 
. Jiang [5]
 Twitter 、
. Shen [6] 
 Twitter , 
. Guo [2] 
. Liu
[7]  – 、 –
、 – 
.

, 
 (、) ,
. , 
[8] .

,  “, 
, ”, 
.  NLPCC
2014[9]  “
！、、、、、
、” , “”  “”  “
” . , 
, . 

.  577 ,  1
.
1
Table 1







42 

[10] , , 
, 
, 、
, 
.

, 

, .
, , 
, 
, 
, 
.
:  1 ; 
2 ;  3 ; 
.

1 
1.1 
 M = (m1 , m2 , · · · , mn ) 
,  E = (ee1 , e 2 , · · · , e n ) 
. 
 mi  e i 
 e ij . , 
.

SCW Emi = f (mi , n i )

(1)

, n i  mi .

1.2  (SCWE)

Statistics in training data

1.2.1 



7.91

 7 

34

 6 

81

 5 

207

 4 

416

 3 

502

        ,         
7.91 ,  3 
 87 %,  “
” .
, 
, . 
, 

 1 . 
 CBOW [11] . CBOW 
, 、
. 
、 K  (K 
,  50 ∼ 200),  (
、cosine ) 
. , 
.
 S 
,  V , Tj  j 
, , 


l(θ) = log L(θ) =

6

: 

1
Fig. 1

arg max

1
v

s



Model of semantical categorization by word embeddings

Tj

 · ” , 

log p(wij |contextij )
j=1

917

arg max(cos(mi , C))

ij=1

(2)
, 
 V 
.

(3)

 · ,  · 
 3,  si  · 
 (0, 0, 0, 1, 0, 0, 0, 0, 0, 0).

1.2.2 
,  T = (t1 , t2 , · · · , tn )
, S = (s1 , s2 , · · · , sn )
, 
.  “, 
, ”, 
,  1.2.1 , 
, 
N . ,  N 
k-means[12] ,  k  C = (c1 , c2 ,
· · · , ck )  k  ( k  k-means 
). ,  k 
,  N .

1.2.3 
 1.2.2 , 
 T  ti  k ,  ti 
.
 2 ,  k = 10,  10 .
、、、 3,
 1,  5. 
 (0, 1, 0, 4, 0, 1, 0, 0, 0, 0). 
, 
 si  “ · ”,  “

2
Fig. 2



Example of the training data


 k (k )
.

1.2.4 
[13−15] , , 
 SVM、, 
. , 
.  T (t∗1 ,
t∗2 , · · · , t∗n ) ,  S(s∗1 , s∗2 ,
· · · , s∗n ) , 
. , t∗i  s∗i  k
. , 
. , 

, .


x = t∗i = (x0 , x1 , · · · , xk−1 )
y=

s∗i

= (y0 , y1 , · · · , yk−1 )

(4)
(5)



918





x, y ) .  y = ci (ci 
 (x
1.2.3 ,  [0, 1, 2, · · · , k − 1])
:
egei (x)

P (y = ci |x) =
1+

k−1

e

(6)

gei (x)





42 

,  ()、 (), 
 (、、
). , 
. , 
[17] ( 2) . , Key 
, Value .

j=1

2

[16] :

Table 2

P (y = ci |x)
=
gci (x) = log
1 − P (y = ci |x)
βm0 + βm1 x1 + · · · + βm(k−1) xk−1

(7)



Examples of synonym lexicon





(Key)

(Value)




.  n 
 (Xi , Yji ), i = 1, 2, · · · , n. 
, :
n


 ·  · 

 · 

Michael Jordan
Michael Jeffrey Jordan


(π0 (Xi )y0i π1 (Xi )y1i · · · πk−1 (Xi )yk−1i )

l(β) =
i=1

(8)
, πj (Xi ) = P (y = j|Xi ). 
:
k−1

, 
. 
( 3),  3  (Key) 
 (List).

n

L(β) =

yji log(πj (Xi ))

(9)

3
Table 3



Examples of ambiguity lexicon

j=0 i=1

, 
.





(Key)

(List)
 ()
 ()

2 

 ()



 ()

2.1 

 ()


. 
,  (Semantic
categorization by word embeddings, SCWE) 
 (Entity frequency, EF). 
:

E ∗ = arg max [λ cos(SCW Emi , eij ) +
∀eij e i

(10)

(1 − λ)f (eij )]

 ()

, 
, 
. 
, [4] 
Wikipedia 
 ( 4), 
 ( 5).
4

∗

, E , eij 
mi , f (eij ) 
, λ .

2.2 
 3 . 
: 、
. 、

Table 4



Examples of entity frequency





(Entity)
 ()
 ()
 ()
 ()

(Frequency)
26
39
158
2

6

: 

3
Fig. 3

5
Table 5

919



Process of entity linking



, . 

Weights of entity frequency

 α,  NIL.















1.0

0.8

0.7

0.6

0.5

, 
.
 1. 

. 
. 
 1. ,  M 
, .
 2. E = (ee1 , e 2 , · · · , e n ),  e i 
.  |eei | = 0 , ,
 mi ,  NIL;  |eei | = 1 , 
;  |eei | > 1 , 
 3  4.
 3. 
.
 4.  (10) 

3 
3.1 
、、
 Wikipedia[18] ,
 2015  7  19 .
, 
 6 .
6
Table 6



Scale of experiment data





 Key 

4 293 406

 Value 

1 948 277

 Key 

213 764

 Value 

2 354 687



4 369 348



920





 NLPCC 2014[9] 
 177 
 400  ( 10 000 
) ,  9 600 
 100 , 
 NLPCC 2014 ,  1 152 
.

3.2 
1) . 
.  NLPCC 2014
 (NLPCC)[19] 、
 (EF*)[20] 
 (CMEL)[21] .
NLPCC 
, EF* 
, CMEL 
.
, 
. 
, ,  α
= 1.4, λ = 0.6, . 
 (Precision)、 (Recall)  F1 
. , in-KB 
, NIL 
.  7  8 .
7

in-KB 

Table 7

Results of in-KB







F1 

NLPCC

0.7927

0.8488

0.8198

SCWE + EF

0.8137

0.8593

0.8358

EF∗

0.7641

0.8142

0.7884

CMEL

0.7951

0.8345

0.8143

8
Table 8


NIL 
Results of NIL





F1 

NLPCC

0.9024

0.8653

0.8835

SCWE + EF

0.9144

0.8763

0.8949

EF∗

0.8871

0.8648

0.8758

CMEL

0.8543

0.8694

0.8461

, . 
, 、
, . 





42 

NLPCC、EF*  CMEL 
, 
. , 
 “！、、
、、、、”, NLPCC
 EF∗  “”  “”, 
 “”.
2)  (SCWE)
.  SCWE 
 (EF) ,  λ 
.  9  10 .
9

in-KB 

Table 9

Results of in-KB

λ





F1 

0

0.7532

0.8016

0.7766

0.2

0.7621

0.8158

0.7880

0.4

0.7943

0.8375

0.8153

0.6

0.8137

0.8593

0.8358

0.8

0.8032

0.8432

0.8227

1.0

0.7983

0.8488

0.8228

 10

NIL 

Table 10

Results of NIL

λ





F1 

0

0.8432

0.8532

0.8482

0.2

0.8643

0.8713

0.8678

0.4

0.8917

0.8732

0.8824

0.6

0.9148

0.8762

0.8951

0.8

0.9032

0.8754

0.8891

1.0

0.9013

0.8743

0.8876

,  λ = 0 , 
,  F1 .  λ 
, F1 .  λ = 0.6 , F1 .
λ > 0.6 , F1 . 
.
 F1  λ , 
 4.
3)  k 
.  SCWE , 
k  k .  5 ,  k
= 10 ,  F1 ,  5 ∼ 15 , k
.  k = 20 ,
SCWE  F1 . 
, 

6

: 

 7.91 .  k = 20  F1 
, .

921

ence on Empirical Methods in Natural Language Processing. Seattle, USA: Association for Computational Linguistic, 2013. 863−868
3 Yang Jin-Feng, Yu Qiu-Bin, Guan Yi, Jiang Zhi-Peng. An
overview of research on electronic medical record oriented
named entity recognition and entity relation extraction.
Acta Automatica Sinica, 2014, 40(8): 1537−1562
(, , , . 
. , 2014, 40(8): 1537−1562)
4 Shen W, Wang J Y, Han J W. Entity linking with a knowledge base: issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering, 2015, 27(2):
443−460

 4  λ  F1 
Fig. 4 F1 scores of the combined measure with
the λ parameter

5 Jiang L, Yu M, Zhou M, Liu X H, Zhao T J. Targetdependent twitter sentiment classification. In: Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: 2011. 151−160
6 Shen W, Wang J Y, Luo P, Wang M. Linking named entities
in tweets with knowledge base via user interest modeling.
In: Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. New
York, USA: ACM, 2013. 68−76
7 Liu X H, Li Y T, Wu H C, Zhou M, Wei F R, Lu Y. Entity
linking for tweets. In: Proceedings of the 51st Annual Meeting of the Association of Computational Linguistics. Sofia,
Bulgaria: Association for Computational Linguistics, 2013.
1304−1311

5
Fig. 5

SCWE  k  F1 
F1 scores of SCWE with the k features

4 
,

, , 
NLPCC 2014 . 

,  NLPCC 
, . 
, , 
.

References
1 Chinese Microblog Service. Sina Weibo User Development
Report in 2014 [Online], available: http://www.199it.com/
archives/324955.html. November 24, 2015
(. 2014  [Online], available: http://www.199it.com/archives/324955.html. November 24, 2015)
2 Guo Y H, Qin B, Liu T, Li S. Microblog entity linking by
leveraging extra posts. In: Proceedings of the 2013 Confer-

8 Odbal, Wang Zeng-Fu. Emotion analysis model using compositional semantics. Acta Automatica Sinica, 2015, 41(12):
2125−2137
(, . . 
, 2015, 41(12): 2125−2137)
9 NLPCC [Online], available: http://tcci.ccf.org.cn/conference/2014/pages/page04 sam.html. October 31, 2015
10 Hachey B, Radford W, Nothman J, Honnibal M, Curran J
R. Evaluating entity linking with Wikipedia. Artificial Intelligence, 2013, 194: 130−150
11 Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation
of word representations in vector space. arXiv: 1301.3781,
2013.
12 Hartigan J A, Wong M A. Algorithm AS 136: a k-means
clustering algorithm. Journal of the Royal Statistical Society — Series C (Applied Statistics), 1979, 28(1): 100−108
13 Fern´
andez-Delgado M, Cernadas E, Barro S, Amorim D. Do
we need hundreds of classifiers to solve real world classification problems? Journal of Machine Learning Research,
2014, 15: 3133−3181
14 Mao Yi, Chen Wen-Lin, Guo Bao-Long, Chen Yi-Xin. A
novel logistic regression model based on density estimation.
Acta Automatica Sinica, 2014, 40(1): 62−72
(, , , . . 
, 2014, 40(1): 62−72)

922







15 Zhou Xiao-Jian. Enhancing ε-support vector regression
with gradient information. Acta Automatica Sinica, 2014,
40(12): 2908−2915
(.  ε-. , 2014,
40(12): 2908−2915)
16 King G, Zeng L C. Logistic regression in rare events data.
Political Analysis, 2001, 9(2): 137−163
17 Guo Y H, Qin B, Li Y Q, Liu T, Lin S. Improving candidate generation for entity linking. In: Proceedings of the
18th International Conference on Applications of Natural
Language to Information Systems. Salford, UK: Springer,
2013. 225−236
18 Wikipedia [Online], available: http://download.wikipedia.
comzhwikilate-stzhwiki-latest-pages-articles.xml.bz2. October 31, 2015
19 Zhu Min, Jia Zhen, Zuo Ling, Wu An-Jun, Chen FangZheng, Bai Yu. Research on entity linking of Chinese microblog. Acta Scientiarum Naturalium Universitatis
Pekinensis, 2014, 50(1): 73−78
(, , , , , . .
 (), 2014, 50(1): 73−78)
20 Guo Yu-Hang. Research on Context-based Entity Linking
Technique [Ph. D. dissertation], Harbin Institute of Technology, China, 2014.
(.  [], 
, , 2014.)
21 Meng Z Y, Yu D, Xun E D. Chinese microblog entity linking system combining Wikipedia and search engine retrieval
results. In: Proceedings of the 3rd CCF Conference on Natural Language Processing and Chinese Computing. Berlin
Heidelberg: Springer, 2014. 449−456

  
. 2005 
. 
, , . 
.
E-mail: fengchong@bit.edu.cn
(FENG Chong Associate professor
at the College of Computer Science and
Technology, Beijing Institute of Technology. He received
his Ph. D. degree from the Department of Computer Science, University of Science and Technology of China in
2005. His research interest covers natural language processing, information extraction, and machine translation.
Corresponding author of this paper.)





42 

  
. ,
, .
E-mail: shige713@126.com
(SHI Ge
Ph. D. candidate at the
College of Computer Science and Technology, Beijing Institute of Technology.
His research interest covers natural language processing, entity linking, and question answering
system.)
 .
2014 
. 
, , .
E-mail: guoyuhang@bit.edu.cn
(GUO Yu-Hang Lecturer at the
College of Computer Science and Technology, Beijing Institute of Technology.
He received his Ph. D. degree from Harbin Institute of
Technology in 2014. His research interest covers natural
language processing, information extraction, and machine
translation.)
  
. ,
, .
E-mail: gongjing@bit.edu.cn
(GONG Jing Master student at the
College of Computer Science and Technology, Beijing Institute of Technology.
Her research interest covers natural language processing, machine translation, and question answering system.)
 .
1989 
. 

, .
E-mail: hhy63@bit.edu.cn
(HUANG He-Yan Professor at the
College of Computer Science and Technology, Beijing Institute of Technology. She received her
Ph. D. degree from the Institute of Computing Technology,
Chinese Academy of Sciences. Her research interest covers natural language processing, machine translation, social
network, information retrieval, and intelligent processing
system.)