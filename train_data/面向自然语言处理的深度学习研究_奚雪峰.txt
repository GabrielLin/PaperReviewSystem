 42   10 
2016  10 

    

Vol. 42, No. 10

ACTA AUTOMATICA SINICA

October, 2016


 1, 2, 3

 1

  , , , 
. 、; , 
, ; 
; , .


, , , , 


DOI

, . . , 2016, 42(10): 1445−1465

10.16383/j.aas.2016.c150682

A Survey on Deep Learning for Natural Language Processing
XI Xue-Feng1, 2, 3

ZHOU Guo-Dong1

Abstract Recently, deep learning has made significant development in the fields of image and voice processing. However,
there is no major breakthrough in natural language processing task which belongs to the same category of human cognition.
In this paper, firstly the basic concepts of deep learning are introduced, such as application motivation, primary task
and basic framework. Secondly, in terms of both data representation and learning model, this paper focuses on the
current research progress and application strategies of deep learning for natural language processing, and further describes
the current deep learning platforms and tools. Finally, the future development difficulties and suggestions for possible
extensions are also discussed.
Key words

Natural language processing, deep learning, representation learning, feature learning, neural network

Citation Xi Xue-Feng, Zhou Guo-Dong. A survey on deep learning for natural language processing. Acta Automatica
Sinica, 2016, 42(10): 1445−1465

 (Deep learning) 
, 、
, 
. , 
; 

. 
, 
; , 
、, 
. 
 2015-11-02
 2016-06-12
Manuscript received November 2, 2015; accepted June 12, 2016
 (61331011, 61472264) 
Supported by National Natural Science Foundation of China
(61331011, 61472264)
 

Recommended by Associate Editor KE Deng-Feng
1.   215006 2. 
  215009
3. 
  215009
1. School of Computer Science and Technology, Soochow University, Suzhou 215006 2. School of Electronic and Information Engineering, Suzhou University of Science and Technology,
Suzhou 215009 3. Suzhou Key Laboratory of Mobile Networking and Applied Technologies, Suzhou 215009

, .
, 
. 
, , 
, 
. , ,
, 
. , 
, 
, . 
, . 
, . , 
, 
; , 

, 

, .
, , 
, 
      .

1446







, 
, . 

, 
. 
. 

 BP (Back-propagation) .


, 
, 
. [1−2] , 

,  BP 
, 
.

,
.
 (Conditional random field, CRF)、 (Hidden Markov
model, HMM)、 (Support vector machine, SVM)、 (Multilayer perceptron,
MLP)  (Maximum entropy, ME) .

, 、
[3] .
 2006 . Hinton 
 (Deep belief network, DBN) 
[4] ,  DBN  (Restricted Boltzmann machine, RBM) 
,  MNIST1 
,  1.2 % [5] .
, Bengio 
 (Auto-encoders) , 
[6−7] . , 
: 
, . 
,  RBM  Auto-encoders 
[8−9] .  2006 ,

[6,10−15] , [16−17] 、[18−19] 、
[20−21] 、[22−23] 、[24−25] 、
   [26−27]         [28−30]  
. , , 
Auto-encoders、RBM  DBN , 
1





42 

, 
, 
.
,

. 2012 , “
(Deep neural network, DNN)”
 ImageNet , 
 26 %  15 %, 
.  2011 ,  DNN
, 
 20 % ∼ 30 %, 
.  DNN 
,  2012  11 
、
, 
.

, 
, . 
, 

, .  1 
;  2 , 

、;  3 
, .

1 
1.1 
, , 
 (Depth), 
 3 , 
 10 . , 
. , 
. , 
, , 
, 
. , 
 (Overfitting) 
.
, 
(Flow graph) .  1 (a) , 
. 
. 
, , 

MNIST  http://yann.lecun.com/exdb/mnist/

10 

: 

.
 1 (). 
, .
 1 (a)          : f (x) =
x × sin(x × a + a/b),  4.  1 (b)
 (Artificial neural network,
x) = tanh(b + w x ), 
ANN)  f (x
 3. ,  BP 
 1,  1 (c) 
 2.  (
 3) .

(a)  4 

(b)  3 

(a) A flow graph with

(b) A multilayer neural network

depth of four

with a depth of three

(c)  2  BP 
(c) A BP neural network with a depth of two

1
Fig. 1



Concept example of depth

1
Table 1

1447

. 
, 
; , 
. , 
, 
.

1.2 
, 
. , 
, 
. , , 
. 
. , 
, .
, 
. 、、, 
 1 .
. 
, , 
, ; 
. , 
. , , ,
？Olshausen
, 
, [31] . 
.

, . 

, , 
, 
. 
. , 
, 
, 
, . 
, , 
.

? Hinton [3−4] .

、、[32]
Feature hierarchy of speech, image and text[32]
















































–
–













1448





, , 
. , 
 2 , .
. Hastad 
[33] , 
 d  O(n) ,
 d − 1 ,  O(2n ) 
, 
.
, 
. , 
, . 
, 
, , 
.
, , 
, 
.  Serre 
[34] : 
; 
.

1.3 

. 
, 
, 
. , 
 (Feature learning)  (Representation
learning) , 
, , ,
.





42 



1.4 
, 
.  2 , 
 ()
, , 
. , 

, .
,  3 [35] ,
.
 1. ; 
 n;
 2. 
;  i = 1;
 3. , 
;
 4. 
, ;
 5.  i  n, 
i = i + 1,  3; ,  6;
 6. 
, ;
 7.  () ;
 () .

3
2
Fig. 2



Basic model of deep learning

, 
, . 
, 
, .
, 
. , 
? 
, ? · · · 
, .

Fig. 3



Basic framework of deep learning

 2∼4 
  ,    “     (Layer-wise pretraining)”[5] .  4 .

4
Fig. 4



Layer-wise pre-training model

10 

: 

 (Autoencoder) . , 
.

1.4.1 
, , 
 4  Encoder A, ,
“”, 
. 
, ,
,  4 
 Decoder A, “”.
“”“”, 
, 
. 
, . 
, 
 (
), .
, 
, , 
、; , 
.

1.4.2 
, 
, 
. , 
. , 
 (Classifier),
 (Label) , 
.
: 
; , 
, 
2
Table 2

1449

.

. 
, 
.  2 
. 
, 
. 
, , 
. 
, 
, 
. , 
, 
, , 
, . 
, .

2 
,
. 
, 
, 
, 
. , , 
, 

. ,  2006  Hinton
[5] , , 

, .

. 2003 , Bengio 
(Word embedding  Word representation)  ,

[32]

Comparison and analysis of shallow model and deep model[32]















1∼2 

5 ∼ 10 





, 

, 、

, 



、














; ;

; ;












1450







; 
 n-gram [36] ; 
, Collobert 
 (Convolutional neural network, CNN),
、、
、
 SENNA (Semantic/syntactic extraction using
a neural network architecture) [28] , 
. 
, ,  3 500  C 
 SENNA , , 
.
 Bengio 
, Mikolov , 
, [37] ; 
, 

.  Schwenk 
[38] ,  BLEU
(Bilingual evaluation understudy) ,
 2 .  (Recursive auto-encoders) [39]  (Sentence
paraphrase detection)  F 1 .
, [40] 、
[41−42] 
, .

2.1 

, 
, 
. , 
, 
.

2.1.1 

. , 
, , 
. , ; 
, ; , 
, , 
. , 
？
. , 
.
2.1.2 

. , 





42 

. 
, , 
. , 
 () . 
, .

2.1.3 
, 
 () ; 
: , 
. , 
. , ,
 (Intermediate
representations). 
.
,  (Recursion).
, , 、
. ,
, 
 (Recursive neural network, RNN).
2.1.4 
,
. 
, 
,  (Multi-core computing)、
 (Graphics processing unit, GPU) .
, 
,  RBM、Auto-encoders
;                 /
[28, 37, 43−44] , .
, 
.
2.2 
, 
: 1) ; 2)
. 
, , 
. , 
[4, 6−7] ; , 
[43] ,  (Phonemes).
, 
.  1), 
[30, 45−46] 、 (Bag-of-words,
BoW)、 (Vector space model, VSM)
;  2), , 
, 
. . , 
, 、. ,

10 

: 

. 
 (Recurrent
neural network, RNN)、、
[37, 47−50] .
, 
: 1) 
, ,  (End-toend) , ; 2) , 
.  1) 
 SENNA [30] , 
、
、; 
Socher 、
[51] .  2)  Turian

[52] , 
.

2.2.1 
2.2.1.1

One-hot representation

            ,     
. 
,  One-hot representation    :          
  ;            1,    
  ;           0;     
     .  “ ”      
[0001000000000000 · · · ], “”
 [0000000010000000 · · · ].
One-hot representation 
, . , 
、、, 
; 
, , 
. “”“”
, , 
, 
. Firth 
[53] : , 
. 

. , 
Distributional similarity. 
.

2.2.1.2




 .           ,    

1451

   (             
   ), Hinton       Distributed
representation      [54] ,     
  .
           , 
[0.792, −0.177, −0.107, 0.109, −0.542, · · · ]. 
,“”“”
“”“”. 
. 
: ,  One-hot
representation , 
, , [36] , 
; , 
, , 
, ; , ,

.
,
“”; 
, , 、
[46] , 、、
. , 
.

2.2.1.3




. 
, ,
, 
. 
.
 1 a). Tom likes to play basketball. Mike
likes too.
 1 b). Mike also likes to play tennis.
, 
 (“Tom”: 1,“likes”: 2,“to”: 3,“play”:
4,“basketball”: 5,“Mike”: 6,“too”: 7,“also”:
8, “tennis”: 9).
 9 , ,
.
, 
:
[1, 2, 1, 1, 1, 1, 1, 0, 0]
[0, 1, 1, 1, 0, 1, 0, 1, 1]
 9 ,  i 
 i . 
. 
, . 
.  D,

1452





 M ; 
,  N . ,
 N , 
.

2.2.1.4



 (Vector space model, VSM) 
Salton[55]  20  70 , 
 SMART (System for the mechanical analysis and retrieval of text) . 
, 
, 
, . 
, 
.
, 

.  Rapp 
 TOEFL 
[56] ,  92.5 % , , 
 64.5 %. ,
Turney [57] ,  SAT
,  56 % 
,  57 % . 
, 、、
, , 
: ; 
、.
、
,  Manning 
[58] , 
.

2.2.2 
.
, 
.
 2.  E = { , ,
 },  W ordU nit = {  |
 }. , , 
[26, 59] .
 3. : 
 S = {w1 w2 · · · wt }, 
 P (S). , S ∈ E, wi ∈ W ordU nit,
(i = 1, 2, · · · , t). :
 1. P (w1 , w2 , · · · , wt ) = P (w1 ) × P (w2 |
w1 ) ×P (w3 |w1 , w2 ) × · · · × P (wt |w1 , w2 , · · · , wt−1 ).
, ,  n 





42 



 (n-gram) .

2.2.2.1

 n 

,  Xu
[60] 
;  Bengio [36]  n
, 
.
 5  wt−n+1 , · · · , wt−2 , wt−1
 n − 1 .  n − 1 
 wt . , 
,  4 .

5
Fig. 5

 n-gram [36]

n-gram model constructed by three layer of neural
networks[36]

 4.  C = |V | × m, 
 |V | ; m ;
c (w)  C , 
w .
 Cwt−n+1 , · · · , Cwt−2 , Cwt−1 
,  m(n − 1) , 
x 
x ;  ()  d + Hx
 ( H , d 
),  tanh 
;  ()  |V |
,  sof tmax  y 
,  (1) .

Pˆ (wt = i|w1t−1 ) =

e yi
e yj

(1)

j

 yi  i .  y
 (2):

x + U tanh(d + Hx
x)
y = b + Wx

(2)

, b ; 
 W = |V | × (n − 1)m; 

10 

: 

 U = |V | × h,  h 
;  H = h × (n − 1)m;
 U 
. , 
[29−30, 47] , .  (2) 
 W . 
,  W  0. 
, 
[36] .
, ,
, . 
, 
, , 
. 
 n , 
 10 % ∼ 20 %[36] .
  [36]      ,      
         : Log-bilinear   
、 Hierarchical log-bilinear    、  
.

2.2.2.2

Log-bilinear 

 [36] , Mnih  Logbilinear [61] , 
. 
. 
 (Restricted Boltzmann machines, RBM) ,
,  Logbilinear . :
t−1

h=

Hic (w)

(3)

i=1

yj = c (wj )Th

1453

 n , 
 3  5 .

2.2.2.3

Hierarchical log-bilinear 

 Log-bilinear , Mnih 
 HLB (Hierarchical logbilinear)  [36] 
, 
, [29] .
 Morin [62] , 
 WordNet  IS-A , 
. 
, , . Mnih 
, 
(Bootstrapping) , 
[29] . ,
; 
. 
 O(|V |)  O(log2 (|V |)).

2.2.2.4 
 [36] , .
Mikolov  (Recurrent neural network language model, RNNLM)
[47] ;  BPTT (Backpropagation through time) ,  n
; 
, Mikolov  RNNLM , 
[37, 48−50] .

, . 
 6 .

(4)

 (3)  (4) :
t−1

c (wj )T Hic (w)

yj =

(5)

i=1

, c (w)  w , Bilinear 
 xT Myy .  (3) , h 
,  m, 
.  Hi = m × m  i  Hi
,  t .  (4) , yj 
 c (wj )  h , 
,  wj  log .
Log-bilinear , 
; .

(a) 

(b) 

(a) Abstract representation of

(b) Transfer process of neural

neural network architecture

network

6
Fig. 6



Structure diagram of recurrent neural network

 6 (a) , 

1454







, 、
 t. :

s(t) = sigmoid(U w (t) + W s(t − 1))

(6)

, w (t)  One-hot representation 
 t ; s (t) 
; s (t − 1) . 
s (0)  ( 0.1) ，
s (1) = s (0).
 6 (b) . 
, 
, ; 
; 
.
 n 
, 
. , 
, 
,  n 
. 
, Mikolov [47] :
,  |V |  |V | , 
|V | , ; 
, ; 
 O( |V |),  Mnih  Hinton [29]
 O(log(|V |)). 
, .

2.2.2.5 
Collobert  Weston  2008 
[30] , 
, 
、、、
.  n , 
.  5.
 5.  f (wt−n+1 , · · · , wt−1 , wt ) 
 n . f , 
. f , ;
f , . , 
, f .
, Collobert  Weston  Pair-wise
[30] . , 
.
max{0, 1 − f (x) + f (x(w))}

(7)

x∈X w∈D

, X  n , D 
, x , x(w) , 





42 

f (x) , f (x(w)) 
.  (7) 
 n ; 
. x(w) 
 w  x , , 
, 
.  (7) , 
 1 .
f  [36] 
. : 1)  n 
; 2) 
. : Collobert
 Weston [30] 
,  [36]  |V | ; , 
HardT anh  tanh .
Collobert  Weston  n 
11,  |V |  130 000, 
 7  C & W
. , C & W 
:
1) C & W . ,
, 
, .
2) C & W . 
 C & W 、
, 
.
Turian  Collobert  Weston 
C & W  Mnih  Hinton [29] 
[52] ,  HLB
(Hierarchical log-bilinear) , 
. Mikolov [45−46] 
 word2vec (
2.5.2 ),  CBOW 
(Continuous bag-of-words model)  Skip-gram 
 (Skip-gram model),  Hierarchical softmax  Negative sampling . CBOW
 Skip-gram , 、
, , 
wt  wt−2 , wt−1 , wt+1 , wt+2 
 wt ,  7 (a) ; 
wt ,  wt−2 , wt−1 , wt+1 , wt+2 ,
 7 (b) .
 word2vec 
 (Word analogy) , 
. 
 TransE[63] 
. 、

10 

: 

,  (、、) 
. TransE 
, . 
, 
.

(a) CBOW 

(b) Skip-gram 

(a) CBOW model

(b) Skip-gram model

7
Fig. 7

1455

. Mikolov 
, , , 
Collobert  Weston[30] .

2.3 
Bengio           (Stochastic
gradient descent, SGD)          
[64] , : 、
、. 
, 
,  8 .

 word2vec 

Model structure diagram of word2vec

8

2.2.3 
, 
,  () 
.  HLB , 
 H  Hi , 
:
[H1 H2 · · · Ht ]cc(w1 )cc(w2 ) · · · c (wt ) =
H1c (w1 ) + H2c (w2 ) + · · · + Htc (wt )

(8)

, , 
. Bengio 
[36] , ;
Collobert  Weston [30] , 
 f ; Mnih  Hinton
[29] , , 
; Mikolov [47] .
, Collobert  Weston [30] :
, , 
; ,
. , Turian
[52]  C & W 
 Mnih  Hinton[29] 
; , .
 Mikolov 
[45] : , 
     .       a  b   ,
     c  d   ,     a、 b、 c,
  c (d)         c (a) − c (b) + c (c).
           c (king) − c (queen) ≈
c (man) − c (woman), , c (queen) 
 c (king) − c (man) + c (woman) .
, 

Fig. 8



Deep learning application architecture for NLP

 1. . , 
, 
.
 2. . 
. .
 3. . 
 b  W .
 4. . 
.
 5. . 
, , 
; , 
(Regularization) .

2.3.1 
, 
,  Single words、Fixed windows、Recursive sentence 
Bag of words; , 
 logistic (“sigmoid”)、tanh、hard
tanh、soft sign、rectifier ,  9 . sigmoid
, 
 [0, 1] , 
; , sigmoid , 
, 
, . 
, rectifier , 
. , , 
, tanh , 
; hard tanh , . 
 9 ，:

1456



(a) logistic 

(b) tanh 

(a) Graphic of logistic

(b) Graphic of tanh

function

function

9
Fig. 9





(c) hard tanh 
function

(e) rectifier 
(e) Graphic of rectifier

function

function

Visual representation of several commonly used nonlinear functions

1
1 + e−z

(9)

ez − e−z
ez + e−z

(10)

3) hard tanh :


 −1, z < −1
f (z) =
z,
−1 ≤ z ≤ 1


1,
z>1

(11)

4) soft sign :
f (z) =

(d) soft sign 



2) tanh :
f (z) =

42 



(c) Graphic of hard tanh (d) Graphic of soft sign

1) logistic (“sigmoid”) :
f (z) =



z
1 + |z|

(12)

5) rectifier :
f (z) = max(z, 0)

(13)

2.3.2 
. 
, . 
, , ; 
, ; , 
, .
2.3.3 
,  0,
 w  0 
; ,  w ∈ (−r, r), r =
6/(f anin + f anout ),  f anin 
, f anout ; , 
.
2.3.4 
            .  θ 
  {W, b}, W       , b    

    (Bias).            
 (SGD)、 LBFGS (Limited-memory BroydenFletcher-Goldfarb-Shanno)、       CG
(Conjugate gradients).
SGD :

θ(t) ← θ(t−1) − εt

∂L(Zt , θ)
∂θ

(14)

, L , Zt , θ ,
εt . SGD ,
, ;
, 
.  O(1/t), 
:
ε0 τ
(15)
εt =
max(t, τ )
, 
, , . 
 ( 1 ) , LBFGS
; , CG 
. , , 
LBFGS  CG ; ,
SGD [65] . 
, 
, 
. 
,  mini-batch. , 
 mini-batch  SGD .
, 
, . 
, 
.

2.3.5 
, , 
. 
: . 
, 、
. ,  L1 

10 

: 

L2  Regularity , 
, 
.
2.4 
, 
, 
, 、、
、, .

2.4.1 
, 
.  (Part-of-speech
tagging, POS) , 
、、, .
, 
 Collobert 
[28] , 
, 、、
、
 SENNA , 
.
, Zheng 
[59] , 
、
. , 
, 
 (Task-specific feature engineering); , 
 (Unlabeled data)  (Internal representation), 
; 
,  Perceptron-style 
Maximum-likelihood , 
, . , 
[28, 36] , 
(Character) , 

.
2.4.2 
 (Syntactic analysis) 

, . : 
, 
.
Henderson     Left-corner    
[66]
 , 
; , Henderson 
[67] ; Titov  SVM 

1457

[68] ; 
[69] .
Collobert 
[70] . 
, 
, 
.
, Costa 
[71] , 
. 
, 
;  2 000 
, .
Menchetti [72]  Collins [73] 
, 
. , Socher  CVG
(Compositional vector grammar) 
[74] ,  PCFG (Probabilistic context
free grammars) , 
. 
,  3.8 % (
90.4 %  F 1 ),  20 %.
Legrand , 
[75] . 
, , , 
.

2.4.3 

, 
. , 
, 
. , 
; , 
. Huang [76] 
Collobert  Weston[30] , 
. 
, 
; , 
; , 
, , 
. , ,
Huang .
Socher [40] . 
, 
, 
, 
. 

1458







, 
. . 
;
; 
. 
,  – 
、、, 
.

2.4.4 
 (Sentiment analysis) 
、 (Opinion extraction)、
(Opinion mining)、 (Sentiment mining)、
 (Subjectivity analysis) , 
、、
, “”、
、、.
Zhou  (Active
deep network, ADN) 
[77] . , 
,  RBM, 
 ADN, 
; ,  (Active
learning) , 
,  ADN , 
. , 
5 . ADN
 RBM , 
, 
.
Glorot 

[78] , , 
Amazon  4 
. Socher  RAE (Recursive auto-encoders)
[79] , 
. ,
 RAE . , 
. 
, Socher 
 (Sentiment treebank), 
[51] ; ,  RNTN (Recursive
neural tensor network) , : 
 80 %  85.4 %;
 71 %  80.7 %. 
, Le 
(Paragraph vector)[41] , 、






42 

, ,
 RNTN
[51]  2.4 %. Kim  Collobert 
CNN [28] ,  Google 
 word2vec  1 000 
, 
,  88.1 % 
[42] .  BigData : 
, 
.

2.4.5 
 (Machine translation) 

, . , 
: 
、
 (Neural machine translation, NMT) 
.

 (Log-linear framework), 
, : 1) 
; 2) 
. , Liu 
 (Additive neural network)[80] , ; ,
, 
, 
.  (Word alignment)
. Yang 
 (DNN) [81] .
, 
;
. 
, 
,  IBM Model 4.

, 
[82−85] : 
, 
; , 
, 
. 
, 、
. , 
 X , 
 Y ,  arg maxY p(Y |X).
, 

10 

: 

 (Encoder-decoders) 
[83−84] , : 

, . 
, , 
. 
, , 
 c . :

ht = f (xt , ht−1 )

(16)

c = q ({h1 , · · · , hTx })

(17)

, ht ∈ Rn  t , c 
, f  q 
.  Sutskever  LSTM (Long shortterm memory)  f [83] . 
c  {y1 , · · · , yt −1 } 
, 
 yt . :
T

p(yt |{y1 , · · · , yt−1 }, c )

p(y) =

(18)

t=1

, y = {y1 , · · · , yTy }, , 
:

p(yt |{y1 , · · · , yt−1 }, c ) = g(yt−1 , st , c )

(19)

, g , 
, st . 

[82] .


, 
, 
. Cho , 
[85] . 
, Bahdanau 
[86] . , 
, , 

. , 

, ; 
, 
, 
. 

. Dong , 

1459


, 
[87] .

2.5 

  ,         ,      
Python、C++、C  Java 
; 
,  RBM/DBN (Deep belief network) 、 (CNN)、
、; 
, 
/、
. 
.

2.5.1 /
, 
 Montreal  LISA (Laboratoire d Informatique des Syst`emes Adaptatifs)
   Bergstra     Theano,     
Python , 
,  RBM/DBN , 
、.
Theano : 1)  NumPy.
NumPy  Python , 
 Scipy . Theano 
 numpy.ndarray ,  Numpy
. 2)  GPU . 
,  32 
CPU ,  100 . 3) 
. Theano  1 
. 4) . 
x ,  log(1 + x) .
5)  C . 6) 
. .
 Theano , 
,  Pylearn2、Blocks、Keras .
 Python  Keras 、
, 
,
. Keras 
, 、
, 
 CPU  GPU . Keras 
 Theano , 
 Keras  TensorFlow.
TensorFlow ,  Google

1460







 (Google
Brain Team) , 
. 
, 
, . 
, , , , 
、, 
CPU/GPU .
 TensorFlow 
 MShadow, 、
,  C++/CUDA ,
 CPU/GPU/ GPU . 
 CXXNet  MxNet 
, .

2.5.2 

        SENNA,  Collobert[28] 
  ,      、     (   
        )、       , 
POS Tagging、Chunking、Named entity recognition、Semantic role labeling        
.
SENNA  3 500  C  (ANSI
C) ,  150 MB 
. 
SENNA V 3.0,  2011  8 . SENNA 
 Wikipedia  2 
, , 
.
, Google  2013 
 word2vec 
. word2vec  2.2.1  Distributed
representation , 
,  K 
; , ,
 (、cosine 
) . word2vec ,
 Huffman 
, , 
, . , 
 word2vec, 
. ,  word2vec 
.

2.5.3 
, 
2
3





42 

. , 
, 
; , 、
、
.

2.5.4 
 Ruslan Salakhutdinov  Matlab 
 (Matrbm、Estimating partition functions of RBM s、Learning deep Boltzmann machines)[35] ,           
,  RBM, . 
Deeplearning4j , 
,  GPU,  Hadoop
, 
. Deeplearning4j  Java/Scala 
RBM、 (DBN)、LSTM、
 (Recursive autoencoder) 
, 、
.
2.5.5 

.  SENNA 
. , 
 Cudaconvnet、ConvNet  2.5.1  Keras .
Cuda-convnet2  Cuda-convnet ,
 C++/CUDA ,  BP ;
ConvNet  Matlab 
.
2.5.6 


, 
,  Python 、 Theano
 Keras,  Java 
 Deeplearning4j . 
 Tomas Mikolov 
2 ( UTF-8 
)[47] 、Richard Socher 
3[39] , 
.

3 
3.1 
“”“

Mikolov  http://www.fit.vutbr.cz/ imikolov/rnnlm/
Socher  http://www.socher.org

10 

: 

”, 、
“”, 
, , 
, . 
. , 
, 、
, 
？, 
, 
, , 、
.
, 
 Word embedding , 
,  Word ,
, 
, ,  Word 
.

, 
. , ,
, 
, 
[49] .
, 
, 
[39−40, 51, 88] . , 

.

3.2 
, 
, 
、
, . 

.
, 
. 
、. 
: ; 
, .
, 
. , 
. , , 
. , 
[89] . , 
. , 
. , 

1461

？

3.3 
3.3.1 
, 

. 
, , 
 (
), , . 
, 
, . , 
. , , ,
;
, , 
, , 
.
, , 
. 
. 
, . 
, 
. , 
, 
. ,  Google 
AlphaGo ,
[90] .
3.3.2 
, 
, 

. 
, 
.
,
. , 
、、、, 
、
, 、, 
.

4 
, 
. 
,  Word embedding . 
, 、、、
、, , 
, 

1462





、, 
. , 
, 
. , 
？
？, 
. ,
, 
, , 
, 
,

.


、
.

References
1 Erhan D, Bengio Y, Couville A, Manzagol P A, Vincent
P, Samy B. Why does unsupervised pre-training help deep
learning? Journal of Machine Learning Research, 2010, 11:
625−660
2 Sun Zhi-Jun, Xue Lei, Xu Yang-Ming, Wang Zheng.
Overview of deep learning. Application Research of Computers, 2012, 29(8): 2806−2810
(, , , . . ,
2012, 29(8): 2806−2810)
3 Bengio Y. Learning deep architectures for AI. Foundations
and Trends in Machine Learning, 2009, 2(1): 1−127
4 Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets. Neural Computation, 2006, 18(7):
1527−1554
5 Hinton G E, Salakhutdinov R R. Reducing the dimensionality of data with neural networks. Science, 2006, 313(5786):
504−507
6 Bengio Y, Lamblin P, Popovici D, Larochelle H. Greedy
layer-wise training of deep networks. In: Proceedings of the
2007 Advances in Neural Information Processing Systems 19
(NIPS 06). Vancouver, Canada: MIT Press, 2007. 153−160
7 Ranzato M A, Poultney C, Chopra S, LeCun Y. Efficient
learning of sparse representations with an energy-based
model. In: Proceedings of the 2007 Advances in Neural
Information Processing Systems 19 (NIPS 06). Vancouver,
Canada: MIT Press, 2007. 1137−1144
8 Weston J, Ratle F, Collobert R. Deep learning via semisupervised embedding. In: Proceedings of the 25th International Conference on Machine Learning (ICML 08). New
York, USA: ACM Press, 2008. 1168−1175
9 Srivastava N, Mansimov E, Salakhutdinov R. Unsupervised
learning of video representations using LSTMs. In: Proceedings of the 32nd International Conference on Machine Learning (ICML 15). Lille, France: Omni Press, 2015. 843−852







42 

10 Jia K, Sun L, Gao S H, Song Z, Shi B E. Laplacian autoencoders: an explicit learning of nonlinear data manifold.
Neurocomputing, 2015, 160: 250−260
11 Chan T H, Jia K, Gao S H, Lu J W, Zeng Z N, Ma Y.
PCANet: a simple deep learning baseline for image classification? IEEE Transactions on Image Processing, 2015,
24(12): 5017−5032
12 Alain G, Bengio Y. What regularized auto-encoders learn
from the data-generating distribution? The Journal of Machine Learning Research, 2014, 15(1): 3563−3593
13 Srivastava N, Hinton G, Krizhevsky A, Sutskever I,
Salakhutdinov R. Dropout: a simple way to prevent neural
networks from overfitting. The Journal of Machine Learning
Research, 2014, 15(1): 1929−1958
14 Dosovitskiy A, Springenberg J T, Riedmiller M, Brox T.
Discriminative unsupervised feature learning with convolutional neural networks. In: Proceedings of the 2014
Advances in Neural Information Processing Systems 27
(NIPS 14). Montr´
eal, Quebec, Canada: MIT Press, 2014.
766−774
15 Sun Y, Wang X G, Tang X O. Deep learning face representation from predicting 10 000 classes. In: Proceedings
of the 2014 IEEE Conference on Computer Vision and
Pattern Recognition. Columbus, Ohio, USA: IEEE, 2014.
1891−1898
16 Qiao Jun-Fei, Pan Guang-Yuan, Han Hong-Gui. Design and
application of continuous deep belief network. Acta Automatica Sinica, 2015, 41(12): 2138−2146
(, , . . 
, 2015, 41(12): 2138−2146)
17 L¨
angkvist M, Karlsson L, Loutfi A. A review of unsupervised
feature learning and deep learning for time-series modeling.
Pattern Recognition Letters, 2014, 42: 11−24
18 Han X F, Leung T, Jia Y Q, Sukthankar R, Berg A
C. MatchNet: unifying feature and metric learning for
patch-based matching. In: Proceedings of the 2015 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR 15). Boston, Massachusetts, USA: IEEE Press,
2015. 3279−3286
19 Szegedy C, Liu W, Jia Y Q, Sermanet P, Reed S, Anguelov
D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with
convolutions. In: Proceedings of the 2015 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR 15).
Boston, Massachusetts, USA: IEEE, 2015. 1−9
20 Denton E L, Chintala S, Szlam A, Fergus R. Deep generative image models using a Laplacian pyramid of adversarial
networks. In: Proceedings of the 2015 Advances in Neural Information Processing Systems 28 (NIPS 15). Montreal,
Canada: MIT Press, 2015. 1486−1494
21 Dong C, Loy C C, He K M, Tang X O. Learning a deep
convolutional network for image super-resolution. In: Proceedings of the 13th European Conference on Computer
Vision (ECCV 14). Zurich, Switzerland: Springer International Publishing, 2014. 184−199

10 

: 

1463

22 Nie S Q, Wang Z H, Ji Q. A generative restricted Boltzmann machine based method for high-dimensional motion
data modeling. Computer Vision and Image Understanding,
2015, 136: 14−22

35 Salakhutdinov R R, Hinton G. Deep Boltzmann machines.
In: Proceedings of the 12th International Conference on
Artificial Intelligence and Statistics (AISTATS 09). Florida,
USA: Omni Press, 2009. 448−455

23 Jain A, Tompson J, LeCun Y, Bregler C. Modeep: a deep
learning framework using motion features for human pose
estimation. In: Proceedings of the 12th Asian Conference
on Computer Vision (ACCV 2014). Singapore: Springer International Publishing, 2015. 302−315

36 Bengio Y, Ducharme R, Vincent P, Jauvin C. A neural probabilistic language model. The Journal of Machine Learning
Research, 2003, 3: 1137−1155

24 Geng Jie, Fan Jian-Chao, Chu Jia-Lan, Wang Hong-Yu. Research on marine floating raft aquaculture SAR image target
recognition based on deep collaborative sparse coding network. Acta Automatica Sinica, 2016, 42(4): 593−604
(, , , . 
 SAR . , 2016, 42(4): 593−604)
25 Erhan D, Szegedy C, Toshev A, Anguelov D. Scalable object detection using deep neural networks. In: Proceedings
of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 14). Columbus, Ohio, USA: IEEE,
2014. 2155−2162
26 Qi Y J, Das S G, Collobert R, Weston J. Deep learning
for character-based information extraction. In: Proceedings
of the 36th European Conference on IR Research on Advances in Information Retrieval. Amsterdam, The Netherland: Springer International Publishing, 2014. 668−674
27 Nie L Q, Wang M, Zhang L M, Yan S C, Zhang B, Chua T
S. Disease inference from health-related questions via sparse
deep learning. IEEE Transactions on Knowledge and Data
Engineering, 2015, 27(8): 2107−2119
28 Collobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu
K, Kuksa P. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research, 2011,
12: 2493−2537
29 Mnih A, Hinton G E. A scalable hierarchical distributed
language model. In: Proceedings of the 2009 Advances in
Neural Information Processing Systems 21 (NIPS 08). Vancouver, Canada: MIT Press, 2009. 1081−1088
30 Collobert R, Weston J. A unified architecture for natural
language processing: deep neural networks with multitask
learning. In: Proceedings of the 25th International Conference on Machine Learning (ICML 08). Helsinki, Finland:
ACM Press, 2008. 160−167
31 Olshausen B A, Field D J. Emergence of simple-cell receptive field properties by learning a sparse code for natural
images. Nature, 1996, 381(6583): 607−609
32 Overview of deep learning and parallel implementation [Online], available: http://djt.qq.com/article/view/1245, June
20, 2016
33 Hastad J. Computational Limitations for Small Depth Circuits. Cambridge, MA, USA: Massachusetts Institute of
Technology, 1987
34 Serre C, Mellot-Draznieks C, Surbl´
e S, Audebrand N, Filinchuk Y, F´
erey G. Role of solvent-host interactions that
lead to very large swelling of hybrid frameworks. Science,
2007, 315(5820): 1828−1831

ˇ
37 Mikolov T, Deoras A, Kombrink S, Burget L, Cernock´
y J H.
Empirical evaluation and combination of advanced language
modeling techniques. In: Proceedings of the 2011 Conference of the International Speech Communication Association (INTERSPEECH 2011). Florence, Italy: ISCA Press,
2011. 605−608
38 Schwenk H, Rousseau A, Attik M. Large, pruned or continuous space language models on a GPU for statistical machine translation. In: Proceedings of the NAACL-HLT 2012
Workshop: Will We ever Really Replace the N -gram Model?
on the Future of Language Modeling for HLT. Montr´
eal,
Canada: ACL Press, 2012. 11−19
39 Socher R, Huang E H, Pennington J, Ng A Y, Manning C D.
Dynamic pooling and unfolding recursive autoencoders for
paraphrase detection. In: Proceedings of the 2011 Advances
in Neural Information Processing Systems 24 (NIPS 11).
Granada, Spain: MIT Press, 2011. 801−809
40 Socher R, Huval B, Manning C D, Ng A Y. Semantic compositionality through recursive matrix-vector spaces. In: Proceedings of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational Natural
Language Learning. Jeju Island, Korea: ACL Press, 2012.
1201−1211
41 Le Q, Mikolov T. Distributed representations of sentences
and documents. In: Proceedings of the 31st International Conference on Machine Learning (ICML 14). Beijing,
China: ACM Press, 2014. 1188−1196
42 Kim Y. Convolutional neural networks for sentence classification. In: Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP 2014).
Doha, Qatar: ACL Press, 2014. 1746−1751
43 Dahl G E, Yu D, Deng L, Acero A. Context-dependent pretrained deep neural networks for large vocabulary speech
recognition. IEEE Transactions on Audio, Speech, and Language Processing, 2012, 20(1): 30−42
44 Mohamed A R, Dahl G E, Hinton G. Acoustic modeling
using deep belief networks. IEEE Transactions on Audio,
Speech, and Language Processing, 2012, 20(1): 14−22
45 Mikolov T, Yih W T, Zweig G. Linguistic regularities in continuous space word representations. In: Proceedings of the
2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2013).
Atlanta, Georgia: ACL Press, 2013. 746−751
46 Mikolov T, Sutskever I, Chen K, Corrado G S, Dean J. Distributed representations of words and phrases and their compositionality. In: Proceedings of the 2013 Advances in Neural Information Processing Systems 26 (NIPS 13). Nevada,
USA: MIT Press, 2013. 3111−3119

1464











42 

ˇ
47 Mikolov T, Karafi´
at M, Burget L, Cernock´
y, Khudanpur S.
Recurrent neural network based language model. In: Proceedings of the 2010 International Conference on Spoken
Language Processing (ICSLP 2010). Chiba, Japan: Speech
Communication Press, 2010. 1045−1048

59 Zheng X Q, Chen H Y, Xu T Y. Deep learning for Chinese word segmentation and POS tagging. In: Proceedings
of the 2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2013). Seattle, Washington,
USA: ACL Press, 2013. 647−657

ˇ
48 Mikolov T, Kombrink S, Burget L, Cernock´
y J H, Khudanpur S. Extensions of recurrent neural network language model. In: Proceedings of the 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Prague, Czech Republic: IEEE, 2011.
5528−5531

60 Xu W, Rudnicky A I. Can artificial neural networks
learn language models?
In: Proceedings of 2000 International Conference on Spoken Language Processing
(ICSLP 2000). Beijing, China: Speech Communication
Press, 2000. 202−205

ˇ
49 Mikolov T, Deoras A, Povey D, Burget L, Cernock´
y J H.
Strategies for training large scale neural network language
models. In: Proceedings of the 2011 IEEE Workshop on
Automatic Speech Recognition and Understanding (ASRU).
Waikoloa, Hawaii, USA: IEEE Press, 2011. 196−201
50 Mikolov T, Zweig G. Context dependent recurrent neural
network language model. In: Proceedings of the 2012 IEEE
Conference on Spoken Language Technology (SLT). Miami,
Florida, USA: IEEE, 2012. 234−239
51 Socher R, Perelygin A, Wu J Y, Chuang J, Manning C D,
Ng A Y, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. In: Proceedings of
the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). Seattle, USA: ACL Press,
2013. 1631−1642

61 Mnih A, Hinton G. Three new graphical models for statistical language modelling. In: Proceedings of the 24th International Conference on Machine Learning (ICML 07). Corvallis, Oregon: ACM Press, 2007. 641−648
62 Morin F, Bengio Y. Hierarchical probabilistic neural network language model. In: Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics
(AISTATS 2005). Barbados: Omni Press, 2005. 246−252
an A, Weston J,
63 Bordes A, Usunier N, Garcia-Dur´
Yakhnenko O. Translating embeddings for modeling multirelational data. In: Proceedings of the 2013 Advances
in Neural Information Processing Systems 26 (NIPS 13).
Nevada, USA: MIT Press, 2013. 2787−2795
64 Bengio Y. Deep learning of representations for unsupervised
and transfer learning. In: Proceedings of the ICML2011
Unsupervised and Transfer Learning Workshop. Bellevue,
Washington, USA: ACM Press, 2012. 17−37

52 Turian J, Ratinov L, Bengio Y. Word representations: a
simple and general method for semi-supervised learning. In:
Proceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010). Uppsala, Sweden: ACL Press, 2010. 384−394

65 Le Q V, Ngiam J, Coates A, Lahiri A, Prochnow B, Ng A Y.
On optimization methods for deep learning. In: Proceedings
of the 28th International Conference on Machine Learning
(ICML 11). Bellevue, Washington, USA: ACM Press, 2011.
67−105

53 Firth J R. A synopsis of linguistic theory 1930-55. Studies
in Linguistic Analysis. Oxford: Philological Society, 1957.
1−32

66 Henderson J. Neural network probability estimation for
broad coverage parsing. In: Proceedings of the 10th Conference on European Chapter of the Association for Computational Linguistics (EACL 03). Budapest, Hungary: ACL
Press, 2003. 131−138

54 Hinton G E. Learning distributed representations of concepts. In: Proceedings of the 8th Annual Conference of the
Cognitive Science Society. Amherst, Massachusetts: Cognitive Science Society Press, 1986. 1−12
55 Salton G. Automatic processing of foreign language documents. Journal of the American Society for Information
Science, 1970, 21(3): 187−194
56 Rapp R. Word sense discovery based on sense descriptor
dissimilarity. In: Proceedings of the 9th Conference on Machine Translation Summit. New Orleans, USA: IAMT Press,
2003. 315−322
57 Turney P D. Expressing implicit semantic relations without
supervision. In: Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Linguistics
(COLING and ACL 2006). Sydney, Australia: ACL Press,
2006. 313−320
utze H. Introduction to
58 Manning C D, Raghavan P, Sch¨
Information Retrieval. Cambridge: Cambridge University
Press, 2008.

67 Henderson J. Discriminative training of a neural network statistical parser. In: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics
(ACL 2004). Barcelona, Spain: ACL Press, 2004. 95−102
68 Titov I, Henderson J. Porting statistical parsers with datadefined kernels. In: Proceedings of the 10th Conference on
Computational Natural Language Learning (CoNLL-2006).
New York, USA: ACL Press, 2006. 6−13
69 Titov I, Henderson J. Constituent parsing with incremental sigmoid belief networks. In: Proceedings of the 45th
Annual Meeting on Association for Computational Linguistics (ACL 2007). Prague, Czech Republic: ACL Press, 2007.
632−639
70 Collobert R. Deep learning for efficient discriminative parsing. In: Proceedings of the 14th International Conference on
Artificial Intelligence and Statistics (AISTATS 2011). Fort
Lauderdale, Florida, USA: Omni Press, 2011. 224−232
71 Costa F, Frasconi P, Lombardo V, Soda G. Towards incremental parsing of natural language using recursive neural
networks. Applied Intelligence, 2003, 19(1−2): 9−25

10 

: 

72 Menchetti S, Costa F, Frasconi P, Pontil M. Wide coverage natural language processing using kernel methods and
neural networks for structured data. Pattern Recognition
Letters, 2005, 26(12): 1896−1906
73 Collins M. Head-driven statistical models for natural language parsing. Computational linguistics, 2003, 29(4):
589−637
74 Socher R, Bauer J, Manning C D, Ng A Y. Parsing with
compositional vector grammars. In: Proceedings of the 51st
Annual Meeting on Association for Computational Linguistics (ACL 2013). Sofia, Bulgaria: ACL Press, 2013. 455−465
75 Legrand J, Collobert R. Recurrent greedy parsing with neural networks. In: Proceedings of the 2014 European Conference on Machine Learning and Knowledge Discovery in
Databases. Nancy, France: Springer Press, 2014. 130−144

1465

84 Cho K, van Merri¨
enboer B, Gulcehre C, Bahdanau D,
Bougares F, Schwenk H, Bengio Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP 2014). Doha, Qatar: ACL Press, 2014. 1724−1734
85 Cho K, van Merri¨
enboer B, Bahdanau D, Bengio Y. On the
properties of neural machine translation: encoder-decoder
approaches. In: Proceedings of the 8th Workshop on Syntax,
Semantics and Structure in Statistical Translation (SSST8). Doha, Qatar: ACL Press, 2014. 103−111
86 Bahdanau D, Cho K, Bengio Y. Neural machine translation
by jointly learning to align and translate. In: Proceedings
of the 3rd International Conference on Learning Representations (ICLR 2015). San Diego, California, USA: arXiv Press,
2015. 1409.0473V7

76 Huang E H, Socher R, Manning C D, Ng A Y. Improving
word representations via global context and multiple word
prototypes. In: Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL 2012).
Jeju Island, Korea: ACL Press, 2012. 873−882

87 Dong D X, Wu H, He W, Yu D H, Wang H F. Multi-task
learning for multiple language translation. In: Proceedings
of the 53rd Annual Meeting on Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing, China: ACL
Press, 2015. 1723−1732

77 Zhou S S, Chen Q C, Wang X L. Active deep networks for
semi-supervised sentiment classification. In: Proceedings of
the 23rd International Conference on Computational Linguistics (COLING 2010). Beijing, China: ACL Press, 2010.
1515−1523

88 Pinheiro P O, Collobert R. Recurrent convolutional neural networks for scene labeling. In: Proceedings of the 31st
International Conference on Machine Learning (ICML 14).
Beijing, China, 2014. 82−90

78 Glorot X, Bordes A, Bengio Y. Domain adaptation for largescale sentiment classification: a deep learning approach.
In: Proceedings of the 28th International Conference on
Machine Learning (ICML 11). Bellevue, Washington, USA:
Omni Press, 2011. 513−520
79 Socher R, Pennington J, Huang E H, Ng A Y, Manning
C D. Semi-supervised recursive autoencoders for predicting
sentiment distributions. In: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing
(EMNLP 2011). Edinburgh, UK: ACL Press, 2011. 151−161
80 Liu L M, Watanabe T, Sumita E, Zhao T J. Additive neural
networks for statistical machine translation. In: Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013). Sofa, Bulgaria: ACL Press,
2013. 791−801
81 Yang N, Liu S J, Li M, Zhou M, Yu N H. Word alignment
modeling with context dependent deep neural network. In:
Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL 2013). Sofa, Bulgaria:
ACL Press, 2013. 166−175
82 Kalchbrenner N, Blunsom P. Recurrent continuous translation models. In: Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Processing
(EMNLP 2013). Seattle, Washington, USA: ACL Press,
2013. 1700−1709
83 Sutskever I, Vinyals O, Le Q V. Sequence to sequence
learning with neural networks. In: Proceedings of the 2014
Advances in Neural Information Processing Systems 27
(NIPS 14). Montr´
eal, Quebec, Canada: MIT Press, 2014.
3104−3112

89 Le Q V. Building high-level features using large scale unsupervised learning. In: Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. Vancouver, BC: IEEE, 2013. 8595−8598
90 Tian Yuan-Dong. A simple analysis of AlphaGo. Acta Automatica Sinica, 2016, 42(5): 671−675
(. . , 2016, 42(5):
671−675)

 
. 
, , .
E-mail: xfxi@mail.usts.edu.cn
(XI Xue-Feng
Ph. D. candidate at
the School of Computer Science and
Technology, Soochow University. His
research interest covers natural language understanding, discourse analysis and questionanswering.)
 . 
, , 
. .
E-mail: gdzhou@suda.edu.cn
(ZHOU Guo-Dong
Distinguished
professor at the School of Computer
Science and Technology, Soochow University. His research interest covers natural language understanding, Chinese computing, and information extraction. Corresponding author of this paper.)