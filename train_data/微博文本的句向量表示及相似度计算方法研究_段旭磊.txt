 43 
Vol． 43

5 
No． 5

    
Computer Engineering

··

： 1000-3428（ 2017） 05-0143-06

2017  5 
M ay 2017
： A

： TP391


，，
（  ， 100192）

 ：  Word2vec ，，。 3 
， Word2vec  TF-IDF ； 
； ，。 3 ，
。，。
： ； ； ； ； 
： ，，． ［J］． ，2017，
43（ 5） ： 143-148．
： Duan Xulei，Zhang Yangsen，Sun Yizhuo． Ｒesearch on Sentence Vector Ｒepresentation and Similarity
Calculation M ethod About M icroblog Texts［J］． Computer Engineering，
2017，
43（ 5） ： 143-148．

Ｒesearch on Sentence Vector Ｒepresentation and Similarity
Calculation Method About Microblog Texts
DUAN Xulei，ZHANG Yangsen，SUN Yizhuo
（ Institute of Intelligence Information Processing，Beijing Information Science and Technology University，Beijing 100192，China）

【Abstract】In Word2vec framework，aiming at the feature of M icroblog text，this paper proposes the method using w ord
vector or sentence vector of high dimension word database mapping calculation，and constructs sentence vector using three
algorithms： Using Word2vec to expand text，and TF-IDF to obtain sentence vector． Adding the w ord vectors into sentence
vector； Building a w ord bank to obtain high dimension vector space for the sentence． Through comparing the three
methods，it selects the model fitting M icroblog field． Experimental results show that the sentence vector calculation
method using high dimension word database mapping is the best．
【Key words】M icroblog texts； similarity calculation； w ord vector； high dimension word database； sentence vector
DOI： 10． 3969 / j． issn． 1000-3428． 2017． 05． 023

0



，
 140 ，，
，
，
，“

”、“…”。，

，
，，
。，
。 ，
，

，
。
Word2vec［1-2］
，            （ Vector Space
M odel，VSM ） ， Distributed Ｒepresentation
。
，
，
。，Word2vec 
，
。Word2vec 
，、、、

： （ 61370139） ； （ IDHT20130519） 。
： （ 1991—） ，，，、、； （ ） ，、； ，
。
： 2016-07-04

： 2016-08-15

E-mail： zys@ bistu． edu． cn

144







［3］
、、 。
，，
，
。 Word2vec ，
，
，
，。 ，
，
， 。

1



，、
、、、、、。 ，
、，
、，，
。 ，
、，
，
。
       
 。 
 ， 。 
 ， VSM 
 ，             。  VSM
 ， ，

 ， 。 
 ， ，
 、
 ， TF-IDF  ，
［4］
 。 
VSM                  
 ， ，
 ， 。 
 ， ，
    ，      （ How Net）
 WordNet 
［5-8］
。

，VSM 
       。      ，  
Distributed Ｒepresentation      ，   
“Word Ｒepresentation”“Word Embedding ”， 
“ ”。  Hinton  1986 
［9］
［10］
 ， Bengio ， Word2vec 
。， Word2vec
。
Word2vec 





2017  5  15 

， （ 
、    ）             。
Word2vec  Huffman ，

，，
，。Word2vec 
， ，
［11］， Word2vec 
。
Word2vec        （ Latent Semantic
Index，LSI ） 、        （ Latent Dirichlet
Allocation，LDA） ，
，。 1 ，
 the，cat，sat ，
 4  on。

1



        ，  ［12］  
Sentence Vector        。 Sentence
Vector ，
， Sentence Vector 
           。  ，Sentence
Vector    Bag-Of-Words   Word    
，。  2 
。

2



 Word2vec， （ Sentence Vector） 
 2 ：
1）  Sentence id，
 id。Sentence id 
 Word ，， Sentence
Vector。Sentence Vector  Word Vector 
， 2 。 
，Sentence Vector  Word Vector 
， Softmax 。 
，Sentence id ，

 43 

5 

，，： 

 Sentence Vector，
，。
2）     ，           
Sentence id， Voftmax 
，
   。    ，        
Sentence Vector。
， Word2vec 
。 3 ，
，
 VSM ，。
，，
，
，
，，
。
，
Wordnet  ，
，
［13］
 。，
，
，。

2



 Word2vec 
，
。 ， 3 
，。
1）        ，  TF-IDF   
。
 TF-IDF ，
。 
，。
， Word2vec
［13］
 ， TF-IDF 
，
， TF-IDF 
。
、
     ，          。
TF-IDF TF ，
。，
。IDF ，
，
，。 
 TF-IDF ，，
。：
 1  S n  S m ，

145

 S'n  S'm 。
 2  Word2vec         
，
， S″n  S m ″。
 3  TF-IDF  S″n  S m ″
 N ， 2 ，
 S v 。
 4  S″n  S m ″  S v
， S v1  S v2 。
 5  Sv1  Sv2 ，
 sim（ Sv1 ，
Sv2 ） = cos（ Sv1 ，
Sv1 ） 。
2） ，
。
，
 Word2vecde ，
，。
 S i  K i ，S i = （ V i1 ，V i2 ，…，V ic ，…，
V iN ）  Word2vec 
，W i （ j ） = （ V 1 （ j ） ，V 2 （ j ） ，…，V C （ j ） ，…，
V N （ j） ）  i  j ，N 
，c  N  c ，
 （ 1） 。
1
Si =
（ 1）
∑ W （ j）
K i 1≤j≤K i i
     ，  （   ）     
（ 2） ：
sim（ S n ，S m ） = ∑ V nC × V mC
（ 2）
1≤C≤N

：
 1  S n  S m ，
 S'n  S'm 。
 2  Word2vec         
。    
，（ 1）  S v1  S v2 。
 3 （ 2）  S v1  S v2 ，
sim（ S v1 ，S v2 ） 。
3） ，，
。
，，
，
。，
， 。
 N ，
，
N ，
 。  
“”，
、、、
。“”，、



146





、。 
，，
。 Bag Of
Words 。
 Word2vec     2    X z  Y l 
  ，X z   N      z     
 ，Y l  i  l  ，
 L（ L            ，  L 
） 。
S i = ｛ max （ X1 × Y Tl ） ，max （ X2 × Y Tl ） ，…，
1≤l≤L

1≤l≤L

max （ X N × Y Tl ） ｝

（ 3）

1≤l≤L

   2        
，（ 4） 。
sim（ S i ，S l ） = cos（ S i ，S l ）
（ 4）
：

 1  S n  S m ，
 S'n  S'm 。
 2  （ 3 ） ， Word2vec 
，

， S v1  S v2 。
 3 ， sim（ S v1 ，S v2 ）
= cos（ S v1 ，S v1 ） 。

3




，
 1  1 。 
， 5 ，
 5 
， ：

3． 1

1






2017  5  15 

43 225 049 ， ICTCLAS 
。 ，
，
［14］
 。
 80W ，
、、、，
“”“”“”“”“
”“ ”。 ，
。
：
： “，，
，，！ ”
： “    ，  
， ，      ！ ”
： “   ，
   ，  ，      ，  
 ！”
，
 Word2vec 。 
［15］
 ，
， Word2vec ，
。
，，
，
 10 GB。  Word2vec 
， VectorSize  Window Size 
， 100  8。
 Word2vec ， analogy 

， 1 ，，analogy（ “”“
”“”）  －  +  = ？。

Word2vec 
TOP1

TOP2

TOP3

analogy（ “”“”“”）

： 0． 992

： 0． 990

： 0． 983

similar（ “”）

： 0． 920

： 0． 875

： 0． 874

 1 ， Word2Vec 
，，：
“”， 。
 A1，B1，A2，B2，A3，B3 
。， TF-IDF 
，。 
 A4，B4。
A1： “，”
B1： “，，
”
A2： “# #【［］： 

 】，、
，
！ ，
！ ↓！ ”
B2： “# #【！ 
 】13 ，，
。，。
？ 
？ ？
？  ↓    
！ ”

 43 

5 

，，： 

A3： ”##，
，
。，
。，
，
。#  # @ 
 @ ”
B3： “# 120 #
，，，
，！ @ ”
A4： ，
。，。
。！
。 ，
。Pray for Paris［］？ ［］？ ［］？。 
。
。
。
。KevinWangCHN 
。《》，
。
，
？。。 \［
### \］
B4： ，
，， 
，，
？         ？。 Pray For Paris ［
］？。，
［］［］［ ］。 ［ ］。 
。，。 
 ～ 
。［］
 3  ［］［ ］。
，
？ 
？ ？ 
？   ，       ？
， ，
，
。   
 …… \［
### \］
3． 2 
 4 ，
 1 ～  3  2 
，
 4 ［10］
，
 3 。
1）  1
 2  1   A1 ～ A4    
B1 ～ B4 ， TF-IDF 
， 2 。

2

147

 1 



A1 / B1

A2 / B2

A3 / B3

A4 / B4

（  1） TF-IDF

0

0

0

0． 739

（  2）  + TF-IDF

0

0

0

0． 842

，
，，。
 TF-IDF ，       
，，
，
，，
。
2）  2
 2  2   A1 ～ A4    
B1 ～ B4 ，
，
 3 。
3

 2 


（  1） 

A1 / B1 A2 / B2 A3 / B3

A4 / B4

0． 289

0． 960

0． 205

0． 083

， 1 
，，
 1 ，。
3）  3
 2 
。 N 
，
， N ，，
     2         。  ，
N = 3 000，
5 000， 4 。
4


 3 
A1 / B1

A2 / B2

A3 / B3

A4 / B4

（  1） N = 3 000

0． 986 0

0． 963

0． 371

0． 970

（  2） N = 5 000

0． 985 5

0． 882

0． 423

0． 971

，，
。
，，
，。，
。  3 000，5 000
， 3 000 
。 3 000 ， Hadoop 

3 000 。
4）  4
［10］ 3 
，
，，



148





。，
10 000，
100 000。 5 。
 2 000，
5

 4 



A1 / B1

A2 / B2

A3 / B3

A4 / B4

（  1） Sentence-2000

0． 988

0． 781

0． 897

0． 574

（  2） Sentence-10000

0． 970

0． 538

0． 380

0． 516

（  3） Sentence-100000

0． 878

0． 274

0． 032

0． 361

 4 ，
，。 
，
。
，，
，，
，，
。， 3。
 4  6，
，
，
。  6 ，
 3 。
，
， 3。 ，
 SPAＲK 。
6

4





A1 / B1

A2 / B2

A3 / B3

A4 / B4

 1（  1）

0． 000 0

0． 000

0． 000

0． 739

 1（  2）

0． 000 0

0． 000

0． 000

0． 842

 2（  1）

0． 289 0

0． 205

0． 083

0． 960

 3（  1）

0． 986 0

0． 963

0． 371

0． 970

 3（  2）

0． 985 5

0． 882

0． 423

0． 971

 4（  1）

0． 988 0

0． 781

0． 897

0． 574

 4（  2）

0． 970 0

0． 538

0． 380

0． 516

 4（  3）

0． 878 0

0． 274

0． 032

0． 361



0． 999 0

0． 980

0． 300

0． 980



 3 ，
， 3 ，
，
，
。，，
 Word2vec 
。，
，，
。





2017  5  15 

。
，

，
。

［1］ Mikolov T，
Chen K，Corrado G，et al． Efficient Estimation
of Word Ｒepresentations in Vector Space ［C］/ / ProUSA： ACM Press，
2013：
ceedings of ICLＲ’13． New York，
1-12．
［2］ Mikolov T，Yih W T，Zweig G． Linguistic Ｒegularities in
Continuous Space Word Ｒepresentations［C］/ / Proceedings
of HLT-NAACL’13． New York，USA： ACM Press，
2013：
236-248．
［3］ Feng S，Liu Ｒ，Wang Q，et al． Word Distributed
Ｒepresentation Based Text Clustering［C］/ / Proceedings
of International Conference on Cloud Computing and
Intelligence Systems． Washington D． C． ，USA： IEEE
Press，
2014： 213-225．
［4］  ， ，，． 
     ［J］．     ，2015，38 （ 7 ） ：
1420-1433．
［5］ ， ，，．  How Net 
2006，
20（ 1） ： 14-20．
［J］． ，
［6］ ， ， ．          
TF-IDF ［J］． ，
2011，
34（ 5） ： 856-864．
［7］ ，，． 
2007，
21（ 1） ： 96-100．
［J］． ，
［8］ ， ，，． 《》
2014，
40（ 12） ： 177-181．
［J］． ，
［9］ Hinton G E． Learning Distributed Ｒepresentations of Concepts［C］/ / Proceedings of the 8th Annual Conference on
Cognitive Science Society． Amherst，
USA： ［s． n． ］
，
1986： 1-12．
［10］ Bengio Y，Schw enk H，Senécal J S，et al． A Neural
Probabilistic Language M odel［J］． Journal of M achine
Learning Ｒesearch，
2003，
3（ 6） ： 1137-1155．
［11］ M ikolov T，Sutskever I，Chen K，et al． Distributed
Ｒepresentations of Words and Phrases and Their
Compositionality［J］． Advances in Neural Information
Processing Systems，
2013，
26（ 1） ： 3111-3119．
［12］ Le Q V，M ikolov T． Distributed Ｒepresentations of
Sentences and Documents［C］/ / Proceedings of the 31st
International Conference on M achine Learning．
Washington D． C． ，USA： IEEE Press． 2014： 1188-1196．
［13］ Zhang W，Xu W，Chen G，et al． A Feature Extraction
Method Based on Word Embedding for Word Similarity
Computing ［J］． Communications in Computer ＆ Info2014，
496（ 1） ： 160-167．
rmation Science，
［14］ ，，． ［J］．
2012，
26（ 4） ： 21-27，
42．
，
［15］ Lai S，Liu K，Xu L，et al． How to Generate a Good Word
Embedding？ ［J］． IEEE Intelligent Systems，2015，
31（ 6） ： 5-14．
 