第 ３６ 卷第 ５ 期

河 北 科 技 大 学 学 报

Ｖｏ
ｌ．
３６，
Ｎｏ．
５

２０１５ 年 １０ 月

Ｊ
ｏｕ
ｒ
ｎａ
ｌｏ
ｆＨｅｂｅ
ｉＵｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
ｆＳｃ
ｉ
ｅｎｃ
ｅａｎｄＴｅ
ｃｈｎｏ
ｌ
ｏｇｙ
ｙｏ

Ｏｃ
ｔ．２０１５


文章编号：
１００８－１５４２（
２０１５）
０５－０５３２－０７

ｄｏ
ｉ：
１０．
７５３５／ｈｂｋｄ．
２０１５ｙｘ
０５０１４

基于 ＰＯＳ－ＣＢＯＷ 语言模型的相似词分析
阮冬茹 ，潘洪岩 ，高

凯

（河北科技大学信息科学与工程学院，河北石家庄 ０５００１８）
摘

要：相似词分析是自然语言处理领域的研究热 点之一，在 文 本 分 类、机 器 翻 译 和 信 息 推 荐 等 领

域中具有重要的研究价值和应用意义。针对新浪微博短 文 本 的 特 点，给 出 一 种 带 词 性 的 连 续 词 袋
模型（
ＰＯＳ－ＣＢＯＷ）。该模型在连续词袋模型的基础上加入过滤层 和 词性 标 注层，对 空 间词 向量进
行优化和词性标注，通过空间词向量的余弦相似度和词性相似度来判别词向量的相似性，并利用统
计分析模型筛选出最优相似词 集 合。 实 验 表 明，基 于 ＰＯＳ－ＣＢＯＷ 语 言 模 型 的 相 似 词 分 析 算 法 优
于传统 ＣＢＯＷ 语言模型。
关键词：自然语言处理；语言模型；词向量；相似词；
ＰＯＳ－ＣＢＯＷ
中图分类号：
ＴＰ３９１

文献标志码：
Ａ

Ｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｓａｎａ
ｌ
ｉ
ｓｂａ
ｓ
ｅｄｏｎＰＯＳ－ＣＢＯＷｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ
ｙｓ
ＲＵＡＮ Ｄｏｎｇ
ｒｕ，ＰＡＮ Ｈｏｎｇｙａｎ，ＧＡＯ Ｋａ
ｉ
（
Ｓｃｈｏｏ
ｌｏ
ｆＩ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎＳｃ
ｉ
ｅｎｃ
ｅａｎｄ Ｅｎｇ
ｉ
ｎｅ
ｅ
ｒ
ｉ
ｎｇ，Ｈｅｂｅ
ｉＵｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
ｆＳｃ
ｉ
ｅｎｃ
ｅａｎｄ Ｔｅ
ｃｈｎｏ
ｌ
ｏｇｙ，Ｓｈ
ｉ
ｉ
ａ
ｚｈｕａｎｇ，Ｈｅｂｅ
ｉ０５００１８，
ｙｏ
ｊ
Ｃｈ
ｉ
ｎａ）

Ａｂ
ｓ
ｔ
ｒ
ａ
ｃ
ｔ：
ｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ，ａｎｄｉ
Ｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｓａｎａ
ｌ
ｓ
ｉ
ｓｉ
ｓｏｎｅｏ
ｆｔ
ｈｅｉｍｐｏ
ｒ
ｔ
ａｎ
ｔａ
ｓｐｅ
ｃ
ｔ
ｓｉ
ｎｔ
ｈｅｆ
ｉ
ｅ
ｌ
ｄｏ
ｆｎａ
ｔ
ｕ
ｒ
ａ
ｌｌ
ａｎｇｕａｇｅｐ
ｔｈａ
ｓｉｍｐｏ
ｒ－
ｙ
ｔ
ａｎ
ｔｒ
ｅ
ｓ
ｅ
ａ
ｒ
ｃｈａｎｄａｐｐ
ｌ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎｖａ
ｌ
ｕｅ
ｓｉ
ｎｔ
ｅｘ
ｔｃ
ｌ
ａ
ｓ
ｓ
ｉ
ｆ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎ，ｍａ
ｃｈ
ｉ
ｎｅｔ
ｒ
ａｎｓ
ｌ
ａ
ｔ
ｉ
ｏｎａｎｄｉ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎｒ
ｅ
ｃ
ｏｍｍｅｎｄａ
ｔ
ｉ
ｏｎ．Ｆｏ
ｃｕｓ
ｉ
ｎｇｏｎｔ
ｈｅ
ｆＳ
ｉ
ｎａ Ｗｅ
ｉ
ｂｏ
＇
ｓｓｈｏ
ｒ
ｔｔ
ｅｘ
ｔ，ｔ
ｈ
ｉ
ｓｐａｐｅ
ａｎｇｕａｇｅｍｏｄｅ
ｌｎａｍｅｄａ
ｓＰＯＳ－ＣＢＯＷ，ｗｈ
ｉ
ｃｈｉ
ｓａｋ
ｉ
ｎｄｏ
ｆｃ
ｏｎ
ｔ
ｉ
ｎｕｏｕｓ
ｒｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ｓａｌ
ｆ
ｅ
ａ
ｔ
ｕ
ｒ
ｅ
ｓｏ
ｂａｇ－ｏ
ｆ－ｗｏ
ｒ
ｄｓｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌｗｉ
ｔ
ｈｔ
ｈｅｆ
ｉ
ｌ
ｔ
ｅ
ｒ
ｉ
ｎｇｌ
ａｙｅ
ｒａｎｄｐａ
ｆ－ｓ
ｅ
ｃｈｔ
ａｇｇ
ｉ
ｎｇｌ
ａｙｅ
ｒ．Ｔｈｅｐ
ｒ
ｏａ
ｃｈｃ
ａｎａｄ
ｕｓ
ｔｔ
ｈｅ
ｒ
ｔ－ｏ
ｒ
ｏｐｏ
ｓ
ｅｄａｐｐ
ｐｅ
ｊ
ｗｏ
ｒ
ｄｖｅ
ｃ
ｔ
ｏ
ｒ
ｓ
＇ｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ｃ
ｃ
ｏ
ｒ
ｄ
ｉ
ｎｇｔ
ｏｔ
ｈｅｃ
ｏ
ｓ
ｉ
ｎｅｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ｈｅ ｗｏ
ｒ
ｄｖｅ
ｃ
ｔ
ｏ
ｒ
ｓ
＇ｐａ
ｒ
ｔ－ｏ
ｆ－ｓ
ｅ
ｃｈ ｍｅ
ｔ
ｒ
ｉ
ｃ
ｓ．Ｉ
ｔｃ
ａｎａ
ｌ
ｓ
ｏｆ
ｉ
ｌ
ｔ
ｅ
ｒｔ
ｈｏ
ｓ
ｅ
ｙａ
ｙａｎｄｔ
ｐｅ
ｒ
ｄｓｓ
ｅ
ｔｏｎｔ
ｈｅｂａ
ｓ
ｅｏ
ｆｔ
ｈｅｓ
ｔ
ａ
ｔ
ｉ
ｓ
ｔ
ｉ
ｃ
ａ
ｌａｎａ
ｌ
ｓ
ｉ
ｓ ｍｏｄｅ
ｌ．Ｔｈｅｅｘｐｅ
ｒ
ｉｍｅｎ
ｔ
ａ
ｌｒ
ｅ
ｓｕ
ｌ
ｔｓｈｏｗｓｔ
ｈａ
ｔｔ
ｈｅｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｓａｎａ
ｌ
ｓ
ｉ
ｓ
ｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｙ
ｙ
ａ
ｌ
ｒ
ｉ
ｔ
ｈｍｂ
ｒ
ｏｐｏ
ｓ
ｅ
ｄＰＯＳ－ＣＢＯＷｌ
ａ
ｓ
ｅ
ｄｏｎｔ
ｈ
ｅｐ
ａ
ｎｇｕ
ａ
ｅｍｏｄ
ｅ
ｌｉ
ｓｂ
ｅ
ｔ
ｔ
ｅ
ｒｔ
ｈ
ａ
ｎｔ
ｈ
ａ
ｔｂ
ａ
ｓ
ｅ
ｄｏｎｔ
ｈ
ｅｔ
ｒ
ａ
ｄ
ｉ
ｔ
ｉ
ｏｎ
ａ
ｌＣＢＯＷｌ
ａ
ｎｇｕ
ａ
ｅｍｏｄ
ｅ
ｌ．
ｇｏ
ｇ
ｇ

Ｋｅ
ｒ
ｄ
ｓ：
ｎａ
ｔ
ｕ
ｒ
ａ
ｌｌ
ａｎｇｕａｇｅｐ
ａｎｇｕａｇｅｍｏｄｅ
ｌ；ｗｏ
ｒ
ｄｖｅ
ｃ
ｔ
ｏ
ｒ；ｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｓ；ＰＯＳ－ＣＢＯＷ
ｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ；ｌ
ｙｗｏ
相似词分析是近些年自然语言处理领域的研究热点之一，在文本分类、机器翻译以及信息推荐等领域中
有着广泛应用。目前相似词的分析大 都 需 要 人 为 干 预 为 主 的 方 法，借 助 人 工 标 注 词 典 来 设 定 词 的 相 似 性。
但是随着社交网络中网络新词的不断涌现，基于人工标注的方法已无法完成庞大的标注任务，而且由于社交
网络的短文本特征（如数据量庞大、书写不规范等），传统方 法 已无法得到较 好 的 分 析 结 果。 现 阶 段，自 然 语
言处理、深度学习等领域的相似词分析研究是解决这一问题的主要手段之一。

收稿日期：
２０１５－０４－１４；修回日期：
２０１５－０６－２６；责任编辑：陈书欣
基金项目：河北省社会科学发展研究课题资助项目（
２０１５０３０３４４）
作者简介：阮冬茹（
１９６７—），女，河北怀安人，副教授，主要从事自然语言处理、微博计算方面的研究。
通讯作者：高

凯副教授。Ｅ－ｍａ
ｉ
ｌ：
ｉ＠ｈｅｂｕｓ
ｔ．
ｅｄｕ．
ｃｎ
ｇａｏｋａ

阮冬茹，潘洪岩，高

凯 ．基于 ＰＯＳ－ＣＢＯＷ 语言模型的相似词分析［
Ｊ］．河北科技大学学报，
２０１５，
３６（
５）：
５３２－５３８．

ＲＵＡＮ Ｄｏｎｇ
ｒ
ｕ，ＰＡＮ Ｈｏｎｇｙａｎ，ＧＡＯ Ｋａ
ｉ．
Ｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｓａｎａ
ｌ
ｉ
ｓｂａ
ｓ
ｅｄｏｎＰＯＳ－ＣＢＯＷｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ［
Ｊ］．
Ｊ
ｏｕｒ
ｎａ
ｌｏ
ｆＨｅｂｅ
ｉＵｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
ｆ
ｙｓ
ｙｏ
Ｓｃ
ｉ
ｅｎｃ
ｅａｎｄＴｅ
ｃｈｎｏ
ｌ
ｏｇｙ，
２０１５，
３６（
５）：
５３２－５３８．

第５期

阮冬茹，等：基于 ＰＯＳ－ＣＢＯＷ 语言模型的相似词分析

５３３

［］
相关研究工作中，
ＢＥＮＧＩＯ 等 １ 利用 ２ 个语句 对 相似词 的概 念进行 了阐 述，句 子 １“
Ｔｈｅｃ
ａ
ｔｉ
ｓｗａ
ｌ
ｋ
ｉ
ｎｇ
ｈｅｂｅｄ
ｒ
ｏｏｍ”和句子２“
Ａｄｏｇｗａ
ｓｒｕｎｎ
ｉ
ｎｇｉ
ｎａｒ
ｏｏｍ”，
２ 个语句在句式结构上非常相似，
ＢＥＮＧＩＯ 等定义
ｉ
ｎｔ
了ｔ
ｈｅ和 ａ，
ｃ
ａ
ｔ和 ｄｏｇ，
ｉ
ｓ和 ｗａ
ｓ，
ｗａ
ｌ
ｋ
ｉ
ｎｇ 和 ｒｕｎｎ
ｉ
ｎｇ，
ｂｅｄ
ｒ
ｏｏｍ 和 ｒ
ｏｏｍ 等为相似词。文献［
２］在神经网络语

言模型基础上进行了完善，重点解决了高维空间的维度问题，利用词在高维空间的概率分布得到词与词的相
似度。 ＭＩＫＯＬＯＶ 等 ［３－４］在 ＢＥＮＧＩＯ 等工作的基础上对神经网络做了进一步优化，减少了神经网络的参数，
提升了训练速度，并提出了一种新的神经网络语言模型，运用单 隐含 层的神经 网 络 生 成 词 向 量，计 算 词 的 相
［］
似性，并在文献［
５］中进一步优化提出了循 环神 经网络 语言 模型。 ＭＩＫＯＬＯＶ 等 ６ 提出 了 ＣＢＯＷ 模 型 和 连

续 Ｓｋ
ｒ
ａｍ 模型，这两种模型都是一种类前馈神经网络语言模型，不同的是 ＣＢＯＷ 模型 是 预测 相 似 词，而
ｉ
ｐ－ｇ
［］
ｒ
ａｍ 模型是预测相近词。随后，ＭＩＫＯＬＯＶ 等 ７ 提出了将词的相似性关系 应用于 机 器 翻译 领域，成 功
Ｓｋ
ｉ
ｐ－ｇ
地预测低频率词汇，并提出了对短 语 的 相 似 性 分 析 和 词 向 量 的 隐 含 语 义 关 系 分 析 ［８－９］。ＬＥＶＹ 等 ［１０］对 稀 疏

空间向量的语义规律关系进一步进行研究和完善。ＱＩＵ 等 ［１１］针对 ＣＢＯＷ 和 Ｓｋ
ｒ
ａｍ 在 词 向 量的 相 近性
ｉ
ｐ－ｇ
［ ］
和歧义性上的缺陷提出了 ＰＡＳ 模型，从而 将 准 确 率 提 高 了 约 １６．
９％ 。ＳＯＵＴＮＥＴ 等 １２ 利 用 Ｓｋ
ｉ
ｒ
ａｍ 与
ｐ－ｇ
ＬＳＴＭ 模型相结合的方法，提升了神经网络语言模型在 长文本 和短 文本中 的 处 理 效 果。文献［
１３］中在 词 袋
模型的基础上提出了段向量，相对于词向量而言，它克服了原有 模型的 一些缺 陷，在 文 本 分 类 和 语 义 分 析 等
领域表现良好。ＺＨＡＮＧ 等 ［１４］利用 ＣＢＯＷ－ＣＬ－Ｓ
ｉｍＨ 模型在语义层 面对 文本 做 重 复性 检 测，利 用 混 合 模 型
将文本生成了向量，进而计算文本的相似度。 ＭＮＩＨ 等 ［１５］提出了基于 ｎ－ｇ
ｒ
ａｍ 语言模型的概率语言 模型，该
模型通过对有序词序列生成词向量，然后利用给出的词向量模型来预测词序列中下一个词向量，该模型是表
［ ］
现较好的 ｎ－ｇ
ｒ
ａｍ 模型。ＢＬＥＩ等 １６ 提出了一种文档主题生成模型，利用词袋的方法，将 文 档 的 词生成 向 量，

从而利用概率分布生成主题，该模型是一个三层贝叶斯概率模型，词袋 中的词 是 无 序 的，所 以 简 化 了 模 型 复
杂度，提高了训练速度。 ＭＡＡＳ 等 ［１７］提出了一种ｌ
ｏｇ－ｂ
ｉ
ｌ
ｉ
ｎｅ
ａ
ｒ 模型来计算词向量的语义信息和情绪 信息，利
用监督与非监督的混合方法从语料中训练得到的词向量不仅有语义信息，还包含丰富的情绪信息，在情绪分
类方面表现较好。
在中文研究领域中，词语的相似度计算大都依托于 同义词 林、ＨｏｗＮｅ
ｔ和 中文 Ｗｏ
ｒｄＮｅ
ｔ等。 文 献［
１８］
采用同义词词林作为语义体系，对中文词语的同义词识别进行了初步研究。 吴思 颖 等 ［１９］采用 了 一 种 基 于中
文 Ｗｏ
ｒｄＮｅ
ｔ的中英文词语相似度计算方法，解决了候选同义 词 集 组 合 的 权 重 和 取 舍 问 题，实 现 了 一 个 可 以
计算英 －英、汉 －英、汉 －汉词语之间相似 度 的 算 法。 近 年 来，非 监 督 方 法 成 为 了 相 似 词 研 究 的 新 兴 方 法，石 静
等 ［２０］基于大规模语料的训练，在一定程度上 提 升 了 汉 语 语 义 相 似 度 计 算 的 准 确 率，并 实 现 了 不 同 语 域 的 集
成。文献［
２１］中通过构造领域类别核心词集，对词向量间语义关系进行了语义相似度的领域词语聚类分析。
文献［
２２］提出了一种新的分词方法，利用 Ｗｏ
ｒｄ２ｖｅ
ｃ生成词向量对中文词进行聚类分析。
本文针对微博短文本的特点，对 ＭＩＫＯＬＯＶ 等 ［５］在 Ｇｏｏｇ
ｌ
ｅＣｏｄｅ 中 的 开 源 连 续 词 袋 模 型 Ｗｏ
ｒｄ２ｖｅ
ｃ进
行了调整和改进，给出了一种基于 ＰＯＳ－ＣＢＯＷ 的语言模型。该模型 将 结构 调整为 输入 层、过滤 层、投 影 层、
词性标注层和输出层，通过过滤层对微 博 短 文 本 进 行 修 正，然 后 在 词 性 标 注 层 使 生 成 的 词 向 量 带 有 词 性 信
息，进而以空间 向 量 的 余 弦 值 和 向 量 的 词 性 比 较 为 条 件，计
算词的相似性。实验证明，该 方 法 在 微 博 短 文 本 分 析 中 有 较

输入层

高的准确率。

投影层

输出层

Input
W （t-2 ）

１ ＣＢＯＷ 语言模型
ＣＢＯＷ 语言模型是 ＭＩＫＯＬＯＶ 等

W （t-1 ）

SUM

Output

［
６］

阐述的一种类前馈

神经网络语言模型，由输 入 层、投 影 层 和 输 出 层 组 成，模 型 结

W （t ）
W （t+1 ）

构如图 １ 所示。ＣＢＯＷ 语言模型不同于标 准 词袋 模型，其引
入了连续分布式词表示的方法，形成了新的连续词袋模型。

W （t+2 ）

ＣＢＯＷ 语言模型通过将 语 料 库 词 Ｃ（
１），
Ｃ（
２），…，
Ｃ（
ｔ）
映射到投影层，得 到 词 典｜Ｖ｜∈Ｒｍ 。 然 后 通 过 共 享 投 影 层，
使语料库词 Ｃ（
ｔ）映 射 到 投 影 层 唯 一 的 位 置 Ｗ （
ｔ），再 通 过

Ｗ（
ｔ）的上下文信息预测 Ｗ （
ｔ），而其中每个 Ｗ （
ｔ）都 与上文的

图 １ ＣＢＯＷ 语言模型
Ｆ
ｉ
１ ＣＢＯＷ ｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ
ｇ．

河 北 科 技 大 学 学 报

５３４

２０１５ 年

词序列无关。ＣＢＯＷ 语言模型的基本训练步骤如下。
步骤 １：在输入层，通过 Ｍ 限定输入层的上下文 大 小，然 后 在 窗 口 中 顺 序 读 取 语 料 库 词 序 列 Ｃ（
ｔ－Ｍ ），

Ｃ（
ｔ－Ｍ ＋１），…，
Ｃ（
ｔ＋Ｍ ），通过哈希表得到投影层的相应词位置 Ｗ （
ｔ－Ｍ ），
Ｗ（
ｔ－Ｍ ＋１），…，
Ｗ（
ｔ＋Ｍ ），获
得Ｗ （
ｔ）词的上下 Ｍ 个词 Ｃｏｎ
ｔ
ｅｘ
ｔ（
Ｗ（
ｔ））。
步骤 ２：在投影层，对 Ｗ （
ｔ）的上下文 Ｃｏｎ
ｔ
ｅｘ
ｔ（
Ｗ（
ｔ））做步骤 １ 操作，
Ｖ（
ｔ）为 Ｗ （
ｔ）上下文累加和。
ｔ＋Ｎ

Ｖ（
ｔ）＝

ｔ））。
∑Ｃｏｎｔｅｘｔ（Ｗ （

（
１）

ｔ－Ｎ

步 骤３：从投影层到输出层，利用词 Ｗ （
ｔ）的上下文信息，通过式（
２）来生成词 Ｗ （
ｔ）的向量值，其中式（
３）
为词向量回归分析操作，来完成对 Ｗ （
ｔ）的判断。

Ｐ（
Ｗ（
ｔ）｜Ｃｏｎ
ｔ
ｅｘ
ｔ（
Ｗ（
ｔ）））＝
Ｖ（
ｔ）
θ）＝
ｆ（

ｔ＋ｎ

Ｖ（
ｔ）
θ），
ｆ（

∏

（
２）

ｔ－ｎ

１
。
ｔ）
ｘ
－Ｖ（
θ
１＋ｅ

（
３）

［］
综上，
ＣＢＯＷ 语言模型与 ＢＥＮＧＩＯ 等 １ 提 出 的 前 馈 神 经 网 络 语 言 模 型 相 似，该 模 型 去 掉 了 隐 含 层，加

快了语言模型的训练速度，且模型中每个词向量的计算只与滑动窗口限定的 Ｃｏｎ
ｔ
ｅｘ
ｔ有关系，减少 了 模型的
训练参数，降低了模型复杂度，提高了模型准确率。但是 ＣＢＯＷ 语言模型仍然需要大量的训练集，而且 训练
模型的好坏与上一级任务有密切关系。滑动窗口概念的引入，忽 略掉 了滑 动窗 口 中 的 上 下 文 内 容 对 词 的 相
似性计算产生的干扰。

２ ＰＯＳ－ＣＢＯＷ 语言模型及相似词计算
２．
１ ＰＯＳ－ＣＢＯＷ 语言模型
本 文 给 出 的 带 有 词 性 的 连 续 词 袋 ＰＯＳ－ＣＢＯＷ

输入层 过滤层 投影层 标注层 输出层

Input

语言模 型，其 结 构 如 图 ２ 所 示。 该 模 型 在 ＣＢＯＷ 语
言模型的基础 上 增 加 了 过 滤 层 和 词 性 标 注 层。 过 滤
层的主要作用是修正短文本语料语 序，优化 词向量空

W （t-2 ）

Filtering
Words

W （t-1 ）
SUM

间，从而使 ＰＯＳ－ＣＢＯＷ 语 言 模 型 相 对 于 前 一 阶 段 的

W （t ）/speech

工作更加独立；词性标注层的作用是 对所有 的词 向量
进行词性标注，使空间词向量在潜在 语义 关系的基础

POS Output

W （t+1 ）

上建立语法关系，从而提高相似词计算的准确率。
２．
１．
１ ＰＯＳ－ＣＢＯＷ 语言模型的过滤层

W （t+2 ）

微博作为一种抒发情感的载体，通常会加 入一些
符号来辅助情 感 的 表 达。 这 些 附 加 信 息 虽 然 具 有 一
定含义，但在本 文 的 实 际 工 作 中，它 改 变 了 训 练 语 料

图 ２ ＰＯＳ－ＣＢＯＷ 语言模型
Ｆ
ｉ
２ ＰＯＳ－ＣＢＯＷ ｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ
ｇ．

的正常语句结构，对训练模型产生 了干 扰。 又因为 微
博的短文本这一特点，使得这些符号在语句中占据较大权重，而这对于传统的长文本分析算法来说是无法处
理的难题。例如微博博文“
Ｉ（‘▽’）
ｌ
ｏｖｅｙｏｕ！”，符号“（‘▽’）”在未经过滤处理的情况下，极 有可能 会 与“
Ｉ”成
为相似词。所以，需要修正其为“
Ｉｌ
ｏｖｅｙｏｕ！”，去掉文本中的噪音信息。
ＰＯＳ－ＣＢＯＷ 语言模型的过滤层介于输入层和投影层之间，使用整理的微博文本停用词表对训练语 料进
行语句修正，从而达到优化词向量空间的目的。过滤层算法步骤如下。
步骤 １：初始化哈希表 Ｖｏｃ
ａｂ，初始值为 －１。
步骤 ２：循环读取训练语料句子的词，计算哈希值，并以哈希值为下标，Ｖｏｃ
ａｂ［Ｈａ
ｓｈ］＝１。
步骤 ３：初始化过滤层 Ｆ
ｉ
ｌ
ｔ
ｅ
ｒ，大小为停用词表个数，循环读取停用词，并计算通用词哈希 值，记 录到 过 滤
层Ｆ
ｉ
ｌ
ｔ
ｅ
ｒ表。
步骤 ４：遍历读取 Ｆ
ｉ
ｌ
ｔ
ｅ
ｒ表每项的值 ｈ，并查询 Ｖｏｃ
ａｂ［
ｈ］是否等于 １，等于 １ 则将 Ｖｏｃ
ａｂ［
ｈ］＝ －１，否 则
循环继续，直到 Ｆ
ｉ
ｌ
ｔ
ｅ
ｒ遍历完毕，过滤完毕。

第５期

阮冬茹，等：基于 ＰＯＳ－ＣＢＯＷ 语言模型的相似词分析

５３５

２．
１．
２ ＰＯＳ－ＣＢＯＷ 语言模型的词性标注层
ＣＢＯＷ 语言 模 型 是 一 种 概 率 模 型，通 过 将
语料中词的 上 下 文 信 息 生 成 相 应 的 词 向 量 映 射
到高维空间中，然后以词向量在 高维空间 中的 关
系来计算 词 与 词 之 间 的 相 似 度。 这 样 虽 然 提 高
了训练效率，加快了计算速 度，但是，同时 忽 略 了
上下文信息中一些不符合相 似词 定义 的词向 量。
例如图 ３ 是词向量“中国”的可视化图，在其临近
向量中出现了“当今”、“近年来”、“国 内”、“海外”
等词，这些词的出现在句子中位置均与“中国”临
近，但 是 严 格 地 从 相 似 词 定 义 上 来 讲，这 些 词 不
图 ３ 词向量“中国”可视化图

是“中国”的相似词。
为了排除上述词的干扰，得到 更 加准 确 的 相

Ｆ
ｉ
３ Ｖｉ
ｓｕａ
ｌ
ｉ
ｚ
ａ
ｔ
ｉ
ｏｎｏ
ｆｗｏ
ｒ
ｄｖｅ
ｃ
ｔ
ｏ
ｒ“
Ｃｈ
ｉ
ｎａ”
ｇ．

似词结果，引入了词性标注层。
ＰＯＳ－ＣＢＯＷ 语言模型的词性标注层介于投影层和输出 层之间，通 过 中 文 分 词 工 具 ＮＬＰＩＲ 对 生 成 词 向
量进行词性标注，所用词性为计算所有汉语词性标记集。针对词的多词性这一性质，对词语的词性建立了词
性体系，为相似词的计算提供词性参考体系，以便得出更加完善的相似词集。词性体系的构造步骤如下。
步骤 １：以 Ｒ 为根节点，创建以 Ｒ 为根节点的所有子节点。
步骤 ２：以上一级子节点为根节点，创建相应节点下的子节点。
步骤 ３：查看上一级节点是否有子节点，如果有，重复步骤 ２，否则构造完毕，如图 ４ 所示。

图 ４ 词性体系
Ｆ
ｉ
４ Ｓｙ
ｓ
ｔ
ｅｍ ｏ
ｆｓｐｅ
ｅ
ｃｈ
ｇ．

２．
２ 基于 ＰＯＳ－ＣＢＯＷ 语言模型的相似词计算
ＰＯＳ－ＣＢＯＷ 语言模型生成的词向量不仅包含潜在的语义关系，还包含着语法关系。语法关系的加 入 完
善了语义关系的不足。在相似词计算中以余弦相似度为计算方式，以语法关系为计算准则，进行词向量的相
似性计算。例如，计算词向量“中国”的 相 似 词，计 算 模 型 将 会 查 找 与 词 向 量 “中 国”同 在 一 个 父 类 词 性 下 的
词，再计算词向量的余弦相似度，从而得到 ２ 个词向量的 相似 度。其中 考虑 到 新 词 问 题，把 未 知 词 性 的 词 也
加入结果集。本文采用了两种择优算法，一是 ＴｏｐＮ 算 法，选 择 Ｎ 个 最 优 结 果；二 是 通 过 建 立 统 计 模 型，选
出最优结果集。
２．
２．
１ ＴｏｐＮ 词向量计算
ＴｏｐＮ 算法是择优的经典算法之一，通过排名得出前 Ｎ 个最优项作 为结 果。 本文在 相 似词 的计 算 中 利

河 北 科 技 大 学 学 报

５３６

２０１５ 年

用了 ＴｏｐＮ 的思想，通过结合余弦相似度和词性信息 ２ 个条件，对整个词向量空间遍历计算后，对相似 词 进
行排序，选出前 Ｎ 个词作为结果集。ＴｏｐＮ 个相似词计算的基本步骤如下。
步 骤１：取词向量空间第ｉ 个词向量Ｖｉ，在词性体系中查找Ｖｉ 的父类词性，如果与 Ｗ 为同一父类词性或
为“
ｕｎ”，进入步骤 ２，否则，查看向量空间是否遍历完，是则结束计算，否，
ｉ＝ｉ＋１，重复步骤 １ 操作。
步骤 ２：计算余弦相似度 Ｓ
ｉｍ（
Ｗ，
Ｖ ）＝Ｗ ·Ｖ／（
ｉｍ（
Ｗ，
Ｖ ）＜０，返 回 步 骤 １，否则 倒叙
｜Ｗ｜×｜Ｖ｜），如果 Ｓ
遍历 Ｓｅ
ｔ集合，比较相似度值，如果小于 Ｓ
ｉｍ（
Ｗ，

Ｖ ），将该位 置 的 值 后 移，插 入 Ｖ 到 该 位 置，重 复
步骤 １。
通过在 ＴｏｐＮ 算法中加入词性分 析，使同 一
词性体系的词聚集在一起，而不同词性的词向 量
则被排除 在 外。 例 如：重 新 利 用 ＰＯＳ－ＣＢＯＷ 语
言模型对词向量“中国”做相似词计算，发现与词
向量“中国”相 邻 近 的 词 向 量 “当 今”、“近 些 年”、
“国内”和 “海 外 ”等 被 排 出 了 结 果 集，如 图 ５
所示。
２．
２．
２ 词向量的统计分析模型
ＴｏｐＮ 算 法 中 相 似 词 的 计 算 结 果 往 往 受 到

Ｎ 值限定，从而导致一些较优词向 量的 丢 失。 为
了能够更加充分地获取最优结果 集，本文 给出 了

图 ５ 基于 ＰＯＳ－ＣＢＯＷ 语言模型相似词
计算的 ＴｏｐＮ 结果
Ｆ
ｉ
５ ＴｏｐＮ ｒ
ｅ
ｓｕ
ｌ
ｔｏ
ｆｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｂａ
ｓ
ｅｄｏｎ
ｇ．
ＰＯＳ－ＣＢＯＷ ｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ

另一种相似词计算方法，采用一种动态阈值的 统
计分析模型来选出结果集。

首先，计算出余弦相似度大于 ０ 的所有词向量，获取相似度集合，计算集合的三阶标准化矩，统计分析出
相似度值的概率分布，如图 ６ 所示。根据相似度集合的偏度，得出词向量相似度值的整体分布情况。如果词
向量相似度集合为正偏态，阈值设为集合的平均数，即结果集合 为平 均数的右 侧 部 分；如 果 集 合 偏 度 为 负 偏
度，那么阈值就选择集合的中位数，即 结 果 集 合 为 中 位 数 的 右 侧 部 分。 表 １ 为 集 合 的 概 率 分 布 和 阈 值 选 择
情况。
表 １ Ｓｅ
ｔ集合概率分布及阈值选取
Ｔａｂ．
１ Ｐｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｔ
ｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｉ
ｏｎｏ
ｆＳｅ
ｔａｎｄ
ｙｄ
ｔ
ｈｅｔ
ｈｒ
ｅ
ｓｈｏ
ｌ
ｄｓ
ｅ
ｌ
ｅ
ｃ
ｔ
ｉ
ｎｇ

负偏度

正偏度

偏度

分布状态

阈值

＜０

平均数 ＜ 中位数 ＜ 众数

平均数

＞０

众数 ＜ 中位数 ＜ 平均数

中位数

＝０

平均数 ＝ 中位数

平均数或中位数

图 ６ 偏度与数值的概率分布
ｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｔ
Ｆ
ｉ
６ Ｓｋｅｗｎｅ
ｓ
ｓａｎｄｉ
ｔ
ｓｐ
ｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｉ
ｏｎ
ｙｄ
ｇ．

三阶标准化矩（偏度）公式为

Ｘ－ ３］ μ３ ｋ３ 。
Ｓｋｅｗ（
Ｘ）＝Ｅ［（ μ）
＝ ３ ＝ ３／２
σ
σ ｋ２

（
４）

推导后得出偏度值公式为
ｎ

１
３
珚）
（
ｘｉ －ｘ
∑
ｎｉ
＝１

ｋ３
。
Ｓｋｅｗ（
Ｘ）＝ ３／２ ＝
ｎ
ｋ２
１
／２
２
３
珚））
（
（
ｘｉ －ｘ
∑
ｎｉ
＝１
输入词向量 Ｗ ，相似词计算算法步骤如下。

（
５）

步骤 １：取词向量空间第ｉ 个词向量 Ｖ ，在词性体系中查找 Ｖ 的 父类词 性，如果 与 Ｗ 为同 一 父 类 词 性或
为“
ｕｎ”，进入步骤 ２，否则，查看向量空间是否遍历完，是则结束计算，否，
ｉ＝ｉ＋１，重复步骤 １ 操作。

第５期

阮冬茹，等：基于 ＰＯＳ－ＣＢＯＷ 语言模型的相似词分析

５３７

步骤 ２：计算余弦相似度 Ｓ
ｉｍ（
Ｗ，
Ｖ）＝Ｗ ·Ｖ／（
Ｖ｜），如果 Ｓ
ｉｍ（
Ｗ，
Ｖ）＞０，倒叙遍历 Ｓｅ
ｔ集合，比较相
｜Ｗ｜×｜
似度值，如果小于 Ｓ
ｉｍ（
Ｗ，
Ｖ），将该位置的值（后值）后移，插入Ｖ 到该位置。重复步骤 １，否则进入步骤 ３。
步骤 ３：计算 Ｓｅ
ｔ的偏度（
Ｓｅ
ｔ的三阶标准化矩），如果 Ｓｅ
ｔ的偏度为正偏态，计算 Ｓｅ
ｔ平均值，以平均 值 为
阈值，筛选出最优词向量集合 Ｓｅ
ｔ
１，如果 Ｓｅ
ｔ偏度 为 负，计 算 Ｓｅ
ｔ中 位 数，以 中 位 数 为 阈 值，获 取 最 优 向 量 集
合 Ｓｅ
ｔ
２。

３ 实验对比及分析
本文所用实验数据为新浪微博语料，语 言 为 中 英 文 混 合，利 用 ＮＬＰＩＲ 分 词 工 具 对 语 料 进 行 分 词，整 理
出了 ２ＧＢ 文本，得到词数约 ４．
４ 亿词的训练集，训练得出约 ４２ 万条词向量。实验硬件环 境为 ２ 台 Ｌｅｎｏｖｏ－
Ｅｒ
ａ
ｚ
ｅ
ｒＸ３１０，处理器Ｉ
ｎ
ｔ
ｅ
ｌ４ 代ｉ
５－４４６０３．
２ＧＨｚ，
４ 核 ４ 线程，内存 ８ＧＢ。 分 别 利 用 Ｗｏ
ｒｄ２ｖｅ
ｃ中 的 ＣＢＯＷ
模型和 ＰＯＳ－ＣＢＯＷ 模型进行了训练，Ｗｉ
ｎｄｏｗ 值 为 ５，层 数 为 ２００ 的 情 况 下，没 有 加 入 反 例 训 练，使 用 ４ 线
程训练。通过 ２ 个模型的训练过程与训练结果的对比分析可得如下结论（表 ２）。
１）经过 ＰＯＳ－ＣＢＯＷ 模型的过滤层之后，微博语料集被过滤掉了约 １／４ 的 噪声，证明 针 对 微 博 数 据 的过
滤层是有效的，且生成的词向量空间相对来说压缩了约 ０．
７％ ；
２）从整个模型的训练过程分析，输入层的过滤与向量空间的压缩减少了训练过程中的迭代计算量，提高
了模型的计算效率。从表 ２ 可知，在同样条件下，
ＰＯＳ－ＣＢＯＷ 模型的训练时间也比 ＣＢＯＷ 模型的 训练 时 间
节省了 ５０％ 左右。
在接下来的实验中，分别利用 ＣＢＯＷ 语言模型和 ＰＯＳ－ＣＢＯＷ 语言 模型对 词 向 量“中 国”进行 了 相 似 词
计算，利用式（
６）对 ＣＢＯＷ 语言模型和 ＰＯＳ－ＣＢＯＷ 语言模型的 ＴｏｐＮ 相似词结果进行评价。
准确率 ＝

相似词数
×１００％ 。
结果集总数

（
６）

从图 ３ 和图 ５ 计算结果来看，
ＰＯＳ－ＣＢＯＷ 语言模 型 加 入 词 性 分 析 后，与 原 ＣＢＯＷ 语 言 模 型 相 比，不 同
词性的词向量被排除之后，使得相同词性的词向量聚集在一起。例如，
ＰＯＳ－ＣＢＯＷ 语言模型结果集中的“本
国”、“西方”、“某国”、“当今”和“各国”等不符合定 义 的词向量 被过 滤除去。为了 对比 ２ 个 模 型 的 准 确 率，对
ＣＢＯＷ 和 ＰＯＳ－ＣＢＯＷ 语言模型计算结果进行了分析，如表 ３ 所示。
表 ２ ＣＢＯＷ 语言模型和 ＰＯＳ－ＣＢＯＷ 语言模型参数
表 ３ 相似词计算分析

Ｔａｂ．
２ Ｐａ
ｒ
ａｍｅ
ｔ
ｅ
ｒ
ｓｏ
ｆＣＢＯＷ ｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌａｎｄ

Ｔａｂ．
３ Ａｎａ
ｌ
ｓ
ｉ
ｓｏ
ｆｓ
ｉｍｉ
ｌ
ａ
ｒｗｏ
ｒ
ｄｓ
ｙ

ａｎｇｕａｇｅｍｏｄｅ
ｌ
ＰＯＳ－ＣＢＯＷ ｌ
模型

ＣＢＯＷ

ＰＯＳ－ＣＢＯＷ

模型

ＣＢＯＷ

ＰＯＳ－ＣＢＯＷ

训练集总词数

４３７２８６８５０

４３７２８６８５０

样本总词数

２００

２００

过滤后的词数

未加入过滤层

３０９７４５０７６

名词数

１１０

１３６

４２６８４１

４２３５５６

动词数

１８

０

５

５

ｕｎ

４７

６４

网络层数

２００

２００

其他

２５

０

训练线程

４

４

相似词数

１３５

１６３

时间／ｍｉ
ｎ

３１４

１２２

准确率／％

６８

８２

词向量数
Ｗｉ
ｎｄｏｗ 值

由表 ３ 分析可知，针对特定输入词“中国”，
ＣＢＯＷ 和 ＰＯＳ－ＣＢＯＷ 两种语言模型的结果集都以 名词 性 的
词为主，但是 ＣＢＯＷ 语言模型往往会有一些与该名词相关的动 词或 者 其 他 不 相 关 的 词 在 内，而 这 些 词 是 不
符合相似词定义的。通过对比两种模型的结果，按照定义进行人工标注，
ＣＢＯＷ 语言模型的 Ｔｏｐ２００ 中 相 似
词数约为 １３５，准确率约为 ６８％ ，而 ＰＯＳ－ＣＢＯＷ 语言模型则为 １６３，准确 率约 为 ８２％ 。 显 然，在加 入 词 性 分
析后，
ＰＯＳ－ＣＢＯＷ 语言模型的准确率较高。

４ 结

语

本文针对微博短文本给出了一种带有词性的连续词袋模型———ＰＯＳ－ＣＢＯＷ 语言模型，通过加入过 滤 层

５３８

河 北 科 技 大 学 学 报

２０１５ 年

和词性标注层，对词向量空间进行优化和提升 相 似 词 计 算 的 准 确 率。 实 验 表 明，
ＰＯＳ－ＣＢＯＷ 语 言 模 型 在 词
向量空间质量和相似词计算方面优于 ＣＢＯＷ 语言模型。提出的模型通过加入词性标注层，对词向量空间 起
到了优化作用。但对于期望词量之间能包含更多的语义关系和语法关系的要求，目前尚有改进空间，这也是
未来的工作重点。
参考文献／Ｒｅ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ
ｓ：
［
１］ ＢＥＮＧＩＯ Ｙ，ＤＵＣＨＡＲＭＥ Ｒ，ＶＩＮＣＥＮＴ Ｐ，ｅ
ｔａ
ｌ．Ａ ｎｅｕｒ
ａ
ｌｐ
ａｎｇｕａｇｅ ｍｏｄｅ
ｌ［
Ｊ］．
Ｊ
ｏｕｒ
ｎａ
ｌｏ
ｆ Ｍａ
ｃｈ
ｉ
ｎｅＬｅ
ａ
ｒ
ｎ
ｉ
ｎｇ Ｒｅ
ｓ
ｅ
ａ
ｒ
ｃｈ，
ｒ
ｏｂａｂ
ｉ
ｌ
ｉ
ｓ
ｔ
ｉ
ｃｌ
，
（
）
：
２００３ ３ ６ １１３７－１１５５．
［
２］ ＢＥＮＧＩＯ Ｙ，ＢＥＮＧＩＯ Ｓ．Ｍｏｄｅ
ｌ
ｉ
ｎｇｈ
ｉ
ｉｍｅｎｓ
ｉ
ｏｎａ
ｌｄ
ｉ
ｓ
ｃ
ｒ
ｅ
ｔ
ｅｄａ
ｔ
ａ ｗｉ
ｔ
ｈ ｍｕ
ｌ
ｔ
ｉ
ａ
ｒｎｅｕｒ
ａ
ｌｎｅ
ｔｗｏ
ｒｋｓ［
Ｊ］．Ｎｅｕｒ
ａ
ｌＩ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎＰｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ
－ｌ
ｇｈ－ｄ
ｙｅ
（
）
，
（
）
：
Ｓｙｓ
ｔ
ｅｍｓ ＮＩＰＳ ２０００ １２ ４００－４０６．
［
３］ ＭＩＫＯＬＯＶ Ｔ．Ｌａｎｇｕａｇｅ Ｍｏｄｅ
ｌ
ｉ
ｎｇｆ
ｏ
ｒＳｐｅ
ｅ
ｃｈＲｅ
ｃ
ｏｇｎ
ｉ
ｔ
ｉ
ｏｎ［
Ｄ］．Ｂｒ
ｎｏ：
Ｂｒ
ｎｏ Ｕｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
ｆＴｅ
ｃｈｎｏ
ｌ
ｏｇｙ，２００７．
ｙｏ
［
］
，
，
，
／／
４ ＭＩＫＯＬＯＶ Ｔ ＫＯＰＥＣＫＹＪ ＢＵＲＧＥＴＬ ｅ
ｔａ
ｌ．Ｎｅｕｒ
ａ
ｌｎｅ
ｔｗｏ
ｒｋｂａ
ｓ
ｅｄｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ
ｓｆ
ｏ
ｒｈ
ｉ
ｌ
ｎ
ｆ
ｌ
ｅ
ｃ
ｔ
ｉ
ｖｅｌ
ａｎｇｕａｇｅ
ｓ［
Ｃ］
ＩＥＥＥＩ
ｎ
ｔ
ｅ
ｒ－
ｇｈ
ｙｉ
，
：
，
：
ｎａ
ｔ
ｉ
ｏｎａ
ｌＣｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅｏｎ Ａｃ
ｏｕｓ
ｔ
ｉ
ｃ
ｓ Ｓｐｅ
ｅ
ｃｈａｎｄＳ
ｉ
ｌＰｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ．Ｔａ
ｉ
ｉ ＩＥＥＥ ２００９ ４７２５－４７２８．
ｇｎａ
ｐｅ
［
／／Ｐｒ
５］ ＭＩＫＯＬＯＶ Ｔ，ＫＡＲＡＦＩＡＴ Ｍ，ＢＵＲＧＥＴ Ｌ，ｅ
ｔａ
ｌ．Ｒｅ
ｃｕｒ
ｒ
ｅｎ
ｔｎｅｕｒ
ａ
ｌｎｅ
ｔｗｏ
ｒｋｂａ
ｓ
ｅｄｌ
ａｎｇｕａｇｅ ｍｏｄｅ
ｌ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆＩ
ｎ
ｔ
ｅ
ｒ
ｓｐｅ
ｅ
ｃｈ．
：
［
］
，
：
Ｃｈ
ｉ
ｂａ ｓ．
ｎ． ２０１０ １０４５－１０４８．
［
／／ａ
６］ ＭＩＫＯＬＯＶ Ｔ，ＣＨＥＮ Ｋ，ＣＯＲＲＡＤＯ Ｇ，ｅ
ｔａ
ｌ．Ｅｆ
ｆ
ｉ
ｃ
ｉ
ｅｎ
ｔＥｓ
ｔ
ｉｍａ
ｔ
ｉ
ｏｎｏ
ｆ Ｗｏ
ｒ
ｄＲｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓｉ
ｎ Ｖｅ
ｃ
ｔ
ｏ
ｒＳｐａ
ｃ
ｅ［
ＥＢ／ＯＬ］．ｈ
ｔ
ｔ
ｒ
ｘ
ｉ
ｖ．
ｐ：
／
／
，
ｏ
ｒ
３７８１ ２０１３－０１－１６．
ｇ ａｂｓ１３０１．
［
／／ａ
７］ ＭＩＫＯＬＯＶ Ｔ，ＬＥ Ｑ Ｖ，ＳＵＴＳＫＥＶＥＲＩ．Ｅｘｐ
ｌ
ｏ
ｉ
ｔ
ｉ
ｎｇＳ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ｉ
ｅ
ｓ Ａｍｏｎｇ Ｌａｎｇｕａｇｅ
ｓｆ
ｏ
ｒ Ｍａ
ｃｈ
ｉ
ｎｅ Ｔｒ
ａｎｓ
ｌ
ａ
ｔ
ｉ
ｏｎ［
ＥＢ／ＯＬ］．ｈ
ｔ
ｔ
ｒ
ｘ
ｉ
ｖ．
ｐ：
／
／
，
４１６８ ２０１３－０９－１７．
ｏ
ｒ
ｇ ａｂｓ１３０９．
［
８］ ＭＩＫＯＬＯＶ Ｔ，ＳＵＴＳＫＥＶＥＲＩ，ＣＨＥＮ Ｋ，ｅ
ｔａ
ｌ．Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄ Ｒｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓｏ
ｆ Ｗｏ
ｒ
ｄｓａｎｄＰｈｒ
ａ
ｓ
ｅ
ｓａｎｄ Ｔｈｅ
ｉ
ｒＣｏｍｐｏ
ｓ
ｉ
ｔ
ｉ
ｏｎａ
ｌ
ｉ
ｔ
ＥＢ／
ｙ［
：
／
／
／
／
，
ＯＬ］．ｈ
ｔ
ｔ
ａ
ｒ
ｘ
ｉ
ｖ
．
ｏ
ｒ
ａ
ｂ
ｓ
１
３
１
０．
４
５
４
６
２
０
１
３
１
０
１
６．
－ －
ｐ
ｇ
［
／／Ｐｒ
９］ ＭＩＫＯＬＯＶ Ｔ，ＹＩＨ Ｗ Ｔ，ＺＷＥＩＧ Ｇ．Ｌ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃｒ
ｅｇｕ
ｌ
ａ
ｒ
ｉ
ｔ
ｉ
ｅ
ｓｉ
ｎｃ
ｏｎ
ｔ
ｉ
ｎｕｏｕｓｓｐａ
ｃ
ｅｗｏ
ｒ
ｄｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅ２０１３Ｃｏｎ－
ｆ
ｅ
ｒ
ｅｎｃ
ｅｏ
ｆｔ
ｈｅＮｏ
ｒ
ｔ
ｈＡｍｅ
ｒ
ｉ
ｃ
ａｎＣｈａｐ
ｔ
ｅ
ｒｏ
ｆｔ
ｈｅＡｓ
ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏｎｆ
ｏ
ｒＣｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌＬ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ：ＨｕｍａｎＬａｎｇｕａｇｅＴｅ
ｃｈｎｏ
ｌ
ｏｇ
ｉ
ｅ
ｓ．［
Ｓ．
ｌ．］：Ａｓ
ｓ
ｏ
ｃ
ｉ－
ａ
ｔ
ｉ
ｏｎｆ
ｏ
ｒＣｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌＬ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ，
２０１３：
７４６－７５１．
［
／／Ｐｒ
１０］ＬＥＶＹ Ｏ，ＧＯＬＤＢＥＲＧ Ｙ．Ｌ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃｒ
ｅｇｕ
ｌ
ａ
ｒ
ｉ
ｔ
ｉ
ｅ
ｓｉ
ｎｓｐａ
ｒ
ｓ
ｅａｎｄｅｘｐ
ｌ
ｉ
ｃ
ｉ
ｔｗｏ
ｒ
ｄｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅＥ
ｉ
ｔ
ｅ
ｅｎ
ｔ
ｈＣｏｎ
ｆ
ｅ
ｒ－
ｇｈ
：
，
：
ｅｎｃ
ｅｏｎＣｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌＬａｎｇｕａｇｅＬｅ
ａ
ｒ
ｎ
ｉ
ｎｇ．Ｍａ
ｒ
ｌ
ａ
ｎ
ｄ
Ａ
ｓ
ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏ
ｎ
ｆ
ｏ
ｒ
Ｃ
ｏ
ｍ
ｕ
ｔ
ａ
ｔ
ｉ
ｏ
ｎ
ａ
ｌ
Ｌ
ｉ
ｎ
ｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ
２
０
１
４
１
７
１
１
８
０．
－
ｙ
ｐ
ｇ
［
／／Ｔｗｅｎ
１１］ＱＩＵ Ｌ，ＣＡＯ Ｙ，ＮＩＥＺ，ｅ
ｔａ
ｌ．Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇｗｏ
ｒ
ｄｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｃ
ｏｎｓ
ｉ
ｄｅ
ｒ
ｉ
ｎｇｐ
ｉ
ｉ
ｔ［
Ｃ］
ｔ
ｉ
ｔ
ｈＡＡＡＩＣｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ
ｒ
ｏｘ
ｉｍｉ
ｔ
ｇｕ
ｙ－Ｅ
ｇｈ
ｙａｎｄａｍｂ
ｏｎ Ａｒ
ｔ
ｉ
ｆ
ｉ
ｃ
ｉ
ａ
ｌＩ
ｎ
ｔ
ｅ
ｌ
ｌ
ｉ
ｅ．
Ｃａ
ｌ
ｉ
ｆ
ｏ
ｒ
ｎ
ｉ
ａ：
ＡＡＡＩＰｒ
ｅ
ｓ
ｓ，２０１４：１５７２－１５７８．
ｇｅｎｃ
［
１２］ＳＯＵＴＮＥＴ Ｄ，ＭＬＬＥＲ Ｌ．Ｃｏｎ
ｔ
ｉ
ｎｕｏｕｓｄ
ｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓｏ
ｆｗｏ
ｒ
ｄｓａ
ｓｉ
ｎｐｕ
ｔｏ
ｆＬＳＴＭ Ｎｅ
ｔｗｏ
ｒｋｌ
ａｎｇｕａｇｅ ｍｏｄｅ
ｌ［
Ｊ］．Ｌｅ
ｃ
ｔ
ｕｒ
ｅ
ｎＣｏｍｐｕ
ｔ
ｅ
ｒＳｃ
ｉ
ｅｎｃ
ｅ，２０１４，
８６５５：
１５０－１５７．
Ｎｏ
ｔ
ｅ
ｓｉ
［
／／Ｐｒ
１３］ＬＥ Ｑ Ｖ，ＭＩＫＯＬＯＶ Ｔ．Ｄｉ
ｓ
ｔ
ｒ
ｉ
ｂｕ
ｔ
ｅｄｒ
ｅｐ
ｒ
ｅ
ｓ
ｅｎ
ｔ
ａ
ｔ
ｉ
ｏｎｓｏ
ｆｓ
ｅｎ
ｔ
ｅｎｃ
ｅ
ｓａｎｄｄｏ
ｃｕｍｅｎ
ｔ
ｓ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅ３１ｓ
ｔＩ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌＣｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ
ｏｎ Ｍａ
ｃｈ
ｉ
ｎｅＬｅ
ａ
ｒ
ｎ
ｉ
ｎｇ．
Ｂｅ
ｉ
ｉ
ｎｇ：［
ｓ．
ｎ．］，
２０１４：１１８８－１１９６．
ｊ
［
／／Ｐｒ
１４］ＺＨＡＮＧ Ｑｉ，ＫＡＮＧＪ
ｉ
ａｈｕａ，ＱＩＡＮＪ
ｉ
ｎ，ｅ
ｔａ
ｌ．Ｃｏｎ
ｔ
ｉ
ｎｕｏｕｓｗｏ
ｒ
ｄｅｍｂｅｄｄ
ｉ
ｎｇｓｆ
ｏ
ｒｄｅ
ｔ
ｅ
ｃ
ｔ
ｉ
ｎｇｌ
ｏ
ｃ
ａ
ｌｔ
ｅｘ
ｔｒ
ｅｕｓ
ｅ
ｓａ
ｔｔ
ｈｅｓ
ｅｍａｎ
ｔ
ｉ
ｃｌ
ｅｖｅ
ｌ［
Ｃ］
ｏ－
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅ３７
ｔ
ｈＩ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌＡＣＭ Ｓ
ＩＧＩＲ Ｃｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅｏｎ Ｒｅ
ｓ
ｅ
ａ
ｒ
ｃｈ ＆ Ｄｅｖｅ
ｌ
ｏｐｍｅｎ
ｔｉ
ｎＩ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎ Ｒｅ
ｔ
ｒ
ｉ
ｅｖａ
ｌ．Ｎｅｗ Ｙｏ
ｒｋ：ＡＣＭ，
２０１４：７９７－８０６．
［
／／Ｐｒ
１５］ ＭＮＩＨ Ａ，
ＨＩＮＴＯＮ Ｇ．Ｔｈｒ
ｅ
ｅｎｅｗｇ
ｌ
ｓｆ
ｏ
ｒｓ
ｔ
ａ
ｔ
ｉ
ｓ
ｔ
ｉ
ｃ
ａ
ｌｌ
ａｎｇｕａｇｅｍｏｄｅ
ｌ
ｌ
ｉ
ｎｇ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅ２４
ｔ
ｈＩ
ｎ
ｔ
ｅ
ｒ
ｎａ
ｔ
ｉ
ｏｎａ
ｌＣｏｎ
ｆ
ｅ
ｒ－
ｒ
ａｐｈ
ｉ
ｃ
ａ
ｌｍｏｄｅ
ｅｎｃ
ｅｏｎ Ｍａ
ｃｈ
ｉ
ｎｅＬｅ
ａ
ｒ
ｎ
ｉ
ｎｇ．Ｎｅｗ Ｙｏ
ｒｋ：ＡＣＭ，２００７：６４１－６４８．
［
１６］ＢＬＥＩＤ，ＮＧ Ａ，ＪＯＲＤＡＮ Ｍ．Ｌａ
ｔ
ｅｎ
ｔｄ
ｉ
ｒ
ｉ
ｃｈ
ｌ
ｅ
ｔａ
ｌ
ｌ
ｏ
ｃ
ａ
ｔ
ｉ
ｏｎ［
Ｊ］．Ｊ
ｏｕｒ
ｎａ
ｌｏ
ｆ Ｍａ
ｃｈ
ｉ
ｎｅＬｅ
ａ
ｒ
ｎ
ｉ
ｎｇＲｅ
ｓ
ｅ
ａ
ｒ
ｃｈ，２００３（
３）：９９３－１０２２．
［
／／Ｐｒ
１７］ ＭＡＡＳＡ Ｌ，ＤＡＬＹ Ｒ Ｅ，ＰＨＡＭ Ｐ Ｔ，ｅ
ｔａ
ｌ．Ｌｅ
ａ
ｒ
ｎ
ｉ
ｎｇｗｏ
ｒ
ｄｖｅ
ｃ
ｔ
ｏ
ｒ
ｓｆ
ｏ
ｒｓ
ｅｎ
ｔ
ｉｍｅｎ
ｔａｎａ
ｌ
ｉ
ｓ［
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅ４９
ｔ
ｈＡｎｎｕａ
ｌＭｅ
ｅ
ｔ
ｉ
ｎｇ
ｙｓ
ｏ
ｆｔ
ｈｅＡｓ
ｓ
ｏ
ｃ
ｉ
ａ
ｔ
ｉ
ｏｎｆ
ｏ
ｒＣｏｍｐｕ
ｔ
ａ
ｔ
ｉ
ｏｎａ
ｌＬ
ｉ
ｎｇｕ
ｉ
ｓ
ｔ
ｉ
ｃ
ｓ．
Ｐｏ
ｒ
ｔ
ｌ
ａｎｄ：
ＡＣＬ，２０１１：１４２－１５０．
［
／／第一届全国信息检索与内容安全学 术 会 议（ＮＣＩＲＣＳ２００４）．北 京：中 国 学 术 期 刊 电
１８］ 章成志 ．词语的语义相似度计算及其应用研究［
Ｃ］
子杂志出版社，２００４：５２－６０．
／／ ＮＣＬＲＳ２００４．Ｂｅ
ＺＨＡＮＧ Ｃｈｅｎｚｈ
ｉ．Ｍｅ
ａ
ｓｕｒ
ｉ
ｎｇａｎｄａｐｐ
ｌ
ｉ
ｃ
ａ
ｔ
ｉ
ｏｎｏ
ｆｓ
ｅｍａｎ
ｔ
ｉ
ｃｓ
ｉｍｉ
ｌ
ａ
ｒ
ｔ
ｔｗｅ
ｅｎｗｏ
ｒ
ｄｓ ［
Ｃ］
ｉ
ｉ
ｎｇ：Ｃｈ
ｉ
ｎａＡｃ
ａｄｅｍｉ
ｃＪ
ｏｕｒ
ｎａ
ｌ
ｙｂｅ
ｊ
Ｅｌ
ｅ
ｃ
ｔ
ｒ
ｏｎ
ｉ
ｃＰｕｂ
ｌ
ｉ
ｓｈ
ｉ
ｎｇ Ｈｏｕｓ
ｅ，２００４：５２－６０．
［
１９］ 吴思颖，吴扬扬 ．基于中文 Ｗｏ
ｒ
ｄＮｅ
ｔ的中英文词语相似度计算［
Ｊ］．郑州大学学报（理学版），２０１０，４２（
２）：６６－６９．
ＷＵ Ｓ
ｉ
ｉ
ｎｇ，ＷＵ Ｙａｎｇｙａｎｇ．Ｃｈ
ｉ
ｎｅ
ｓ
ｅａｎｄｅｎｇ
ｌ
ｉ
ｓｈｗｏ
ｒ
ｄｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ａ
ｓｕｒ
ｅｂａ
ｓ
ｅｄｏｎｃｈ
ｉ
ｎｅ
ｓ
ｅ Ｗｏ
ｒ
ｄＮｅ
ｔ［
Ｊ］．Ｊ
ｏｕｒ
ｎａ
ｌｏ
ｆＺｈｅｎｇ
ｚｈｏｕＵｎ
ｉ
ｖｅ
ｒ
ｓ
ｉ
ｔ
ｙ
ｙｍｅ
ｙ
（
Ｎａ
ｔ
ｕｒ
ａ
ｌＳｃ
ｉ
ｅｎｃ
ｅＥｄ
ｉ
ｔ
ｉ
ｏｎ），２０１０，４２（
２）：６６－６９．
［
２０］ 石静，吴云芳，邱立坤，等 ．基于大规模语料库的汉语词义相似度计算方法［
Ｊ］．中文信息学报，
２０１３，
２７（
１）：
１－６．
ＳＨＩＪ
ｉ
ｎｇ，ＷＵ Ｙｕｎ
ｆ
ａｎｇ，ＱＩＵ Ｌ
ｉ
ｋｕｎ，
ｅ
ｔａ
ｌ．Ｃｈ
ｉ
ｎｅ
ｓ
ｅｌ
ｅｘ
ｉ
ｃ
ａ
ｌｓ
ｅｍａｎ
ｔ
ｉ
ｃｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
ｏｍｐｕ
ｔ
ｉ
ｎｇｂａ
ｓ
ｅｄｏｎｌ
ａ
ｒ
ｓ
ｃ
ａ
ｌ
ｅｃ
ｏ
ｒ
Ｊ］．Ｊ
ｏｕｒ
ｎａ
ｌｏ
ｆＣｈ
ｉ－
ｙｃ
ｇｅ
ｐｕｓ［
ｎｅ
ｓ
ｅＩ
ｎ
ｆ
ｏ
ｒｍａ
ｔ
ｉ
ｏｎＰｒ
ｏ
ｃ
ｅ
ｓ
ｓ
ｉ
ｎｇ，
２０１３，
２７（
１）：
１－６．
［
２１］ 郑文超，徐鹏 ．利用 ｗｏ
ｒ
ｄ２ｖｅ
ｃ 对中文词进行聚类的研究［
Ｊ］．软件，２０１３，３４（
１２）：１６０－１６２．
ＺＨＥＮＧ Ｗｅｎｃｈａｏ，ＸＵ Ｐｅｎｇ．Ｒｅ
ｓ
ｅ
ａ
ｒ
ｃｈｏｎｃｈ
ｉ
ｎｅ
ｓ
ｅｗｏ
ｒ
ｄｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇ ｗｉ
ｔ
ｈ ｗｏ
ｒ
ｄ２ｖｅ
ｃ［
Ｊ］．Ｓｏ
ｆ
ｔｗａ
ｒ
ｅ，２０１３，３４（
１２）：１６０－１６２．
［
／／第 三 十 三 届 中 国 控 制 会 议 论 文 集 ．北 京：中 国 学 术 期 刊 电 子 杂 志
２２］ 罗杰，王庆林，李原 ．基于 Ｗｏ
ｒ
ｄ２ｖｅ
ｃ与语义相似度的领域词聚类［
Ｃ］
出版社，２０１４：５１７－５２１．
／／Ｐｒ
ＬＵＯＪ
ｉ
ｅ，ＷＡＮＧ Ｑｉ
ｎｇ
ｌ
ｉ
ｎ，ＬＩＹｕａｎ．Ｗｏ
ｒ
ｄｃ
ｌ
ｕｓ
ｔ
ｅ
ｒ
ｉ
ｎｇｂａ
ｓ
ｅｄｏｎ Ｗｏ
ｒ
ｄ２ｖｅ
ｃａｎｄｓ
ｅｍａｎ
ｔ
ｉ
ｃｓ
ｉｍｉ
ｌ
ａ
ｒ
ｉ
ｔ
Ｃ］
ｏ
ｃ
ｅ
ｅｄ
ｉ
ｎｇｓｏ
ｆｔ
ｈｅ３３ｒ
ｄＣｈ
ｉ
ｎｅ
ｓ
ｅ
ｙ［
Ｃｏｎ
ｔ
ｒ
ｏ
ｌＣｏｎ
ｆ
ｅ
ｒ
ｅｎｃ
ｅ．Ｂｅ
ｉ
ｉ
ｎｇ：Ｃｈ
ｉ
ｎａＡｃ
ａｄｅｍｉ
ｃＪ
ｏｕｒ
ｎａ
ｌＥｌ
ｅ
ｃ
ｔ
ｒ
ｏｎ
ｉ
ｃＰｕｂ
ｌ
ｉ
ｓｈ
ｉ
ｎｇ Ｈｏｕｓ
ｅ，２０１４：５１７－５２１．
ｊ