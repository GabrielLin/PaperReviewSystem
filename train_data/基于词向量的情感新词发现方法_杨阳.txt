 49 
Vol． 49

 11 
No． 11













（



）

Journal of Shandong University（ Natural Science）

：1671-9352（2014）11-0051-08

2014  11 
Nov． 2014

DOI：10. 6040 / j． issn． 1671-9352. 3. 2014. 255


，，，
（ ，  116023）

：，，
。，
，，
。。
：；；
：TP391

：A

New methods for extracting emotional w ords based on
distributed representations of w ords
YANG Yang，LIU Long-fei，WEI Xian-hui，LIN Hong-fei
（ Information Ｒetrieval Laboratory，Dalian University of Technology，Dalian 116023，Liaoning，China）
Abstract： Word-level sentiment analysis is a hot research interest in the field of affective computing． How to recognize
and analyze these new emotional w ords automatically becomes an urgent problem． Firstly，statistics-based approach w as
used to identify the new w ords in M icro-blog corpus and then distributed representation of new w ords w as trained by using neural netw ork in order to get the correlation betw een w ords in corpus． Finally three vector-based methods to find
new emotional w ords w ere introduced． The experimental results indicate that the proposed methods in this paper can be
effectively used in discovery of new emotional w ords．
Key words：emotional w ords； neural netw ork； distributed representations of w ords

0



，、、 。 
、、、、，
。， ，
，。“”，“ ”，
“”，“”，“”，“ ”。 “ ”，“ ”
，“”，“”，“ ”。 
、、、， ，
：2014-08-28； ：2014-10-24 14∶ 10
：http： / / w w w ． cnki． net / kcms / doi /10． 6040 / j． issn． 1671． 9352． 3． 2014． 255． html
60973068） ；（ “”） （2006AA01Z151） ；
：（60673039，
（20090041110002） ；（20110041110034）
：（1989 － ） ，，，． E-mail：yangyang0477@ mail． dlut． edu． cn
* ：（1962 － ） ，，，、、． E-mail： hflin@ dlut． edu． cn

52













（



）

 49 

， 。
。 ， ，
Google  w ord2vec ，
， ， 。

1



［1］
，（ ） ，。  How Net
 ，，

，。，， 80% 
［2］
。  ，
， ，
［3］
， 。 
，、，
。 。
，， ，，
 。，
，。 ，
， 。

2



，，。 “ ”，
 COAE 2014 ， COAE 2014 
（ ） ，， 。
2. 1



 ：
      （ ，） 、  （ ！） 、  （ 。） 、 

Table 1


（ ；） 、（ 、） ； 
，。  1
“”。
，
，。 
 10 ，“
”，“”。
2. 2

 1 
Example of the suffix and prefix




























，、、、 4 
。
1） 。（  6，，） 
。
2） 。 Huang ［4］。 
 t  。， t 
，，。 “ ”，，“ ”
，“”，“”，“”，“ ”，
“”，“ ”，。 “ ”

 11 

53

，：

，“”，“ ”。  2. 1 
 ， 。 
 x  y  t ， t  HL（ t）  HＲ（ t） 
：
HL（ t） = － ∑p（ x | t） log p（ x | t） ，

（1）

HＲ（ t） = － ∑p（ y | t） log p（ y | t） 。

（2）

x

y

，P（ x | t）  x  t ，P（ y | t）  y  t 。
3） 。［5］。“ （ t） ”“
（ x） ”“（ y） ” ，（3） ，
， 。
M I（ t） = log

n
( p（ x）p（p（t） y） ) = log ( N·n ·n
)。
t

a

（3）

b

 ：P（ t） = n t / N，p（ x） = n x / N，p（ y） = n y / N， n t ，n x ，n y 
t、x、y ，N （  6） 。
2. 3



，， 4 ，
。，，
 4 ，，（4） ：
w ordset = ｛ t | Fre（ t） ＞ a1 ∧M I（ t） ＞ a2 ∧HL（ t） ＞ a3 ∧HＲ（ t） ＞ a4 ｝ ，

（4）

，a1 、a2 、a3 、a4 、、、。
 2 ， Top 10。
， （3） 。
 2  Top 10
Table 2 Top 10 new w ords

≥



≤









0. 001 140 870

355

4. 685 104 850

4. 254 826 247



0. 000 715 372

406

2. 605 456 894

4. 046 636 711



0. 000 701 579

1 121

3. 813 146 574

4. 508 211 613



0. 000 494 604

555

4. 163 172 662

4. 519 149 197



0. 000 432 311

421

4. 194 576 893

3. 970 243 046



0. 000 396 102

1 313

4. 869 316 838

3. 991 196 373



0. 000 360 844

417

2. 519 200 053

2. 563 723 244



0. 000 350 936

441

2. 893 203 773

2. 886 910 264



0. 000 338 519

973

4. 529 916 125

4. 155 318 673



0. 000 333 174

780

3. 921 305 888

4. 686 654 537


， ， 24 136 ， COAE
2014 ， 16 597 。

2. 4

 16 597 ， ：
） ”、“≥
，“ ^
≤”、“”，

。
⌒

⌒

，“”，“”，“”、“ A4l”、“”、“
”。
，“Orz”、“”、“”、“”、“”。
，“fall”、“room”、“know”、“root”。
，“TM D”、“SB”、“CNM ”。

54













（



）

 49 

，“”、“”、“”。
，“”、“”、“”、“”、“”。
，“”、“”、“”、“？ ：”。
，。 5 ，
 500 ，， 82. 7% ， 1 。

Fig． 1

 1 
Precision rate of new w ords

，“ ”，“ ”、
“”、“”、“”、“”，，，
。，“”，，。 
，，。

3



， ，
，。 （ distributed representation） 
， N 。 
 One-hot Ｒepresentation（ ，， 1） ，N 
 ，①。
 Google  w ord2vec（ https： / / code． google． com / p / w ord2vec / ） 
， Skip-Gram ， 2 。  w 1 、w 2 、w 3 、…、w t ，SkipGram ［6-7］（5） ：
1 T
F = ∑ ∑ log p（ w t + i | w t ） 。
T t = 1 － b≤i≤b，i≠0

（5）

b ，
b ，
，
。
［7］
6，
 100 。
 Hierarchical Softmax  Skip-Gram ，

 2 w ord2vec  Skip-Gram 
Fig． 2 Skip-Gram model in w ord2vec

①

（ curse of dimensionality） ：，，

。

 11 

4

55

，：



 ， 16 597 ，
 （ https： / / github． com / fxsjy / jieba） 。  HMM 
［8］
  。， 3 
， ，。
4. 1 
［9］
 ， DUTIＲ  
， 100 ， （6）  Cosine  （ http： / / en． w ikipedia． org / w iki /

Cosine － similarity）  0. 8 ，， 4. 1. 1  4. 1. 2 ，
。
A·B
。
（6）
‖A‖ × ‖B‖
 Cosine “” 3 。，
 ， 。
similarity（ A，B） = cos（ A，B） =

 3 “”
Table 3 The w ords close to “nima”in corpus










0. 924 357 53



0. 862 207 65



0. 903 864 30



0. 859 399 20



0. 881 188 33



0. 847 774 74



0. 874 074 50



0. 846 065 34



0. 862 696 65



0. 845 525 40



0. 862 365 25

nnd

0. 839 742 66


 1  ，。

4. 1. 1

 1





 DUTIＲ  S1， S2。






（1）  S1  T， T  T． pos  3，T． neg  1。  T 
，；
（2）  S1  T， T  0. 8  W， W  S2 ，
W． pos = 0，W． neg = 0， W  S2 ；
（3） （2） ，
 S2  W，
 W  S1  P = Similarity（W，
T） ， W． neg = W． neg + P* T． neg，W． pos = W． pos + P* T． pos ；
（4）  T． neg  T． pos ，，。
SVM 

4. 1. 2

 100 ， SVM 。
 7 000 ，
、
、
 1∶ 1. 5∶ 1，

。
 3 ， SVM ， 3、 － 5 ，，
79. 581 4% 。 4. 1. 1  S2  T  100 ，
，。

56





Fig． 3

4. 1. 3









（



）

 49 

 3 SVM 
Parameters of the SVM training



，。，
， 。 （7） ，v c ，x  C 
，| C |  C 。 Cosine 
：
∑x

vc =

x∈c

|C|

。

（7）

“”、“”、“”、“”、“”、“BT”、“”、“”、“”、“”、“
”、“”、“”、“” 15 ， 4（ a） 。 “
”、“”、“”、“”、“”、“”、“”、“ ”， 4
（ b） 。 。

 4  Top 50
Fig． 4 Top 50 w ords close to central vector


 4. 1. 1  4. 1. 2 ， COAE 2014 ，
（  4） ，：

4. 2

（1）  ，
， ，
 ，，
。 4. 1. 1  4. 1. 2 ， W － P（  ） 、W － Ｒ

 11 

57

，：

（ ） 、W － F（  F ） ， 4. 1. 1 
，B － P（ ） 、 B － Ｒ（  ） 、 B － F（  F ） 
4. 1. 2 。
（2）  COAE 2014 ，，：①
“
 ，，！”，
“”。②“ ”，“ ”。
“”“”，。
。


DUTIＲ1

W － P/%

 4 COAE 2014 
Table 4 The evaluation results of COAE 2014 task3
W － Ｒ/%
W－F
B － P/%

4. 846

5. 266

0. 050

B － Ｒ/%

B－F

3. 344

3. 634

0. 034 829 739
0. 032 463 958

DUTIＲ2

4. 846

5. 266

0. 050

3. 117

3. 387

M EDIAN

16. 743

10. 070

0. 118

10. 870

5. 579

0. 067 934 096

M AX

49. 702

20. 961

0. 207

21. 793

16. 834

0. 166 141 662

 4. 1. 1  4. 1. 2 ， ，
 。
P@ N ，
 4. 1. 3 ，
 P@ N ，
 N ，

（ ） 。，
，

5 ，
 2 ，
。 5 ， 6 。


 1
 2
 3
 4
 5


Table 5
P@ 50
0. 74
0. 76
0. 80
0. 70
0. 62
0. 72

 5 
The evaluation results of negative emotional w ords
P@ 100
P@ 200
P@ 500
0. 76
0. 750
0. 628
0. 75
0. 735
0. 656
0. 78
0. 730
0. 682
0. 72
0. 690
0. 670
0. 69
0. 715
0. 690
0. 74
0. 724
0. 665

P@ 1 000
0. 498
0. 620
0. 663
0. 564
0. 648
0. 599


 1
 2
 3
 4
 5


Table 6
P@ 50
0. 70
0. 56
0. 73
0. 64
0. 68
0. 66

 6 
The evaluation results of positive emotional w ords
P@ 100
P@ 200
P@ 500
0. 64
0. 550
0. 522
0. 54
0. 460
0. 372
0. 66
0. 610
0. 542
0. 61
0. 570
0. 474
0. 65
0. 565
0. 432
0. 62
0. 551
0. 468

P@ 1 000
0. 493
0. 287
0. 435
0. 442
0. 415
0. 414

，，
。 N ，
，， P
， 5 。 P@ N  N 
：
（1） ， N ，
：“”、“”
。，
，
 4. 1. 3 
。

Fig． 5

 5 
The evaluation results of emotional w ords

58













（



）

 49 

（2） ，，，
，。、、 N 
，，。

5



 。，
，，。，
。 ，，
 ， ，。
：
［1］ ，，，．  How Net ［J］． ，
2006，20（1） ：14-20．
ZHU Yanlan，M IN Jin，ZHOU Yaqian，et al． Semantic orientation computing based on How Net［J］． Journal of Chinese Information Processing，2006，20（1） ：14-20．
［2］ ，，，． ［J］． ，
2009，
23（5） ：68-74．
WANG Suge，LI Deyu，WEI Yingjie，et al． A synonyms based w ord sentiment orientation discriminating［J］． Journal of Chinese Information Processing，2009，23（5） ：68-74．
［3］ ． ［D］． ：，
2012．
TANG Duyu． Ｒesearch on domain adaptive Chinese sentiment lexicon construction［D］． Harbin： Harbin Institute of Technology，2012．
［4］ HUANG J H，POWEＲS D． Chinese w ord segmentation based on contextual entropy［C］/ / Proceedings of the 17th Asian Pacific Conference on Language，Information and Computation． Singapore，2003：152-158．
［5］ YE Yunming，WU Qingyao，LI Yan，et al． Unknow n Chinese w ord extraction based on variety of overlapping strings［J］． Information Processing ＆ M anagement，2013，49（2） ： 497-512．
［6］ M IKOLOV T，CHEN K，COＲＲADO G，et al． Efficient estimation of w ord representations in vector space［EB / OL］． （201310-23） ［2014-02-23］． http： / / dblp． uni-trier． de / db / journals / corr / corr1301． html#abs-1301-3781．
［7］ M IKOLOV T，SUTSKEVEＲ I，CHEN K，et al． Distributed representations of w ords and phrases and their compositionality
［J］． Advances in Neural Information Processing Systems，2013：3111-3119．
［8］ ，． ［J］． ，
2007，
21（3） ：8-19．
HUANG Changning，ZHAO Hai． Chinese w ord segmentation：a decade review［J］． Journal of Chinese Information Processing，2007，21（3） ：8-19．
［9］ ，，，． ［J］． ，
2008，
27（2） ：180-185．
XU Linhong，LIN Hongfei，PAN Yu，et al． Constructing the affective lexicon ontology［J］． Journal of the China Society for
Scientific and Technical Information，2008，27（2） ：180-185．

（ ：）
（  36 ）
［7］ ＲENDLE S． Factorization machines ［C］/ / Proceedings of the 10th IEEE International Conference on Data M ining
（ ICDM2010） ． Los Alamitos： IEEE Computer Society，2010：995-1000．
［8］ ＲENDLE S． Factorization machines w ith libFM［J］． ACM Transactions on Intelligent Systems and Technology （ TIST ） ，
2012，3（3） ：57. 1-57. 22．
［9］ BLEI D M ，NG A Y，JOＲDAN M I，et al． Latent dirichlet allocation［J］． The Journal of M achine Learning Ｒesearch，2003，
3：993-1022．
［10］ WANG Yi，BAI Hongjie，STANTON M ，et al． PLDA： parallel latent dirichlet allocation for large-scale applications［C］/ /
Proceedings of Algorithmic Applications in M anagement（ AAIM ） ． Berlin，Heidelberg： Springer，2009： 301-314．
［11］ FAN Ｒ E，CHANG K W，HSIEH C J，et al． LIBLINEAＲ： a library for large linear classification［J］． The Journal of M achine Learning Ｒesearch，2008（9） ：1871-1874．
［12］ JOACHIM S T． Training linear SVM s in linear time［C］/ / Proceedings of the 12th ACM SIGKDD International Conference
on Know ledge Discovery and Data M ining（ KDD06） ． New York： ACM ，2006： 217-226．

（ ：）